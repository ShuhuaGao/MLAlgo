{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting a decision stump\n",
    "[Assignment](https://www.coursera.org/learn/ml-classification/supplement/3TYwk/boosting-a-decision-stump) of the Coursera opencousre [*Machine Learning: classification*](https://www.coursera.org/learn/ml-classification/home/welcome) week 5 on Boosting a classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2698: DtypeWarning: Columns (19,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "loans = pd.read_csv('../Data/lending-club-data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and apply one-hot encoding to change the categorical features into numeric ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  reassign the labels to have +1 for a safe loan, and -1 for a risky (bad) loan\n",
    "loans['safe_loans'] = loans['bad_loans'].map({0: +1, 1: -1})\n",
    "loans = loans.drop('bad_loans', axis=1)\n",
    "# consider four features\n",
    "features = ['grade',              # grade of the loan\n",
    "            'term',               # the term of the loan\n",
    "            'home_ownership',     # home_ownership status: own, mortgage or rent\n",
    "            #'emp_length',         # number of years of employment\n",
    "           ]\n",
    "target = 'safe_loans'\n",
    "# extract these columns from the dataset and discard others\n",
    "loans = loans[features + [target]]\n",
    "# one hot encoding\n",
    "loans = pd.get_dummies(loans)\n",
    "loans.head(5)\n",
    "# after encoding, we have new feature names\n",
    "features = list(loans)\n",
    "features.remove(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grade_A',\n",
       " 'grade_B',\n",
       " 'grade_C',\n",
       " 'grade_D',\n",
       " 'grade_E',\n",
       " 'grade_F',\n",
       " 'grade_G',\n",
       " 'term_ 36 months',\n",
       " 'term_ 60 months',\n",
       " 'home_ownership_MORTGAGE',\n",
       " 'home_ownership_OTHER',\n",
       " 'home_ownership_OWN',\n",
       " 'home_ownership_RENT']"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balance the two classes in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape:  (37224, 14) . \n",
      "Value counts: \n",
      "  1    18748\n",
      "-1    18476\n",
      "Name: safe_loans, dtype: int64\n",
      "test shape:  (9284, 14) . \n",
      "Value counts: \n",
      " -1    4674\n",
      " 1    4610\n",
      "Name: safe_loans, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# here we use the index provided by the lecturer\n",
    "import json\n",
    "train_idx_file = '../data/module-5-assignment-2-train-idx.json'\n",
    "test_idx_file = '../data/module-5-assignment-2-test-idx.json'\n",
    "with open(train_idx_file) as f:\n",
    "    train_idx = json.load(f)\n",
    "with open(test_idx_file) as f:\n",
    "    test_idx = json.load(f)\n",
    "train_data = loans.iloc[train_idx, :]\n",
    "test_data = loans.iloc[test_idx, :]\n",
    "print('train shape: ', train_data.shape, '. \\nValue counts: \\n', train_data['safe_loans'].value_counts())\n",
    "print('test shape: ', test_data.shape, '. \\nValue counts: \\n', test_data['safe_loans'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Weighted impurity metric definition\n",
    "\n",
    "+ The basic principle to handle data point weight is to consider an example with weight $\\alpha$ as $\\alpha$ examples.\n",
    "+ In this part, intead of error rate used in the course, we use the Gini Index as the impurity measure of a certain node. It is shown that Entropy or Gini Index is a better metric than classification error when choosing the splitting feature. Reason can be found [here](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/decisiontree-error-vs-entropy.md).\n",
    "+ For a binary decision tree, Gini Index for node t is defined as $GI(t)=1-p_{c1}^2-p_{c2}^2$, where $p_{c1}$ is the ratio of examples belonging to class $c1$. When data points are weighted, we compute $p_{c1}$ as follows\n",
    "\\begin{equation*}\n",
    "p_{c1} = \\frac{\\sum_{c1}\\alpha}{\\sum\\alpha}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weighted_Gini(labels_in_node, data_weights):\n",
    "    total_weight_positive = data_weights[labels_in_node == +1].sum()\n",
    "    total_weight_negative = data_weights[labels_in_node == -1].sum()\n",
    "    total_weight = total_weight_positive + total_weight_negative\n",
    "    return 1 - (total_weight_negative / total_weight) ** 2 - (total_weight_positive / total_weight) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Function to pick the best feature to split on\n",
    "The one which leads to the maximum (weighted) impurity decrease (similar to Information gain)\n",
    "\\begin{equation}\n",
    "\\Delta_{impurity} = GI(parent) - \\frac{left-total-weight}{parent-total-weight}GI(left) - \\frac{right-total-weight}{parent-total-weight}GI(right)\n",
    "\\end{equation}\n",
    "where *left* and *right* are the two children.\n",
    "\n",
    "Because $GI(parent)$ is fixed, we need to find the feature which minimizes $\\frac{left-total-weight}{parent-total-weight}GI(left) + \\frac{right-total-weight}{parent-total-weight}GI(right)$. In addition, since *parent-total-weight* is also fixed, we just need to **minimize** ${(left-total-weight)}GI(left) + {(right-total-weight)}GI(right)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def best_splitting_feature(data, features, target, data_weights):\n",
    "    best_feature = None\n",
    "    min_obj = float('inf')\n",
    "    \n",
    "    for feature in features:\n",
    "        left_filter = data[feature] == 0\n",
    "        right_filter = data[feature] == 1\n",
    "        left_split = data[left_filter]\n",
    "        right_split = data[right_filter]\n",
    "        left_data_weights = data_weights[left_filter]\n",
    "        right_data_weights = data_weights[right_filter]\n",
    "        obj = left_data_weights.sum() * weighted_Gini(left_split[target], left_data_weights) \\\n",
    "            + right_data_weights.sum() * weighted_Gini(right_split[target], right_data_weights) # see the above formula\n",
    "        if obj < min_obj:\n",
    "            min_obj = obj\n",
    "            best_feature = feature\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Build the tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self):\n",
    "        self.is_leaf = False\n",
    "        self.predication = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.splitting_feature = None  \n",
    "        \n",
    "    def __str__(self):\n",
    "        return 'leaf: {}\\npredication:{}\\nsplitting_feature:{}\\n'.format(self.is_leaf, self.predication, self.splitting_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a leaf node from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_leaf(labels_in_node, data_weights):\n",
    "    # sum the total weights for the two classes respectively\n",
    "    # then the larger one will label this leaf node\n",
    "    leaf = Node()\n",
    "    leaf.is_leaf = True\n",
    "    leaf.predication = 1 if data_weights[labels_in_node == 1].sum() > data_weights[labels_in_node == -1].sum() else -1\n",
    "    return leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn a weighted decision tree\n",
    "Stopping conditions:\n",
    "+ All data points in a node belong to the same class.\n",
    "+ No more features are available.\n",
    "+ Maximum depth reached.\n",
    "+ The best feature has only value (0 or 1), i.e., one side of the split is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weighted_decision_tree_create(data, features, target, data_weights, current_depth = 1, max_depth = 10, annotate=False):\n",
    "    def info(s):\n",
    "        if annotate:\n",
    "            print(s)\n",
    "    # stop conditions\n",
    "    if (data[target] == 1).all() or (data[target] == -1).all():\n",
    "        info('>stop condition 1: all the same class')\n",
    "        return create_leaf(data[target], data_weights)\n",
    "    if not features:\n",
    "        info('>stop condtion 2: no more features')\n",
    "        return create_leaf(data[target], data_weights)\n",
    "    if current_depth > max_depth:\n",
    "        info('>stop condtion 3: max depth {} reached'.format(max_depth))\n",
    "        return create_leaf(data[target], data_weights)\n",
    "    # stop condition 4: if one child is empty, i.e., the best feature has only one value in data, then it can be shown that all the other features have only one value in this data as well. In this case, we label current node as a leaf.\n",
    "    best_feature = best_splitting_feature(data, features, target, data_weights)\n",
    "    left_filter = data[best_feature] == 0\n",
    "    right_filter = data[best_feature] == 1\n",
    "    left_data = data[left_filter]\n",
    "    right_data = data[right_filter]\n",
    "    if len(left_data) == 0 or len(right_data) == 0:\n",
    "        info('>stop condtion 4: best feature has only one value')\n",
    "        return create_leaf(data[target], data_weights)\n",
    "    \n",
    "    # build subtrees recursively\n",
    "    node = Node()\n",
    "    node.is_leaf = False\n",
    "    node.splitting_feature = best_feature\n",
    "    if annotate:\n",
    "        print('splitting on ', best_feature)\n",
    "    # remove the used feature\n",
    "    remaining_features = list(features) # first copy \n",
    "    remaining_features.remove(best_feature)\n",
    "    node.left = weighted_decision_tree_create(left_data, remaining_features, target, data_weights[left_filter], current_depth + 1, max_depth)\n",
    "    node.right = weighted_decision_tree_create(right_data, remaining_features, target, data_weights[right_filter], current_depth + 1, max_depth)\n",
    "    return node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility: count the nodes in the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_nodes(tree):\n",
    "    if tree.is_leaf:\n",
    "        return 1\n",
    "    return 1 + count_nodes(tree.left) + count_nodes(tree.right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicate with the weighted decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify(tree, x, annotate=False):\n",
    "    if tree.is_leaf:\n",
    "        return tree.predication\n",
    "    feature = tree.splitting_feature\n",
    "    if x[feature] == 0:\n",
    "        return classify(tree.left, x, annotate)\n",
    "    else:\n",
    "        return classify(tree.right, x, annotate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the tree: compute its classification error rate\n",
    "The function does not change because of adding data point weights. That is, when evaluating the tree, we don't consider the data point weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(tree, data, target):\n",
    "    predications = data.apply(lambda x: classify(tree, x), axis=1) # apply to each row\n",
    "    return (predications != data[target]).sum() / len(predications)  # error rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## intuition on how weighted data points affect the tree being built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose we only care about making good predictions for the first 10 and last 10 items in train_data\n",
    "# assign weight 1 to these 20 examples and 0 to others\n",
    "data_weights = np.zeros(len(train_data))\n",
    "data_weights[0:10] = 1\n",
    "data_weights[-10:] = 1\n",
    "train_data_20 = train_data[data_weights == 1]  # a small subset containing only the 20 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "tree_20 = weighted_decision_tree_create(train_data, features, target, data_weights, current_depth = 1, max_depth = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14999999999999999"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(tree_20, train_data_20, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46854180098860948"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(tree_20, train_data, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, this decision tree performs better on the small dataset. This is because our training process focuses on the 20 data points of the small subset (others have weight 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. AdaBoost: Boosting the decision trees\n",
    "Now that we have a weighted decision tree working, it takes only a bit of work to implement Adaboost. For the sake of simplicity, let us stick with decision tree stumps by training trees with max_depth=1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the weight for classifier and data points\n",
    "\n",
    "### Compute classifier (decision stump here) weight\n",
    "Note that unlike the final evaluation, when computing the tree weight $w$, we use the **weighted error rate** as follows\n",
    "\\begin{equation}\n",
    "w = \\frac{1}{2}ln\\Big(\\frac{1-weighted\\_error\\_rate}{weighted\\_error\\_rate}\\Big)\n",
    "\\end{equation}\n",
    "**weighted error rate** is computed as \n",
    "\\begin{equation}\n",
    "weighted\\_error\\_rate=\\frac{total\\_weight\\_misclassified\\_points}{total\\_weight\\_all\\_points}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weighted_error_rate(data_weights, is_error):\n",
    "    # is_error: array_like, indicating wether the classification of the tree is right\n",
    "    return data_weights[is_error].sum() / data_weights.sum()\n",
    "\n",
    "def compute_classifier_weight(data_weights, is_error):\n",
    "    we = weighted_error_rate(data_weights, is_error)\n",
    "    if np.isclose(we, 0): # default threshold 1e-8\n",
    "        return 20\n",
    "    return 0.5*np.log((1-we) / we)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute(update) the data point weights\n",
    "if $f(x)=y$, i.e., right classified points: $\\alpha \\leftarrow \\alpha \\exp(-w)$\n",
    "\n",
    "if $f(x)\\neq y$, i.e., wrong classified points: $\\alpha \\leftarrow \\alpha \\exp(w)$\n",
    "\n",
    "where *w* is the weight of the current classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_data_weights(data_weights, is_error, w):\n",
    "    # w is the classifier weight\n",
    "    new_data_weights = np.copy(data_weights)\n",
    "    new_data_weights[is_error] = data_weights[is_error] * np.exp(w)\n",
    "    new_data_weights[~is_error] = data_weights[~is_error] * np.exp(-w)\n",
    "    # normalize the weights to ensure numerical stability\n",
    "    new_data_weights = new_data_weights / new_data_weights.sum()\n",
    "    return new_data_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost framework\n",
    "\n",
    "## Learn an ensemble of decision stumps, aka, decision tree of max_depth=1\n",
    "Now write your own Adaboost function. The function accepts 4 parameters:\n",
    "\n",
    "+ data: a data frame with binary features\n",
    "+ features: list of feature names\n",
    "+ target: name of target column\n",
    "+ num_tree_stumps: number of tree stumps to train for the ensemble\n",
    "\n",
    "The function should return the list of tree stumps, along with the list of corresponding tree stump weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def AdaBoost_with_decision_stumps(data, features, target, num_tree_stumps):\n",
    "    # initial data point weights are all equal\n",
    "    data_weights = np.ones(len(data)) / len(data)\n",
    "    \n",
    "    # sequentially train multiple decision stumps\n",
    "    tree_stumps = [None] * num_tree_stumps\n",
    "    tree_stump_weights = [None] * num_tree_stumps\n",
    "    for i in range(num_tree_stumps):\n",
    "        print('============AdaBoost iteration {}============='.format(i))\n",
    "        tree_stumps[i] = weighted_decision_tree_create(data, features, target, data_weights, current_depth=1, max_depth=1, annotate=True)\n",
    "        # classify using this tree stump\n",
    "        predications = data.apply(lambda x: classify(tree_stumps[i], x), axis='columns')\n",
    "        is_error = predications != data[target]\n",
    "        # compute the tree weight\n",
    "        w = compute_classifier_weight(data_weights, is_error)\n",
    "        tree_stump_weights[i] = w\n",
    "        # reweight the data points\n",
    "        data_weights = update_data_weights(data_weights, is_error, w)\n",
    "        print(data_weights)\n",
    "    return tree_stumps, tree_stump_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict with AdaBoost decision stumps\n",
    "\n",
    "$\\hat{y}=sign\\big(\\sum_i w_if_i(x)\\big)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_AdaBoost(tree_stumps, tree_stump_weights, data):\n",
    "    predications = np.empty([len(data), len(tree_stumps)]) # [i, j] is the predication of ith example with the jth stump\n",
    "    for j, stump in enumerate(tree_stumps):\n",
    "        predications[:, j] = data.apply(lambda x: classify(stump, x), axis=1)\n",
    "    print(predications[0:3, :])\n",
    "    # weight sum\n",
    "    ws = np.dot(predications, tree_stump_weights)\n",
    "    return np.sign(ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a boosted ensemble of 10 stumps\n",
    "To make sure there are no errors in our implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============AdaBoost iteration 0=============\n",
      "splitting on  home_ownership_RENT\n",
      "============AdaBoost iteration 1=============\n",
      "splitting on  home_ownership_RENT\n",
      "============AdaBoost iteration 2=============\n",
      "splitting on  home_ownership_RENT\n",
      "============AdaBoost iteration 3=============\n",
      "splitting on  home_ownership_RENT\n",
      "============AdaBoost iteration 4=============\n",
      "splitting on  home_ownership_RENT\n",
      "============AdaBoost iteration 5=============\n",
      "splitting on  home_ownership_RENT\n",
      "============AdaBoost iteration 6=============\n",
      "splitting on  home_ownership_RENT\n",
      "============AdaBoost iteration 7=============\n",
      "splitting on  home_ownership_RENT\n",
      "============AdaBoost iteration 8=============\n",
      "splitting on  home_ownership_RENT\n",
      "============AdaBoost iteration 9=============\n",
      "splitting on  home_ownership_RENT\n"
     ]
    }
   ],
   "source": [
    "num_tree_stumps = 10\n",
    "tree_stumps, tree_stump_weights = AdaBoost_with_decision_stumps(train_data, features, target, num_tree_stumps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing training error/test error vs number of iterations\n",
    "For n = 1 to 30, do the following:\n",
    "\n",
    "+ Make predictions on train_data using tree stumps 0, ..., n-1.\n",
    "+ Compute classification error for the predictions\n",
    "+ Record the classification error for that n.\n",
    "\n",
    "After first iteration, we have one tree stump; after second iteration, we have 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============AdaBoost iteration 0=============\n",
      "splitting on  home_ownership_RENT\n",
      "[  2.52806148e-05   2.86598647e-05   2.52806148e-05 ...,   2.52806148e-05\n",
      "   2.52806148e-05   2.52806148e-05]\n",
      "[<__main__.Node object at 0x00000291CCA81C88>]\n",
      "[0.062729906418386244]\n",
      "[[-1.]\n",
      " [ 1.]\n",
      " [-1.]]\n",
      "[[-1.]\n",
      " [ 1.]\n",
      " [-1.]]\n",
      "============AdaBoost iteration 0=============\n",
      "splitting on  home_ownership_RENT\n",
      "[  2.52806148e-05   2.86598647e-05   2.52806148e-05 ...,   2.52806148e-05\n",
      "   2.52806148e-05   2.52806148e-05]\n",
      "============AdaBoost iteration 1=============\n",
      "splitting on  home_ownership_RENT\n",
      "[  2.53036050e-05   2.86859280e-05   2.53036050e-05 ...,   2.52576664e-05\n",
      "   2.52576664e-05   2.52576664e-05]\n",
      "[<__main__.Node object at 0x00000291CCA94128>, <__main__.Node object at 0x00000291D1245438>]\n",
      "[0.062729906418386244, 0.00090857215732339082]\n",
      "[[-1.  1.]\n",
      " [ 1.  1.]\n",
      " [-1.  1.]]\n",
      "[[-1.  1.]\n",
      " [ 1.  1.]\n",
      " [-1.  1.]]\n",
      "============AdaBoost iteration 0=============\n",
      "splitting on  home_ownership_RENT\n",
      "[  2.52806148e-05   2.86598647e-05   2.52806148e-05 ...,   2.52806148e-05\n",
      "   2.52806148e-05   2.52806148e-05]\n",
      "============AdaBoost iteration 1=============\n",
      "splitting on  home_ownership_RENT\n",
      "[  2.53036050e-05   2.86859280e-05   2.53036050e-05 ...,   2.52576664e-05\n",
      "   2.52576664e-05   2.52576664e-05]\n",
      "============AdaBoost iteration 2=============\n",
      "splitting on  home_ownership_RENT\n",
      "[  2.53059533e-05   2.86832662e-05   2.53059533e-05 ...,   2.52600105e-05\n",
      "   2.52600105e-05   2.52600105e-05]\n",
      "[<__main__.Node object at 0x00000291D2553160>, <__main__.Node object at 0x00000291CCA81C88>, <__main__.Node object at 0x00000291CCA81668>]\n",
      "[0.062729906418386244, 0.00090857215732339082, 9.279726316542966e-05]\n",
      "[[-1.  1.  1.]\n",
      " [ 1.  1. -1.]\n",
      " [-1.  1.  1.]]\n",
      "[[-1.  1.  1.]\n",
      " [ 1.  1. -1.]\n",
      " [-1.  1.  1.]]\n",
      "============AdaBoost iteration 0=============\n",
      "splitting on  home_ownership_RENT\n",
      "[  2.52806148e-05   2.86598647e-05   2.52806148e-05 ...,   2.52806148e-05\n",
      "   2.52806148e-05   2.52806148e-05]\n",
      "============AdaBoost iteration 1=============\n",
      "splitting on  home_ownership_RENT\n",
      "[  2.53036050e-05   2.86859280e-05   2.53036050e-05 ...,   2.52576664e-05\n",
      "   2.52576664e-05   2.52576664e-05]\n",
      "============AdaBoost iteration 2=============\n",
      "splitting on  home_ownership_RENT\n",
      "[  2.53059533e-05   2.86832662e-05   2.53059533e-05 ...,   2.52600105e-05\n",
      "   2.52600105e-05   2.52600105e-05]\n",
      "============AdaBoost iteration 3=============\n",
      "splitting on  home_ownership_RENT\n",
      "[  2.53061931e-05   2.86835381e-05   2.53061931e-05 ...,   2.52597711e-05\n",
      "   2.52597711e-05   2.52597711e-05]\n",
      "[<__main__.Node object at 0x00000291D12457B8>, <__main__.Node object at 0x00000291D1245B00>, <__main__.Node object at 0x00000291CCA94390>, <__main__.Node object at 0x00000291CCA811D0>]\n",
      "[0.062729906418386244, 0.00090857215732339082, 9.279726316542966e-05, 9.4778762215129059e-06]\n",
      "[[-1.  1.  1.  1.]\n",
      " [ 1.  1. -1.  1.]\n",
      " [-1.  1.  1.  1.]]\n",
      "[[-1.  1.  1.  1.]\n",
      " [ 1.  1. -1.  1.]\n",
      " [-1.  1.  1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "train_error_all = []\n",
    "test_error_all = []\n",
    "for n in range(1, 5):\n",
    "    tree_stumps, tree_stump_weights = AdaBoost_with_decision_stumps(train_data, features, target, n)\n",
    "    print(tree_stumps)\n",
    "    print(tree_stump_weights)\n",
    "    train_predications = predict_AdaBoost(tree_stumps, tree_stump_weights, train_data)\n",
    "    train_error_all.append((train_predications != train_data[target]).sum() / len(train_data))\n",
    "    test_predications = predict_AdaBoost(tree_stumps, tree_stump_weights, test_data)\n",
    "    test_error_all.append((test_predications != test_data[target]).sum() / len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.46867612293144206, 0.46867612293144206, 0.46867612293144206]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_error_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (30,) and (3,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-229-9d238248d38c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'figure.figsize'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m31\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_error_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'-'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Training error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m31\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_error_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'-'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Test error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   3315\u001b[0m                       mplDeprecation)\n\u001b[0;32m   3316\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3317\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3318\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3319\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_hold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwashold\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1895\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[0;32m   1896\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[1;32m-> 1897\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1898\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1899\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1404\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_alias_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1406\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1407\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1408\u001b[0m             \u001b[0mlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_grab_next_args\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    405\u001b[0m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 407\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mseg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    408\u001b[0m                     \u001b[1;32myield\u001b[0m \u001b[0mseg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[1;34m(self, tup, kwargs)\u001b[0m\n\u001b[0;32m    383\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'plot'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m    242\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[1;32m--> 244\u001b[1;33m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[0;32m    245\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (30,) and (3,)"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbQAAAEzCAYAAABZgfYmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x291cea1d0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = 7, 5\n",
    "plt.plot(range(1,31), train_error_all, '-', linewidth=4.0, label='Training error')\n",
    "plt.plot(range(1,31), test_error_all, '-', linewidth=4.0, label='Test error')\n",
    "\n",
    "plt.title('Performance of Adaboost ensemble')\n",
    "plt.xlabel('# of iterations')\n",
    "plt.ylabel('Classification error')\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.legend(loc='best', prop={'size':15})\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
