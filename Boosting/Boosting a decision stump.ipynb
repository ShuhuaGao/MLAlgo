{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting a decision stump\n",
    "[Assignment](https://www.coursera.org/learn/ml-classification/supplement/3TYwk/boosting-a-decision-stump) of the Coursera opencousre [*Machine Learning: classification*](https://www.coursera.org/learn/ml-classification/home/welcome) week 5 on Boosting a classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython.core.debugger import Pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2698: DtypeWarning: Columns (19,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "loans = pd.read_csv('../Data/lending-club-data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and apply one-hot encoding to change the categorical features into numeric ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  reassign the labels to have +1 for a safe loan, and -1 for a risky (bad) loan\n",
    "loans['safe_loans'] = loans['bad_loans'].map({0: +1, 1: -1})\n",
    "loans = loans.drop('bad_loans', axis=1)\n",
    "# consider four features\n",
    "features = ['grade',              # grade of the loan\n",
    "            'term',               # the term of the loan\n",
    "            'home_ownership',     # home_ownership status: own, mortgage or rent\n",
    "            'emp_length',         # number of years of employment\n",
    "           ]\n",
    "target = 'safe_loans'\n",
    "# extract these columns from the dataset and discard others\n",
    "loans = loans[features + [target]]\n",
    "# one hot encoding\n",
    "loans = pd.get_dummies(loans)\n",
    "loans.head(5)\n",
    "# after encoding, we have new feature names\n",
    "features = list(loans)\n",
    "features.remove(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grade_A',\n",
       " 'grade_B',\n",
       " 'grade_C',\n",
       " 'grade_D',\n",
       " 'grade_E',\n",
       " 'grade_F',\n",
       " 'grade_G',\n",
       " 'term_ 36 months',\n",
       " 'term_ 60 months',\n",
       " 'home_ownership_MORTGAGE',\n",
       " 'home_ownership_OTHER',\n",
       " 'home_ownership_OWN',\n",
       " 'home_ownership_RENT',\n",
       " 'emp_length_1 year',\n",
       " 'emp_length_10+ years',\n",
       " 'emp_length_2 years',\n",
       " 'emp_length_3 years',\n",
       " 'emp_length_4 years',\n",
       " 'emp_length_5 years',\n",
       " 'emp_length_6 years',\n",
       " 'emp_length_7 years',\n",
       " 'emp_length_8 years',\n",
       " 'emp_length_9 years',\n",
       " 'emp_length_< 1 year',\n",
       " 'emp_length_n/a']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balance the two classes in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape:  (37224, 26) . \n",
      "Value counts: \n",
      "  1    18748\n",
      "-1    18476\n",
      "Name: safe_loans, dtype: int64\n",
      "test shape:  (9284, 26) . \n",
      "Value counts: \n",
      " -1    4674\n",
      " 1    4610\n",
      "Name: safe_loans, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# here we use the index provided by the lecturer\n",
    "import json\n",
    "train_idx_file = '../data/module-5-assignment-2-train-idx.json'\n",
    "test_idx_file = '../data/module-5-assignment-2-test-idx.json'\n",
    "with open(train_idx_file) as f:\n",
    "    train_idx = json.load(f)\n",
    "with open(test_idx_file) as f:\n",
    "    test_idx = json.load(f)\n",
    "train_data = loans.iloc[train_idx, :]\n",
    "test_data = loans.iloc[test_idx, :]\n",
    "print('train shape: ', train_data.shape, '. \\nValue counts: \\n', train_data['safe_loans'].value_counts())\n",
    "print('test shape: ', test_data.shape, '. \\nValue counts: \\n', test_data['safe_loans'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Weighted impurity metric definition\n",
    "\n",
    "+ The basic principle to handle data point weight is to consider an example with weight $\\alpha$ as $\\alpha$ examples.\n",
    "+ In this part, intead of error rate used in the course, we use the Gini Index as the impurity measure of a certain node. It is shown that Entropy or Gini Index is a better metric than classification error when choosing the splitting feature. Reason can be found [here](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/decisiontree-error-vs-entropy.md).\n",
    "+ For a binary decision tree, Gini Index for node t is defined as $GI(t)=1-p_{c1}^2-p_{c2}^2$, where $p_{c1}$ is the ratio of examples belonging to class $c1$. When data points are weighted, we compute $p_{c1}$ as follows\n",
    "\\begin{equation*}\n",
    "p_{c1} = \\frac{\\sum_{c1}\\alpha}{\\sum\\alpha}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weighted_Gini(labels_in_node, data_weights):\n",
    "    if data_weights.size == 0:  # empty\n",
    "        return 0\n",
    "    total_weight_positive = data_weights[labels_in_node == +1].sum()\n",
    "    total_weight_negative = data_weights[labels_in_node == -1].sum()\n",
    "    total_weight = total_weight_positive + total_weight_negative\n",
    "    return 1 - (total_weight_negative / total_weight) ** 2 - (total_weight_positive / total_weight) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Function to pick the best feature to split on\n",
    "The one which leads to the maximum (weighted) impurity decrease (similar to Information gain)\n",
    "\\begin{equation}\n",
    "\\Delta_{impurity} = GI(parent) - \\frac{left-total-weight}{parent-total-weight}GI(left) - \\frac{right-total-weight}{parent-total-weight}GI(right)\n",
    "\\end{equation}\n",
    "where *left* and *right* are the two children.\n",
    "\n",
    "Because $GI(parent)$ is fixed, we need to find the feature which minimizes $\\frac{left-total-weight}{parent-total-weight}GI(left) + \\frac{right-total-weight}{parent-total-weight}GI(right)$. In addition, since *parent-total-weight* is also fixed, we just need to **minimize** ${(left-total-weight)}GI(left) + {(right-total-weight)}GI(right)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def best_splitting_feature(data, features, target, data_weights):\n",
    "    best_feature = None\n",
    "    min_obj = float('inf')\n",
    "    \n",
    "    for feature in features:\n",
    "        left_filter = data[feature] == 0\n",
    "        right_filter = data[feature] == 1\n",
    "        left_split = data[left_filter]\n",
    "        right_split = data[right_filter]\n",
    "        left_data_weights = data_weights[left_filter]\n",
    "        right_data_weights = data_weights[right_filter]\n",
    "        obj = left_data_weights.sum() * weighted_Gini(left_split[target], left_data_weights) \\\n",
    "            + right_data_weights.sum() * weighted_Gini(right_split[target], right_data_weights) # see the above formula\n",
    "        if obj < min_obj:\n",
    "            min_obj = obj\n",
    "            best_feature = feature\n",
    "    if best_feature is None:\n",
    "        Pdb().set_trace()\n",
    "    return best_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Build the tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self):\n",
    "        self.is_leaf = False\n",
    "        self.predication = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.splitting_feature = None  \n",
    "        \n",
    "    def __str__(self):\n",
    "        return 'leaf: {}\\npredication:{}\\nsplitting_feature:{}\\n'.format(self.is_leaf, self.predication, self.splitting_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a leaf node from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_leaf(labels_in_node, data_weights):\n",
    "    # sum the total weights for the two classes respectively\n",
    "    # then the larger one will label this leaf node\n",
    "    leaf = Node()\n",
    "    leaf.is_leaf = True\n",
    "    leaf.predication = 1 if data_weights[labels_in_node == 1].sum() > data_weights[labels_in_node == -1].sum() else -1\n",
    "    return leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn a weighted decision tree\n",
    "Stopping conditions:\n",
    "+ All data points in a node belong to the same class.\n",
    "+ No more features are available.\n",
    "+ Maximum depth reached.\n",
    "+ The best feature has only value (0 or 1), i.e., one side of the split is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weighted_decision_tree_create(data, features, target, data_weights, current_depth = 1, max_depth = 10, annotate=False):\n",
    "    def info(s):\n",
    "        if annotate:\n",
    "            print(s)\n",
    "    # stop conditions\n",
    "    if (data[target] == 1).all() or (data[target] == -1).all():\n",
    "        info('>stop condition 1: all the same class')\n",
    "        return create_leaf(data[target], data_weights)\n",
    "    if len(features) == 0:\n",
    "        print('>stop condtion 2: no more features')\n",
    "        return create_leaf(data[target], data_weights)\n",
    "    if current_depth > max_depth:\n",
    "        info('>stop condtion 3: max depth {} reached'.format(max_depth))\n",
    "        return create_leaf(data[target], data_weights)\n",
    "    # stop condition 4: if one child is empty, i.e., the best feature has only one value in data, then it can be shown that all the other features have only one value in this data as well. In this case, we label current node as a leaf.\n",
    "    best_feature = best_splitting_feature(data, features, target, data_weights)\n",
    "    if best_feature is None:\n",
    "        Pdb().set_trace()\n",
    "    left_filter = data[best_feature] == 0\n",
    "    right_filter = data[best_feature] == 1\n",
    "    left_data = data[left_filter]\n",
    "    right_data = data[right_filter]\n",
    "    if len(left_data) == 0 or len(right_data) == 0:\n",
    "        info('>stop condtion 4: best feature has only one value')\n",
    "        return create_leaf(data[target], data_weights)\n",
    "    \n",
    "    # build subtrees recursively\n",
    "    node = Node()\n",
    "    node.is_leaf = False\n",
    "    node.splitting_feature = best_feature\n",
    "    if annotate:\n",
    "        print('splitting on ', best_feature)\n",
    "    # remove the used feature\n",
    "    remaining_features = list(features) # first copy \n",
    "    remaining_features.remove(best_feature)\n",
    "    node.left = weighted_decision_tree_create(left_data, remaining_features, target, data_weights[left_filter], current_depth + 1, max_depth)\n",
    "    node.right = weighted_decision_tree_create(right_data, remaining_features, target, data_weights[right_filter], current_depth + 1, max_depth)\n",
    "    return node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility: count the nodes in the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_nodes(tree):\n",
    "    if tree.is_leaf:\n",
    "        return 1\n",
    "    return 1 + count_nodes(tree.left) + count_nodes(tree.right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicate with the weighted decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify(tree, x, annotate=False):\n",
    "    if tree.is_leaf:\n",
    "        return tree.predication\n",
    "    feature = tree.splitting_feature\n",
    "    if x[feature] == 0:\n",
    "        return classify(tree.left, x, annotate)\n",
    "    else:\n",
    "        return classify(tree.right, x, annotate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the tree: compute its classification error rate\n",
    "The function does not change because of adding data point weights. That is, when evaluating the tree, we don't consider the data point weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(tree, data, target):\n",
    "    predications = data.apply(lambda x: classify(tree, x), axis=1) # apply to each row\n",
    "    return (predications != data[target]).sum() / len(predications)  # error rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## intuition on how weighted data points affect the tree being built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Suppose we only care about making good predictions for the first 10 and last 10 items in train_data\n",
    "# assign weight 1 to these 20 examples and 0 to others\n",
    "data_weights = np.zeros(len(train_data)) + 1e-3\n",
    "data_weights[0:10] = 1\n",
    "data_weights[-10:] = 1\n",
    "train_data_20 = train_data[data_weights == 1]  # a small subset containing only the 20 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_20 = weighted_decision_tree_create(train_data, features, target, data_weights, current_depth = 1, max_depth = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(tree_20, train_data_20, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42477971201375458"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(tree_20, train_data, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, this decision tree performs better on the small dataset (no error). This is because our training process focuses on the 20 data points of the small subset (others have weight 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. AdaBoost: Boosting the decision trees\n",
    "Now that we have a weighted decision tree working, it takes only a bit of work to implement Adaboost. For the sake of simplicity, let us stick with decision tree stumps by training trees with max_depth=1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the weight for classifier and data points\n",
    "\n",
    "### Compute classifier (decision stump here) weight\n",
    "Note that unlike the final evaluation, when computing the tree weight $w$, we use the **weighted error rate** as follows\n",
    "\\begin{equation}\n",
    "w = \\frac{1}{2}ln\\Big(\\frac{1-weighted\\_error\\_rate}{weighted\\_error\\_rate}\\Big)\n",
    "\\end{equation}\n",
    "**weighted error rate** is computed as \n",
    "\\begin{equation}\n",
    "weighted\\_error\\_rate=\\frac{total\\_weight\\_misclassified\\_points}{total\\_weight\\_all\\_points}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weighted_error_rate(data_weights, is_error):\n",
    "    # is_error: array_like, indicating wether the classification of the tree is right\n",
    "    return data_weights[is_error].sum() / data_weights.sum()\n",
    "\n",
    "def compute_classifier_weight(data_weights, is_error):\n",
    "    we = weighted_error_rate(data_weights, is_error)\n",
    "    if np.isclose(we, 0): # default threshold 1e-8\n",
    "        return 20\n",
    "    return 0.5*np.log((1-we) / we)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute(update) the data point weights\n",
    "if $f(x)=y$, i.e., right classified points: $\\alpha \\leftarrow \\alpha \\exp(-w)$\n",
    "\n",
    "if $f(x)\\neq y$, i.e., wrong classified points: $\\alpha \\leftarrow \\alpha \\exp(w)$\n",
    "\n",
    "where *w* is the weight of the current classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_data_weights(data_weights, is_error, w):\n",
    "    # w is the classifier weight\n",
    "    new_data_weights = np.copy(data_weights)\n",
    "    new_data_weights[is_error] = data_weights[is_error] * np.exp(w)\n",
    "    new_data_weights[~is_error] = data_weights[~is_error] * np.exp(-w)\n",
    "    # normalize the weights to ensure numerical stability\n",
    "    new_data_weights = new_data_weights / new_data_weights.sum()\n",
    "    return new_data_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost framework\n",
    "\n",
    "## Learn an ensemble of decision stumps, aka, decision tree of max_depth=1\n",
    "Now write your own Adaboost function. The function accepts 4 parameters:\n",
    "\n",
    "+ data: a data frame with binary features\n",
    "+ features: list of feature names\n",
    "+ target: name of target column\n",
    "+ num_tree_stumps: number of tree stumps to train for the ensemble\n",
    "\n",
    "The function should return the list of tree stumps, along with the list of corresponding tree stump weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def AdaBoost_with_decision_stumps(data, features, target, num_tree_stumps):\n",
    "    # initial data point weights are all equal\n",
    "    data_weights = np.ones(len(data)) / len(data)\n",
    "    \n",
    "    # sequentially train multiple decision stumps\n",
    "    tree_stumps = [None] * num_tree_stumps\n",
    "    tree_stump_weights = [None] * num_tree_stumps\n",
    "    for i in range(num_tree_stumps):\n",
    "        print('============AdaBoost iteration {}============='.format(i))\n",
    "        tree_stumps[i] = weighted_decision_tree_create(data, features, target, data_weights, current_depth=1, max_depth=1, annotate=False)\n",
    "        # classify using this tree stump\n",
    "        predications = data.apply(lambda x: classify(tree_stumps[i], x), axis='columns')\n",
    "        is_error = predications != data[target]\n",
    "        # compute the tree weight\n",
    "        w = compute_classifier_weight(data_weights, is_error)\n",
    "        tree_stump_weights[i] = w\n",
    "        # reweight the data points\n",
    "        data_weights = update_data_weights(data_weights, is_error, w)\n",
    "    return tree_stumps, tree_stump_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict with AdaBoost decision stumps\n",
    "\n",
    "$\\hat{y}=sign\\big(\\sum_i w_if_i(x)\\big)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_AdaBoost(tree_stumps, tree_stump_weights, data):\n",
    "    predications = np.empty([len(data), len(tree_stumps)]) # [i, j] is the predication of ith example with the jth stump\n",
    "    for j, stump in enumerate(tree_stumps):\n",
    "        predications[:, j] = data.apply(lambda x: classify(stump, x), axis=1)\n",
    "    # weight sum\n",
    "    ws = np.dot(predications, tree_stump_weights)\n",
    "    return np.sign(ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a boosted ensemble of 10 stumps\n",
    "To make sure there are no errors in our implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============AdaBoost iteration 0=============\n",
      "============AdaBoost iteration 1=============\n",
      "============AdaBoost iteration 2=============\n",
      "============AdaBoost iteration 3=============\n",
      "============AdaBoost iteration 4=============\n",
      "============AdaBoost iteration 5=============\n",
      "============AdaBoost iteration 6=============\n",
      "============AdaBoost iteration 7=============\n",
      "============AdaBoost iteration 8=============\n",
      "============AdaBoost iteration 9=============\n"
     ]
    }
   ],
   "source": [
    "num_tree_stumps = 10\n",
    "tree_stumps, tree_stump_weights = AdaBoost_with_decision_stumps(train_data, features, target, num_tree_stumps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing training error/test error vs number of iterations\n",
    "For n = 1 to 30, do the following:\n",
    "\n",
    "+ Make predictions on train_data using tree stumps 0, ..., n-1.\n",
    "+ Compute classification error for the predictions\n",
    "+ Record the classification error for that n.\n",
    "\n",
    "After first iteration, we have one tree stump; after second iteration, we have 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============AdaBoost iteration 0=============\n",
      "stump weights:  [0.1339350739879504]\n",
      "============AdaBoost iteration 0=============\n",
      "============AdaBoost iteration 1=============\n",
      "stump weights:  [0.1339350739879504, 0.19445707401239792]\n",
      "============AdaBoost iteration 0=============\n",
      "============AdaBoost iteration 1=============\n",
      "============AdaBoost iteration 2=============\n",
      "stump weights:  [0.1339350739879504, 0.19445707401239792, 0.052939803799880156]\n",
      "============AdaBoost iteration 0=============\n",
      "============AdaBoost iteration 1=============\n",
      "============AdaBoost iteration 2=============\n",
      "============AdaBoost iteration 3=============\n",
      "stump weights:  [0.1339350739879504, 0.19445707401239792, 0.052939803799880156, 0.06473431232415848]\n",
      "============AdaBoost iteration 0=============\n",
      "============AdaBoost iteration 1=============\n",
      "============AdaBoost iteration 2=============\n",
      "============AdaBoost iteration 3=============\n",
      "============AdaBoost iteration 4=============\n",
      "stump weights:  [0.1339350739879504, 0.19445707401239792, 0.052939803799880156, 0.06473431232415848, 0.074489110328114436]\n",
      "============AdaBoost iteration 0=============\n",
      "============AdaBoost iteration 1=============\n",
      "============AdaBoost iteration 2=============\n",
      "============AdaBoost iteration 3=============\n",
      "============AdaBoost iteration 4=============\n",
      "============AdaBoost iteration 5=============\n",
      "stump weights:  [0.1339350739879504, 0.19445707401239792, 0.052939803799880156, 0.06473431232415848, 0.074489110328114436, 0.078491148368661195]\n",
      "============AdaBoost iteration 0=============\n",
      "============AdaBoost iteration 1=============\n",
      "============AdaBoost iteration 2=============\n",
      "============AdaBoost iteration 3=============\n",
      "============AdaBoost iteration 4=============\n",
      "============AdaBoost iteration 5=============\n",
      "============AdaBoost iteration 6=============\n",
      "stump weights:  [0.1339350739879504, 0.19445707401239792, 0.052939803799880156, 0.06473431232415848, 0.074489110328114436, 0.078491148368661195, 0.037737848718270392]\n",
      "============AdaBoost iteration 0=============\n",
      "============AdaBoost iteration 1=============\n",
      "============AdaBoost iteration 2=============\n",
      "============AdaBoost iteration 3=============\n",
      "============AdaBoost iteration 4=============\n",
      "============AdaBoost iteration 5=============\n",
      "============AdaBoost iteration 6=============\n",
      "============AdaBoost iteration 7=============\n",
      "stump weights:  [0.1339350739879504, 0.19445707401239792, 0.052939803799880156, 0.06473431232415848, 0.074489110328114436, 0.078491148368661195, 0.037737848718270392, 0.054049386819014986]\n",
      "============AdaBoost iteration 0=============\n",
      "============AdaBoost iteration 1=============\n",
      "============AdaBoost iteration 2=============\n",
      "============AdaBoost iteration 3=============\n",
      "============AdaBoost iteration 4=============\n",
      "============AdaBoost iteration 5=============\n",
      "============AdaBoost iteration 6=============\n",
      "============AdaBoost iteration 7=============\n",
      "============AdaBoost iteration 8=============\n",
      "stump weights:  [0.1339350739879504, 0.19445707401239792, 0.052939803799880156, 0.06473431232415848, 0.074489110328114436, 0.078491148368661195, 0.037737848718270392, 0.054049386819014986, 0.024887269109361763]\n",
      "============AdaBoost iteration 0=============\n",
      "============AdaBoost iteration 1=============\n",
      "============AdaBoost iteration 2=============\n",
      "============AdaBoost iteration 3=============\n",
      "============AdaBoost iteration 4=============\n",
      "============AdaBoost iteration 5=============\n",
      "============AdaBoost iteration 6=============\n",
      "============AdaBoost iteration 7=============\n",
      "============AdaBoost iteration 8=============\n",
      "============AdaBoost iteration 9=============\n",
      "stump weights:  [0.1339350739879504, 0.19445707401239792, 0.052939803799880156, 0.06473431232415848, 0.074489110328114436, 0.078491148368661195, 0.037737848718270392, 0.054049386819014986, 0.024887269109361763, 0.031543877188622442]\n",
      "============AdaBoost iteration 0=============\n",
      "============AdaBoost iteration 1=============\n",
      "============AdaBoost iteration 2=============\n",
      "============AdaBoost iteration 3=============\n",
      "============AdaBoost iteration 4=============\n",
      "============AdaBoost iteration 5=============\n",
      "============AdaBoost iteration 6=============\n",
      "============AdaBoost iteration 7=============\n",
      "============AdaBoost iteration 8=============\n",
      "============AdaBoost iteration 9=============\n",
      "============AdaBoost iteration 10=============\n",
      "stump weights:  [0.1339350739879504, 0.19445707401239792, 0.052939803799880156, 0.06473431232415848, 0.074489110328114436, 0.078491148368661195, 0.037737848718270392, 0.054049386819014986, 0.024887269109361763, 0.031543877188622442, 0.02654777054055147]\n",
      "============AdaBoost iteration 0=============\n",
      "============AdaBoost iteration 1=============\n",
      "============AdaBoost iteration 2=============\n",
      "============AdaBoost iteration 3=============\n",
      "============AdaBoost iteration 4=============\n",
      "============AdaBoost iteration 5=============\n",
      "============AdaBoost iteration 6=============\n",
      "============AdaBoost iteration 7=============\n",
      "============AdaBoost iteration 8=============\n",
      "============AdaBoost iteration 9=============\n",
      "============AdaBoost iteration 10=============\n",
      "============AdaBoost iteration 11=============\n",
      "stump weights:  [0.1339350739879504, 0.19445707401239792, 0.052939803799880156, 0.06473431232415848, 0.074489110328114436, 0.078491148368661195, 0.037737848718270392, 0.054049386819014986, 0.024887269109361763, 0.031543877188622442, 0.02654777054055147, 0.02179057869638679]\n",
      "============AdaBoost iteration 0=============\n",
      "============AdaBoost iteration 1=============\n",
      "============AdaBoost iteration 2=============\n",
      "============AdaBoost iteration 3=============\n",
      "============AdaBoost iteration 4=============\n",
      "============AdaBoost iteration 5=============\n",
      "============AdaBoost iteration 6=============\n",
      "============AdaBoost iteration 7=============\n",
      "============AdaBoost iteration 8=============\n",
      "============AdaBoost iteration 9=============\n",
      "============AdaBoost iteration 10=============\n",
      "============AdaBoost iteration 11=============\n",
      "============AdaBoost iteration 12=============\n",
      "stump weights:  [0.1339350739879504, 0.19445707401239792, 0.052939803799880156, 0.06473431232415848, 0.074489110328114436, 0.078491148368661195, 0.037737848718270392, 0.054049386819014986, 0.024887269109361763, 0.031543877188622442, 0.02654777054055147, 0.02179057869638679, 0.018475103347443952]\n",
      "============AdaBoost iteration 0=============\n",
      "============AdaBoost iteration 1=============\n",
      "============AdaBoost iteration 2=============\n",
      "============AdaBoost iteration 3=============\n",
      "============AdaBoost iteration 4=============\n",
      "============AdaBoost iteration 5=============\n",
      "============AdaBoost iteration 6=============\n",
      "============AdaBoost iteration 7=============\n",
      "============AdaBoost iteration 8=============\n",
      "============AdaBoost iteration 9=============\n",
      "============AdaBoost iteration 10=============\n",
      "============AdaBoost iteration 11=============\n",
      "============AdaBoost iteration 12=============\n",
      "============AdaBoost iteration 13=============\n",
      "stump weights:  [0.1339350739879504, 0.19445707401239792, 0.052939803799880156, 0.06473431232415848, 0.074489110328114436, 0.078491148368661195, 0.037737848718270392, 0.054049386819014986, 0.024887269109361763, 0.031543877188622442, 0.02654777054055147, 0.02179057869638679, 0.018475103347443952, 0.016866807355454604]\n",
      "============AdaBoost iteration 0=============\n",
      "============AdaBoost iteration 1=============\n",
      "============AdaBoost iteration 2=============\n",
      "============AdaBoost iteration 3=============\n",
      "============AdaBoost iteration 4=============\n",
      "============AdaBoost iteration 5=============\n",
      "============AdaBoost iteration 6=============\n",
      "============AdaBoost iteration 7=============\n",
      "============AdaBoost iteration 8=============\n",
      "============AdaBoost iteration 9=============\n",
      "============AdaBoost iteration 10=============\n",
      "============AdaBoost iteration 11=============\n",
      "============AdaBoost iteration 12=============\n",
      "============AdaBoost iteration 13=============\n",
      "============AdaBoost iteration 14=============\n",
      "stump weights:  [0.1339350739879504, 0.19445707401239792, 0.052939803799880156, 0.06473431232415848, 0.074489110328114436, 0.078491148368661195, 0.037737848718270392, 0.054049386819014986, 0.024887269109361763, 0.031543877188622442, 0.02654777054055147, 0.02179057869638679, 0.018475103347443952, 0.016866807355454604, 0.01482884071227579]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============AdaBoost iteration 0=============\n",
      "============AdaBoost iteration 1=============\n",
      "============AdaBoost iteration 2=============\n",
      "============AdaBoost iteration 3=============\n",
      "============AdaBoost iteration 4=============\n",
      "============AdaBoost iteration 5=============\n",
      "============AdaBoost iteration 6=============\n",
      "============AdaBoost iteration 7=============\n",
      "============AdaBoost iteration 8=============\n",
      "============AdaBoost iteration 9=============\n",
      "============AdaBoost iteration 10=============\n",
      "============AdaBoost iteration 11=============\n",
      "============AdaBoost iteration 12=============\n",
      "============AdaBoost iteration 13=============\n",
      "============AdaBoost iteration 14=============\n",
      "============AdaBoost iteration 15=============\n",
      "stump weights:  [0.1339350739879504, 0.19445707401239792, 0.052939803799880156, 0.06473431232415848, 0.074489110328114436, 0.078491148368661195, 0.037737848718270392, 0.054049386819014986, 0.024887269109361763, 0.031543877188622442, 0.02654777054055147, 0.02179057869638679, 0.018475103347443952, 0.016866807355454604, 0.01482884071227579, 0.013680890713232859]\n",
      "============AdaBoost iteration 0=============\n",
      "============AdaBoost iteration 1=============\n",
      "============AdaBoost iteration 2=============\n",
      "============AdaBoost iteration 3=============\n",
      "============AdaBoost iteration 4=============\n",
      "============AdaBoost iteration 5=============\n",
      "============AdaBoost iteration 6=============\n",
      "============AdaBoost iteration 7=============\n",
      "============AdaBoost iteration 8=============\n",
      "============AdaBoost iteration 9=============\n",
      "============AdaBoost iteration 10=============\n",
      "============AdaBoost iteration 11=============\n",
      "============AdaBoost iteration 12=============\n",
      "============AdaBoost iteration 13=============\n",
      "============AdaBoost iteration 14=============\n",
      "============AdaBoost iteration 15=============\n",
      "============AdaBoost iteration 16=============\n",
      "stump weights:  [0.1339350739879504, 0.19445707401239792, 0.052939803799880156, 0.06473431232415848, 0.074489110328114436, 0.078491148368661195, 0.037737848718270392, 0.054049386819014986, 0.024887269109361763, 0.031543877188622442, 0.02654777054055147, 0.02179057869638679, 0.018475103347443952, 0.016866807355454604, 0.01482884071227579, 0.013680890713232859, 0.015435257162707085]\n",
      "============AdaBoost iteration 0=============\n",
      "============AdaBoost iteration 1=============\n",
      "============AdaBoost iteration 2=============\n",
      "============AdaBoost iteration 3=============\n",
      "============AdaBoost iteration 4=============\n",
      "============AdaBoost iteration 5=============\n",
      "============AdaBoost iteration 6=============\n",
      "============AdaBoost iteration 7=============\n",
      "============AdaBoost iteration 8=============\n",
      "============AdaBoost iteration 9=============\n",
      "============AdaBoost iteration 10=============\n",
      "============AdaBoost iteration 11=============\n",
      "============AdaBoost iteration 12=============\n",
      "============AdaBoost iteration 13=============\n",
      "============AdaBoost iteration 14=============\n",
      "============AdaBoost iteration 15=============\n",
      "============AdaBoost iteration 16=============\n",
      "============AdaBoost iteration 17=============\n",
      "stump weights:  [0.1339350739879504, 0.19445707401239792, 0.052939803799880156, 0.06473431232415848, 0.074489110328114436, 0.078491148368661195, 0.037737848718270392, 0.054049386819014986, 0.024887269109361763, 0.031543877188622442, 0.02654777054055147, 0.02179057869638679, 0.018475103347443952, 0.016866807355454604, 0.01482884071227579, 0.013680890713232859, 0.015435257162707085, 0.032573791581041199]\n",
      "============AdaBoost iteration 0=============\n",
      "============AdaBoost iteration 1=============\n",
      "============AdaBoost iteration 2=============\n",
      "============AdaBoost iteration 3=============\n",
      "============AdaBoost iteration 4=============\n",
      "============AdaBoost iteration 5=============\n",
      "============AdaBoost iteration 6=============\n",
      "============AdaBoost iteration 7=============\n",
      "============AdaBoost iteration 8=============\n",
      "============AdaBoost iteration 9=============\n",
      "============AdaBoost iteration 10=============\n",
      "============AdaBoost iteration 11=============\n",
      "============AdaBoost iteration 12=============\n",
      "============AdaBoost iteration 13=============\n",
      "============AdaBoost iteration 14=============\n",
      "============AdaBoost iteration 15=============\n",
      "============AdaBoost iteration 16=============\n",
      "============AdaBoost iteration 17=============\n",
      "============AdaBoost iteration 18=============\n",
      "stump weights:  [0.1339350739879504, 0.19445707401239792, 0.052939803799880156, 0.06473431232415848, 0.074489110328114436, 0.078491148368661195, 0.037737848718270392, 0.054049386819014986, 0.024887269109361763, 0.031543877188622442, 0.02654777054055147, 0.02179057869638679, 0.018475103347443952, 0.016866807355454604, 0.01482884071227579, 0.013680890713232859, 0.015435257162707085, 0.032573791581041199, 0.027188606599908089]\n",
      "============AdaBoost iteration 0=============\n",
      "============AdaBoost iteration 1=============\n",
      "============AdaBoost iteration 2=============\n",
      "============AdaBoost iteration 3=============\n",
      "============AdaBoost iteration 4=============\n",
      "============AdaBoost iteration 5=============\n",
      "============AdaBoost iteration 6=============\n",
      "============AdaBoost iteration 7=============\n",
      "============AdaBoost iteration 8=============\n",
      "============AdaBoost iteration 9=============\n",
      "============AdaBoost iteration 10=============\n",
      "============AdaBoost iteration 11=============\n",
      "============AdaBoost iteration 12=============\n",
      "============AdaBoost iteration 13=============\n",
      "============AdaBoost iteration 14=============\n",
      "============AdaBoost iteration 15=============\n",
      "============AdaBoost iteration 16=============\n",
      "============AdaBoost iteration 17=============\n",
      "============AdaBoost iteration 18=============\n",
      "============AdaBoost iteration 19=============\n",
      "stump weights:  [0.1339350739879504, 0.19445707401239792, 0.052939803799880156, 0.06473431232415848, 0.074489110328114436, 0.078491148368661195, 0.037737848718270392, 0.054049386819014986, 0.024887269109361763, 0.031543877188622442, 0.02654777054055147, 0.02179057869638679, 0.018475103347443952, 0.016866807355454604, 0.01482884071227579, 0.013680890713232859, 0.015435257162707085, 0.032573791581041199, 0.027188606599908089, 0.022153461291409084]\n",
      "============AdaBoost iteration 0=============\n",
      "============AdaBoost iteration 1=============\n",
      "============AdaBoost iteration 2=============\n",
      "============AdaBoost iteration 3=============\n",
      "============AdaBoost iteration 4=============\n",
      "============AdaBoost iteration 5=============\n",
      "============AdaBoost iteration 6=============\n",
      "============AdaBoost iteration 7=============\n",
      "============AdaBoost iteration 8=============\n",
      "============AdaBoost iteration 9=============\n",
      "============AdaBoost iteration 10=============\n",
      "============AdaBoost iteration 11=============\n",
      "============AdaBoost iteration 12=============\n",
      "============AdaBoost iteration 13=============\n",
      "============AdaBoost iteration 14=============\n",
      "============AdaBoost iteration 15=============\n",
      "============AdaBoost iteration 16=============\n",
      "============AdaBoost iteration 17=============\n",
      "============AdaBoost iteration 18=============\n",
      "============AdaBoost iteration 19=============\n",
      "============AdaBoost iteration 20=============\n",
      "stump weights:  [0.1339350739879504, 0.19445707401239792, 0.052939803799880156, 0.06473431232415848, 0.074489110328114436, 0.078491148368661195, 0.037737848718270392, 0.054049386819014986, 0.024887269109361763, 0.031543877188622442, 0.02654777054055147, 0.02179057869638679, 0.018475103347443952, 0.016866807355454604, 0.01482884071227579, 0.013680890713232859, 0.015435257162707085, 0.032573791581041199, 0.027188606599908089, 0.022153461291409084, 0.030242949631492648]\n",
      "============AdaBoost iteration 0=============\n",
      "============AdaBoost iteration 1=============\n",
      "============AdaBoost iteration 2=============\n",
      "============AdaBoost iteration 3=============\n",
      "============AdaBoost iteration 4=============\n",
      "============AdaBoost iteration 5=============\n",
      "============AdaBoost iteration 6=============\n",
      "============AdaBoost iteration 7=============\n",
      "============AdaBoost iteration 8=============\n",
      "============AdaBoost iteration 9=============\n",
      "============AdaBoost iteration 10=============\n",
      "============AdaBoost iteration 11=============\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============AdaBoost iteration 12=============\n",
      "============AdaBoost iteration 13=============\n",
      "============AdaBoost iteration 14=============\n",
      "============AdaBoost iteration 15=============\n",
      "============AdaBoost iteration 16=============\n",
      "============AdaBoost iteration 17=============\n",
      "============AdaBoost iteration 18=============\n",
      "============AdaBoost iteration 19=============\n",
      "============AdaBoost iteration 20=============\n",
      "============AdaBoost iteration 21=============\n",
      "stump weights:  [0.1339350739879504, 0.19445707401239792, 0.052939803799880156, 0.06473431232415848, 0.074489110328114436, 0.078491148368661195, 0.037737848718270392, 0.054049386819014986, 0.024887269109361763, 0.031543877188622442, 0.02654777054055147, 0.02179057869638679, 0.018475103347443952, 0.016866807355454604, 0.01482884071227579, 0.013680890713232859, 0.015435257162707085, 0.032573791581041199, 0.027188606599908089, 0.022153461291409084, 0.030242949631492648, 0.027773565973944585]\n",
      "============AdaBoost iteration 0=============\n",
      "============AdaBoost iteration 1=============\n",
      "============AdaBoost iteration 2=============\n",
      "============AdaBoost iteration 3=============\n",
      "============AdaBoost iteration 4=============\n",
      "============AdaBoost iteration 5=============\n",
      "============AdaBoost iteration 6=============\n",
      "============AdaBoost iteration 7=============\n",
      "============AdaBoost iteration 8=============\n",
      "============AdaBoost iteration 9=============\n",
      "============AdaBoost iteration 10=============\n",
      "============AdaBoost iteration 11=============\n",
      "============AdaBoost iteration 12=============\n",
      "============AdaBoost iteration 13=============\n",
      "============AdaBoost iteration 14=============\n",
      "============AdaBoost iteration 15=============\n",
      "============AdaBoost iteration 16=============\n",
      "============AdaBoost iteration 17=============\n",
      "============AdaBoost iteration 18=============\n",
      "============AdaBoost iteration 19=============\n",
      "============AdaBoost iteration 20=============\n",
      "============AdaBoost iteration 21=============\n",
      "============AdaBoost iteration 22=============\n",
      "stump weights:  [0.1339350739879504, 0.19445707401239792, 0.052939803799880156, 0.06473431232415848, 0.074489110328114436, 0.078491148368661195, 0.037737848718270392, 0.054049386819014986, 0.024887269109361763, 0.031543877188622442, 0.02654777054055147, 0.02179057869638679, 0.018475103347443952, 0.016866807355454604, 0.01482884071227579, 0.013680890713232859, 0.015435257162707085, 0.032573791581041199, 0.027188606599908089, 0.022153461291409084, 0.030242949631492648, 0.027773565973944585, 0.010525427305548306]\n",
      "============AdaBoost iteration 0=============\n",
      "============AdaBoost iteration 1=============\n",
      "============AdaBoost iteration 2=============\n",
      "============AdaBoost iteration 3=============\n",
      "============AdaBoost iteration 4=============\n",
      "============AdaBoost iteration 5=============\n",
      "============AdaBoost iteration 6=============\n",
      "============AdaBoost iteration 7=============\n",
      "============AdaBoost iteration 8=============\n",
      "============AdaBoost iteration 9=============\n",
      "============AdaBoost iteration 10=============\n",
      "============AdaBoost iteration 11=============\n",
      "============AdaBoost iteration 12=============\n",
      "============AdaBoost iteration 13=============\n",
      "============AdaBoost iteration 14=============\n",
      "============AdaBoost iteration 15=============\n",
      "============AdaBoost iteration 16=============\n",
      "============AdaBoost iteration 17=============\n",
      "============AdaBoost iteration 18=============\n",
      "============AdaBoost iteration 19=============\n",
      "============AdaBoost iteration 20=============\n",
      "============AdaBoost iteration 21=============\n",
      "============AdaBoost iteration 22=============\n",
      "============AdaBoost iteration 23=============\n",
      "stump weights:  [0.1339350739879504, 0.19445707401239792, 0.052939803799880156, 0.06473431232415848, 0.074489110328114436, 0.078491148368661195, 0.037737848718270392, 0.054049386819014986, 0.024887269109361763, 0.031543877188622442, 0.02654777054055147, 0.02179057869638679, 0.018475103347443952, 0.016866807355454604, 0.01482884071227579, 0.013680890713232859, 0.015435257162707085, 0.032573791581041199, 0.027188606599908089, 0.022153461291409084, 0.030242949631492648, 0.027773565973944585, 0.010525427305548306, 0.018557868686709209]\n",
      "============AdaBoost iteration 0=============\n",
      "============AdaBoost iteration 1=============\n",
      "============AdaBoost iteration 2=============\n",
      "============AdaBoost iteration 3=============\n",
      "============AdaBoost iteration 4=============\n",
      "============AdaBoost iteration 5=============\n",
      "============AdaBoost iteration 6=============\n",
      "============AdaBoost iteration 7=============\n",
      "============AdaBoost iteration 8=============\n",
      "============AdaBoost iteration 9=============\n",
      "============AdaBoost iteration 10=============\n",
      "============AdaBoost iteration 11=============\n",
      "============AdaBoost iteration 12=============\n",
      "============AdaBoost iteration 13=============\n",
      "============AdaBoost iteration 14=============\n",
      "============AdaBoost iteration 15=============\n",
      "============AdaBoost iteration 16=============\n",
      "============AdaBoost iteration 17=============\n",
      "============AdaBoost iteration 18=============\n",
      "============AdaBoost iteration 19=============\n",
      "============AdaBoost iteration 20=============\n",
      "============AdaBoost iteration 21=============\n",
      "============AdaBoost iteration 22=============\n",
      "============AdaBoost iteration 23=============\n",
      "============AdaBoost iteration 24=============\n",
      "stump weights:  [0.1339350739879504, 0.19445707401239792, 0.052939803799880156, 0.06473431232415848, 0.074489110328114436, 0.078491148368661195, 0.037737848718270392, 0.054049386819014986, 0.024887269109361763, 0.031543877188622442, 0.02654777054055147, 0.02179057869638679, 0.018475103347443952, 0.016866807355454604, 0.01482884071227579, 0.013680890713232859, 0.015435257162707085, 0.032573791581041199, 0.027188606599908089, 0.022153461291409084, 0.030242949631492648, 0.027773565973944585, 0.010525427305548306, 0.018557868686709209, 0.012586793535837795]\n",
      "============AdaBoost iteration 0=============\n",
      "============AdaBoost iteration 1=============\n",
      "============AdaBoost iteration 2=============\n",
      "============AdaBoost iteration 3=============\n",
      "============AdaBoost iteration 4=============\n",
      "============AdaBoost iteration 5=============\n",
      "============AdaBoost iteration 6=============\n",
      "============AdaBoost iteration 7=============\n",
      "============AdaBoost iteration 8=============\n",
      "============AdaBoost iteration 9=============\n",
      "============AdaBoost iteration 10=============\n",
      "============AdaBoost iteration 11=============\n",
      "============AdaBoost iteration 12=============\n",
      "============AdaBoost iteration 13=============\n",
      "============AdaBoost iteration 14=============\n",
      "============AdaBoost iteration 15=============\n",
      "============AdaBoost iteration 16=============\n",
      "============AdaBoost iteration 17=============\n",
      "============AdaBoost iteration 18=============\n",
      "============AdaBoost iteration 19=============\n",
      "============AdaBoost iteration 20=============\n",
      "============AdaBoost iteration 21=============\n",
      "============AdaBoost iteration 22=============\n",
      "============AdaBoost iteration 23=============\n",
      "============AdaBoost iteration 24=============\n",
      "============AdaBoost iteration 25=============\n",
      "stump weights:  [0.1339350739879504, 0.19445707401239792, 0.052939803799880156, 0.06473431232415848, 0.074489110328114436, 0.078491148368661195, 0.037737848718270392, 0.054049386819014986, 0.024887269109361763, 0.031543877188622442, 0.02654777054055147, 0.02179057869638679, 0.018475103347443952, 0.016866807355454604, 0.01482884071227579, 0.013680890713232859, 0.015435257162707085, 0.032573791581041199, 0.027188606599908089, 0.022153461291409084, 0.030242949631492648, 0.027773565973944585, 0.010525427305548306, 0.018557868686709209, 0.012586793535837795, 0.011615808568941937]\n",
      "============AdaBoost iteration 0=============\n",
      "============AdaBoost iteration 1=============\n",
      "============AdaBoost iteration 2=============\n",
      "============AdaBoost iteration 3=============\n",
      "============AdaBoost iteration 4=============\n",
      "============AdaBoost iteration 5=============\n",
      "============AdaBoost iteration 6=============\n",
      "============AdaBoost iteration 7=============\n",
      "============AdaBoost iteration 8=============\n",
      "============AdaBoost iteration 9=============\n",
      "============AdaBoost iteration 10=============\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============AdaBoost iteration 11=============\n",
      "============AdaBoost iteration 12=============\n",
      "============AdaBoost iteration 13=============\n",
      "============AdaBoost iteration 14=============\n",
      "============AdaBoost iteration 15=============\n",
      "============AdaBoost iteration 16=============\n",
      "============AdaBoost iteration 17=============\n",
      "============AdaBoost iteration 18=============\n",
      "============AdaBoost iteration 19=============\n",
      "============AdaBoost iteration 20=============\n",
      "============AdaBoost iteration 21=============\n",
      "============AdaBoost iteration 22=============\n",
      "============AdaBoost iteration 23=============\n",
      "============AdaBoost iteration 24=============\n",
      "============AdaBoost iteration 25=============\n",
      "============AdaBoost iteration 26=============\n",
      "stump weights:  [0.1339350739879504, 0.19445707401239792, 0.052939803799880156, 0.06473431232415848, 0.074489110328114436, 0.078491148368661195, 0.037737848718270392, 0.054049386819014986, 0.024887269109361763, 0.031543877188622442, 0.02654777054055147, 0.02179057869638679, 0.018475103347443952, 0.016866807355454604, 0.01482884071227579, 0.013680890713232859, 0.015435257162707085, 0.032573791581041199, 0.027188606599908089, 0.022153461291409084, 0.030242949631492648, 0.027773565973944585, 0.010525427305548306, 0.018557868686709209, 0.012586793535837795, 0.011615808568941937, 0.006444315185610317]\n",
      "============AdaBoost iteration 0=============\n",
      "============AdaBoost iteration 1=============\n",
      "============AdaBoost iteration 2=============\n",
      "============AdaBoost iteration 3=============\n",
      "============AdaBoost iteration 4=============\n",
      "============AdaBoost iteration 5=============\n",
      "============AdaBoost iteration 6=============\n",
      "============AdaBoost iteration 7=============\n",
      "============AdaBoost iteration 8=============\n",
      "============AdaBoost iteration 9=============\n",
      "============AdaBoost iteration 10=============\n",
      "============AdaBoost iteration 11=============\n",
      "============AdaBoost iteration 12=============\n",
      "============AdaBoost iteration 13=============\n",
      "============AdaBoost iteration 14=============\n",
      "============AdaBoost iteration 15=============\n",
      "============AdaBoost iteration 16=============\n",
      "============AdaBoost iteration 17=============\n",
      "============AdaBoost iteration 18=============\n",
      "============AdaBoost iteration 19=============\n",
      "============AdaBoost iteration 20=============\n",
      "============AdaBoost iteration 21=============\n",
      "============AdaBoost iteration 22=============\n",
      "============AdaBoost iteration 23=============\n",
      "============AdaBoost iteration 24=============\n",
      "============AdaBoost iteration 25=============\n",
      "============AdaBoost iteration 26=============\n",
      "============AdaBoost iteration 27=============\n",
      "stump weights:  [0.1339350739879504, 0.19445707401239792, 0.052939803799880156, 0.06473431232415848, 0.074489110328114436, 0.078491148368661195, 0.037737848718270392, 0.054049386819014986, 0.024887269109361763, 0.031543877188622442, 0.02654777054055147, 0.02179057869638679, 0.018475103347443952, 0.016866807355454604, 0.01482884071227579, 0.013680890713232859, 0.015435257162707085, 0.032573791581041199, 0.027188606599908089, 0.022153461291409084, 0.030242949631492648, 0.027773565973944585, 0.010525427305548306, 0.018557868686709209, 0.012586793535837795, 0.011615808568941937, 0.006444315185610317, 0.0063010299230944868]\n",
      "============AdaBoost iteration 0=============\n",
      "============AdaBoost iteration 1=============\n",
      "============AdaBoost iteration 2=============\n",
      "============AdaBoost iteration 3=============\n",
      "============AdaBoost iteration 4=============\n",
      "============AdaBoost iteration 5=============\n",
      "============AdaBoost iteration 6=============\n",
      "============AdaBoost iteration 7=============\n",
      "============AdaBoost iteration 8=============\n",
      "============AdaBoost iteration 9=============\n",
      "============AdaBoost iteration 10=============\n",
      "============AdaBoost iteration 11=============\n",
      "============AdaBoost iteration 12=============\n",
      "============AdaBoost iteration 13=============\n",
      "============AdaBoost iteration 14=============\n",
      "============AdaBoost iteration 15=============\n",
      "============AdaBoost iteration 16=============\n",
      "============AdaBoost iteration 17=============\n",
      "============AdaBoost iteration 18=============\n",
      "============AdaBoost iteration 19=============\n",
      "============AdaBoost iteration 20=============\n",
      "============AdaBoost iteration 21=============\n",
      "============AdaBoost iteration 22=============\n",
      "============AdaBoost iteration 23=============\n",
      "============AdaBoost iteration 24=============\n",
      "============AdaBoost iteration 25=============\n",
      "============AdaBoost iteration 26=============\n",
      "============AdaBoost iteration 27=============\n",
      "============AdaBoost iteration 28=============\n",
      "stump weights:  [0.1339350739879504, 0.19445707401239792, 0.052939803799880156, 0.06473431232415848, 0.074489110328114436, 0.078491148368661195, 0.037737848718270392, 0.054049386819014986, 0.024887269109361763, 0.031543877188622442, 0.02654777054055147, 0.02179057869638679, 0.018475103347443952, 0.016866807355454604, 0.01482884071227579, 0.013680890713232859, 0.015435257162707085, 0.032573791581041199, 0.027188606599908089, 0.022153461291409084, 0.030242949631492648, 0.027773565973944585, 0.010525427305548306, 0.018557868686709209, 0.012586793535837795, 0.011615808568941937, 0.006444315185610317, 0.0063010299230944868, 0.0061611752991920336]\n",
      "============AdaBoost iteration 0=============\n",
      "============AdaBoost iteration 1=============\n",
      "============AdaBoost iteration 2=============\n",
      "============AdaBoost iteration 3=============\n",
      "============AdaBoost iteration 4=============\n",
      "============AdaBoost iteration 5=============\n",
      "============AdaBoost iteration 6=============\n",
      "============AdaBoost iteration 7=============\n",
      "============AdaBoost iteration 8=============\n",
      "============AdaBoost iteration 9=============\n",
      "============AdaBoost iteration 10=============\n",
      "============AdaBoost iteration 11=============\n",
      "============AdaBoost iteration 12=============\n",
      "============AdaBoost iteration 13=============\n",
      "============AdaBoost iteration 14=============\n",
      "============AdaBoost iteration 15=============\n",
      "============AdaBoost iteration 16=============\n",
      "============AdaBoost iteration 17=============\n",
      "============AdaBoost iteration 18=============\n",
      "============AdaBoost iteration 19=============\n",
      "============AdaBoost iteration 20=============\n",
      "============AdaBoost iteration 21=============\n",
      "============AdaBoost iteration 22=============\n",
      "============AdaBoost iteration 23=============\n",
      "============AdaBoost iteration 24=============\n",
      "============AdaBoost iteration 25=============\n",
      "============AdaBoost iteration 26=============\n",
      "============AdaBoost iteration 27=============\n",
      "============AdaBoost iteration 28=============\n",
      "============AdaBoost iteration 29=============\n",
      "stump weights:  [0.1339350739879504, 0.19445707401239792, 0.052939803799880156, 0.06473431232415848, 0.074489110328114436, 0.078491148368661195, 0.037737848718270392, 0.054049386819014986, 0.024887269109361763, 0.031543877188622442, 0.02654777054055147, 0.02179057869638679, 0.018475103347443952, 0.016866807355454604, 0.01482884071227579, 0.013680890713232859, 0.015435257162707085, 0.032573791581041199, 0.027188606599908089, 0.022153461291409084, 0.030242949631492648, 0.027773565973944585, 0.010525427305548306, 0.018557868686709209, 0.012586793535837795, 0.011615808568941937, 0.006444315185610317, 0.0063010299230944868, 0.0061611752991920336, 0.0060246536708601127]\n"
     ]
    }
   ],
   "source": [
    "train_error_all = []\n",
    "test_error_all = []\n",
    "T = 31\n",
    "for n in range(1, T):\n",
    "    tree_stumps, tree_stump_weights = AdaBoost_with_decision_stumps(train_data, features, target, n)\n",
    "    print('stump weights: ', tree_stump_weights)\n",
    "    train_predications = predict_AdaBoost(tree_stumps, tree_stump_weights, train_data)\n",
    "    train_error_all.append((train_predications != train_data[target]).sum() / len(train_data))\n",
    "    test_predications = predict_AdaBoost(tree_stumps, tree_stump_weights, test_data)\n",
    "    test_error_all.append((test_predications != test_data[target]).sum() / len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1   -1\n",
       "6   -1\n",
       "7   -1\n",
       "Name: safe_loans, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[target][0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeMAAAFTCAYAAAAKvWRNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmZnl7P8FOgLhKYqlEQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ea400d1160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = 7, 5\n",
    "plt.plot(range(1,T), train_error_all, '-', linewidth=4.0, label='Training error')\n",
    "plt.plot(range(1,T), test_error_all, '-', linewidth=4.0, label='Test error')\n",
    "\n",
    "plt.title('Performance of Adaboost ensemble')\n",
    "plt.xlabel('# of iterations')\n",
    "plt.ylabel('Classification error')\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.legend(loc='best', prop={'size':15})\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
