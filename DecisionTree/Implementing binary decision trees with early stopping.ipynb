{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing binary decision trees with early stopping\n",
    "[Programming assignment 3](https://www.coursera.org/learn/ml-classification/supplement/AqDoX/decision-trees-in-practice) of *Machine Learning: Classification* by University of Washington on Coursera.\n",
    "\n",
    "In [programming 2](Implementing binary decision trees.ipynb), we have implemented a basic version of decision tree. Here, we add more early stopping criteria to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2698: DtypeWarning: Columns (19,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "loans = pd.read_csv('../Data/lending-club-data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  reassign the labels to have +1 for a safe loan, and -1 for a risky (bad) loan\n",
    "loans['safe_loans'] = loans['bad_loans'].map({0: +1, 1: -1})\n",
    "loans.drop('bad_loans', axis=1)\n",
    "# consider four features\n",
    "features = ['grade',              # grade of the loan\n",
    "            'term',               # the term of the loan\n",
    "            'home_ownership',     # home_ownership status: own, mortgage or rent\n",
    "            'emp_length',         # number of years of employment\n",
    "           ]\n",
    "target = 'safe_loans'\n",
    "# extract these columns from the dataset and discard others\n",
    "loans = loans[features + [target]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding\n",
    "By one-hot encoding, we have only numeric features. In this case, each encoded feature is either 1 or 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(122607, 26)\n"
     ]
    }
   ],
   "source": [
    "loans = pd.get_dummies(loans)\n",
    "loans.head(5)\n",
    "print(loans.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## balance the two classes in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape:  (37224, 26) . \n",
      "Value counts: \n",
      "  1    18748\n",
      "-1    18476\n",
      "Name: safe_loans, dtype: int64\n",
      "test shape:  (9284, 26) -1    4674\n",
      " 1    4610\n",
      "Name: safe_loans, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# the train and test set index\n",
    "import json\n",
    "train_idx_file = '../data/module-5-assignment-2-train-idx.json'\n",
    "test_idx_file = '../data/module-5-assignment-2-test-idx.json'\n",
    "with open(train_idx_file) as f:\n",
    "    train_idx = json.load(f)\n",
    "with open(test_idx_file) as f:\n",
    "    test_idx = json.load(f)\n",
    "train_data = loans.iloc[train_idx, :]\n",
    "test_data = loans.iloc[test_idx, :]\n",
    "print('train shape: ', train_data.shape, '. \\nValue counts: \\n', train_data['safe_loans'].value_counts())\n",
    "print('test shape: ', test_data.shape, test_data['safe_loans'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2ã€‚Implement a binary decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to count number of mistakes while predicting majority class\n",
    "In each intermediate node, we label it with the majority class. Then, the misclassification rate can be calculated. This is used to determine the best feature for splitting.\n",
    "\n",
    "**Note:** Keep in mind that in order to compute the number of mistakes for a majority classifier, we only need the label (y values) of the data points in the node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def intermediate_node_num_mistakes(labels_in_node):\n",
    "    num_safe_loans = np.count_nonzero(labels_in_node == 1)\n",
    "    num_risky_loans = np.count_nonzero(labels_in_node == -1)\n",
    "    return num_risky_loans if num_safe_loans > num_risky_loans else num_safe_loans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "# test case 1\n",
    "example_labels = np.array([-1, -1, 1, -1, -1])\n",
    "if intermediate_node_num_mistakes(example_labels) == 1:\n",
    "    print('Test passed!')\n",
    "else:\n",
    "    print('Test 1 failed... try again!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "# test case 2\n",
    "example_labels = np.array([-1, -1, 1, 1, 1, 1, 1])\n",
    "if intermediate_node_num_mistakes(example_labels) == 2:\n",
    "    print('Test passed!')\n",
    "else:\n",
    "    print('Test 1 failed... try again!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "example_labels = np.array([-1, -1, -1, -1, -1, 1, 1])\n",
    "if intermediate_node_num_mistakes(example_labels) == 2:\n",
    "    print('Test passed!')\n",
    "else:\n",
    "    print('Test 1 failed... try again!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to pick best feature to split on\n",
    "The function will loop through the list of possible features, and consider splitting on each of them. It will calculate the classification error of each split and return the feature that had the smallest classification error when split on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def best_splitting_feature(data, features, target):\n",
    "    best_feature = None\n",
    "    min_mistakes = float('Inf')\n",
    "    for feature in features:\n",
    "        # split into two subsets: left for 0 and right for 1\n",
    "        left_split = data[data[feature] == 0]\n",
    "        right_split = data[data[feature] == 1]\n",
    "        # number of misclassifications\n",
    "        left_mistakes = intermediate_node_num_mistakes(left_split[target])\n",
    "        right_mistakes = intermediate_node_num_mistakes(right_split[target])\n",
    "        # error rate is: (left_mistakes + right_mistakes) / number of records in data\n",
    "        # since number of records in data remains the same for this splitting, no need to compute\n",
    "        mistakes = left_mistakes + right_mistakes\n",
    "        if mistakes < min_mistakes:\n",
    "            min_mistakes = mistakes\n",
    "            best_feature = feature\n",
    "    return best_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the tree\n",
    "Each node in the tree is represented as following\n",
    "\n",
    "Early *stopping conditions* 1, 2, and 3\n",
    "+ Reached a maximum depth. (set by parameter max_depth).\n",
    "+ Reached a minimum node size. (set by parameter min_node_size).\n",
    "+ Don't split if the gain in error reduction is too small. (set by parameter min_error_reduction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self):\n",
    "        self.is_leaf = False\n",
    "        self.predication = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.splitting_feature = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a leaf node given a set of target values: majority \n",
    "def create_leaf(labels_in_node):\n",
    "    leaf = Node()\n",
    "    leaf.is_leaf = True\n",
    "    if np.count_nonzero(labels_in_node == 1) > np.count_nonzero(labels_in_node == -1):\n",
    "        leaf.predication = 1\n",
    "    else:\n",
    "        leaf.predication = -1\n",
    "    return leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recursive tree building stop criteria:\n",
    "+ Condition 1: all data points in the node are from the same class\n",
    "+ Condition 2: no more features available (each feature can be used for splitting once along a path in a tree)\n",
    "\n",
    "**Early stopping criteria**\n",
    "+ max depth reached\n",
    "+ minimum node size reached (an intermediate node contains too few data points)\n",
    "+ the classification error rate drops quite little after any splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Early stopping condition 2: Whether one node has reached the minimum dataset size\n",
    "def reached_minimum_node_size(data, min_node_size):\n",
    "    return len(data) <= min_node_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# early stopping condition 3: the classification error reduction is too small\n",
    "def error_reduction(error_before_split, error_after_split):\n",
    "    return error_before_split - error_after_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decision_tree_create(data, features, target, current_depth=0, max_depth=10, min_node_size=1, min_error_reduction=0):\n",
    "    target_values = data[target];\n",
    "    \n",
    "    # stop\n",
    "    # stopping condition1: in the same class, that is, misclassification error will be zero\n",
    "    if intermediate_node_num_mistakes(target_values) == 0:\n",
    "        print('Stopping condition 1 reached: all in the same class')\n",
    "        return create_leaf(target_values)\n",
    "    # stopping condition 2: no more features to use\n",
    "    if len(features) == 0:\n",
    "        print('Stopping condition 2 reached: no more features')\n",
    "        return create_leaf(target_values)\n",
    "    # early stopping condition 1: max depth reached\n",
    "    if current_depth >= max_depth:\n",
    "        print('Early stopping condition 1: max depth.')\n",
    "        return create_leaf(target_values)\n",
    "    # early stopping condition 2: minimum node size reached\n",
    "    if reached_minimum_node_size(data, min_node_size):\n",
    "        print('Early stopping condition 2: minimum node size.')\n",
    "        return create_leaf(target_values)\n",
    "    \n",
    "    # find the best feature to split\n",
    "    best_feature = best_splitting_feature(data, features, target)\n",
    "    # split\n",
    "    left_split = data[data[best_feature] == 0]\n",
    "    right_split = data[data[best_feature] == 1]\n",
    "    \n",
    "    # early stopping condition 3: after we obtain the splitting, what is the error reduction?\n",
    "    error_before_split = intermediate_node_num_mistakes(data[target])\n",
    "    error_after_split = intermediate_node_num_mistakes(left_split[target]) + intermediate_node_num_mistakes(right_split[target])\n",
    "    if error_reduction(error_before_split, error_after_split) / len(data) <= min_error_reduction:\n",
    "        print('Early stopping condition 3: min error reduction.')\n",
    "        return create_leaf(target_values)\n",
    "    \n",
    "    # remove this feature from current recursion path\n",
    "    # in Python, generally do NOT change the arguments due to the reference semantics\n",
    "    remaining_features = features[:]\n",
    "    remaining_features.remove(best_feature)\n",
    "    print('Split on feature {0} into two subsets of size {1} and {2}.'.format(best_feature, len(left_split), len(right_split)))\n",
    "    \n",
    "    # if the selected feature has only one value in this dataset, then either left or right_split will be empty. In this case,\n",
    "    # we will build a leaf node for the empty split subset, whose class is the majority of its parent.\n",
    "    if len(left_split) == 0 or len(right_split) == 0:\n",
    "        print('The chosen splitting feature has only one value in the dataset.')\n",
    "        return create_leaf(target_values)\n",
    "    \n",
    "    # recursion\n",
    "    node = Node()\n",
    "    node.is_leaf = False\n",
    "    node.splitting_feature = best_feature\n",
    "    if len(left_split) > 0:\n",
    "        node.left = decision_tree_create(left_split, features, target, current_depth + 1, max_depth)\n",
    "    else:\n",
    "        node.left = create_leaf(target_values)\n",
    "    if len(right_split) == 0:\n",
    "        node.right = create_leaf(target_values)\n",
    "    else:\n",
    "        node.right = decision_tree_create(right_split, features, target, current_depth + 1, max_depth)\n",
    "    return node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ In the above building process, when the chosen feature has only one value in the intermediate subset D, the procedure labels such an intermediate node D as a leaf. \n",
    "\n",
    "+ However, a better way is to continue the tree building: for the child whose data is empty, assign it as a leaf and its class is the majority of its parent D. For the other child whose data is not empty (actually also D), continue the building process for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split on feature term_ 36 months into two subsets of size 9223 and 28001.\n",
      "Split on feature grade_A into two subsets of size 9122 and 101.\n",
      "Early stopping condition 3: min error reduction.\n",
      "Split on feature emp_length_n/a into two subsets of size 96 and 5.\n",
      "Split on feature emp_length_< 1 year into two subsets of size 85 and 11.\n",
      "Early stopping condition 3: min error reduction.\n",
      "Early stopping condition 3: min error reduction.\n",
      "Early stopping condition 3: min error reduction.\n",
      "Split on feature grade_D into two subsets of size 23300 and 4701.\n",
      "Split on feature grade_E into two subsets of size 22024 and 1276.\n",
      "Split on feature grade_F into two subsets of size 21666 and 358.\n",
      "Split on feature emp_length_n/a into two subsets of size 20734 and 932.\n",
      "Split on feature grade_G into two subsets of size 20638 and 96.\n",
      "Early stopping condition 1: max depth.\n",
      "Early stopping condition 1: max depth.\n",
      "Split on feature grade_A into two subsets of size 702 and 230.\n",
      "Early stopping condition 1: max depth.\n",
      "Early stopping condition 1: max depth.\n",
      "Split on feature emp_length_8 years into two subsets of size 347 and 11.\n",
      "Early stopping condition 3: min error reduction.\n",
      "Split on feature home_ownership_OWN into two subsets of size 9 and 2.\n",
      "Early stopping condition 1: max depth.\n",
      "Stopping condition 1 reached: all in the same class\n",
      "Early stopping condition 3: min error reduction.\n",
      "Early stopping condition 3: min error reduction.\n"
     ]
    }
   ],
   "source": [
    "# build the tree\n",
    "features = list(train_data) # equivalent to my_dataframe.columns.values.tolist()\n",
    "features.remove(target)\n",
    "tree_new = decision_tree_create(train_data, features, target, 0, max_depth=6, min_node_size=0, min_error_reduction=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split on feature term_ 36 months into two subsets of size 9223 and 28001.\n",
      "Split on feature grade_A into two subsets of size 9122 and 101.\n",
      "Early stopping condition 3: min error reduction.\n",
      "Split on feature emp_length_n/a into two subsets of size 96 and 5.\n",
      "Split on feature emp_length_< 1 year into two subsets of size 85 and 11.\n",
      "Early stopping condition 3: min error reduction.\n",
      "Early stopping condition 3: min error reduction.\n",
      "Early stopping condition 3: min error reduction.\n",
      "Split on feature grade_D into two subsets of size 23300 and 4701.\n",
      "Split on feature grade_E into two subsets of size 22024 and 1276.\n",
      "Split on feature grade_F into two subsets of size 21666 and 358.\n",
      "Split on feature emp_length_n/a into two subsets of size 20734 and 932.\n",
      "Split on feature grade_G into two subsets of size 20638 and 96.\n",
      "Early stopping condition 1: max depth.\n",
      "Early stopping condition 1: max depth.\n",
      "Split on feature grade_A into two subsets of size 702 and 230.\n",
      "Early stopping condition 1: max depth.\n",
      "Early stopping condition 1: max depth.\n",
      "Split on feature emp_length_8 years into two subsets of size 347 and 11.\n",
      "Early stopping condition 3: min error reduction.\n",
      "Split on feature home_ownership_OWN into two subsets of size 9 and 2.\n",
      "Early stopping condition 1: max depth.\n",
      "Stopping condition 1 reached: all in the same class\n",
      "Early stopping condition 3: min error reduction.\n",
      "Early stopping condition 3: min error reduction.\n"
     ]
    }
   ],
   "source": [
    "# ignore the early stopping condition 2 and 3\n",
    "tree_old = decision_tree_create(train_data, features, target, 0, max_depth=6, min_node_size=2000, min_error_reduction=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predication\n",
    "Just preorder traversal of a binary tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify(tree, x, annotate=False):\n",
    "    if tree.is_leaf:\n",
    "        if annotate:\n",
    "            print('At leaf, predicting {}'.format(tree.predication))\n",
    "        return tree.predication\n",
    "    # goto the left or right subtree depending on the feature value\n",
    "    split_feature_value = x[tree.splitting_feature]\n",
    "    if annotate:\n",
    "        print('Split on {} = {}'.format(tree.splitting_feature, split_feature_value))\n",
    "    if split_feature_value == 0:\n",
    "        return classify(tree.left, x, annotate)\n",
    "    else:\n",
    "        return classify(tree.right, x, annotate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_classification_error(tree, data):\n",
    "    predications = data.apply(lambda record: classify(tree, record), axis=1) # apply to each row\n",
    "    return (predications != data[target]).sum() / len(predications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38377854373115039"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classification_error(tree_new, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38377854373115039"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classification_error(tree_old, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_leaves(tree):\n",
    "    if tree.is_leaf:\n",
    "        return 1\n",
    "    return count_leaves(tree.left) + count_leaves(tree.right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# leaves:  13 ,  13\n"
     ]
    }
   ],
   "source": [
    "print('# leaves: ', count_leaves(tree_new), ', ', count_leaves(tree_old))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
