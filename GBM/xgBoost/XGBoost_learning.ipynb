{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn XGBoost with [A Guide to Gradient Boosted Trees with XGBoost in Python](https://jessesw.com/XG-Boost/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data', header = None)\n",
    "test_set = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test',\n",
    "                      skiprows = 1, header = None) # Make sure to skip a row for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0                  1       2           3   4                    5   \\\n",
       "0  39          State-gov   77516   Bachelors  13        Never-married   \n",
       "1  50   Self-emp-not-inc   83311   Bachelors  13   Married-civ-spouse   \n",
       "2  38            Private  215646     HS-grad   9             Divorced   \n",
       "3  53            Private  234721        11th   7   Married-civ-spouse   \n",
       "4  28            Private  338409   Bachelors  13   Married-civ-spouse   \n",
       "\n",
       "                   6               7       8        9     10  11  12  \\\n",
       "0        Adm-clerical   Not-in-family   White     Male  2174   0  40   \n",
       "1     Exec-managerial         Husband   White     Male     0   0  13   \n",
       "2   Handlers-cleaners   Not-in-family   White     Male     0   0  40   \n",
       "3   Handlers-cleaners         Husband   Black     Male     0   0  40   \n",
       "4      Prof-specialty            Wife   Black   Female     0   0  40   \n",
       "\n",
       "               13      14  \n",
       "0   United-States   <=50K  \n",
       "1   United-States   <=50K  \n",
       "2   United-States   <=50K  \n",
       "3   United-States   <=50K  \n",
       "4            Cuba   <=50K  "
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>Private</td>\n",
       "      <td>226802</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>89814</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>336951</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>160323</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>7688</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>?</td>\n",
       "      <td>103497</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>?</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0           1       2              3   4                    5   \\\n",
       "0  25     Private  226802           11th   7        Never-married   \n",
       "1  38     Private   89814        HS-grad   9   Married-civ-spouse   \n",
       "2  28   Local-gov  336951     Assoc-acdm  12   Married-civ-spouse   \n",
       "3  44     Private  160323   Some-college  10   Married-civ-spouse   \n",
       "4  18           ?  103497   Some-college  10        Never-married   \n",
       "\n",
       "                   6           7       8        9     10  11  12  \\\n",
       "0   Machine-op-inspct   Own-child   Black     Male     0   0  40   \n",
       "1     Farming-fishing     Husband   White     Male     0   0  50   \n",
       "2     Protective-serv     Husband   White     Male     0   0  40   \n",
       "3   Machine-op-inspct     Husband   Black     Male  7688   0  40   \n",
       "4                   ?   Own-child   White   Female     0   0  30   \n",
       "\n",
       "               13       14  \n",
       "0   United-States   <=50K.  \n",
       "1   United-States   <=50K.  \n",
       "2   United-States    >50K.  \n",
       "3   United-States    >50K.  \n",
       "4   United-States   <=50K.  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_labels = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation', \n",
    "              'relationship', 'race', 'sex', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country',\n",
    "             'wage_class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.columns = col_labels\n",
    "train_set.columns = col_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32561 entries, 0 to 32560\n",
      "Data columns (total 15 columns):\n",
      "age               32561 non-null int64\n",
      "workclass         32561 non-null object\n",
      "fnlwgt            32561 non-null int64\n",
      "education         32561 non-null object\n",
      "education_num     32561 non-null int64\n",
      "marital_status    32561 non-null object\n",
      "occupation        32561 non-null object\n",
      "relationship      32561 non-null object\n",
      "race              32561 non-null object\n",
      "sex               32561 non-null object\n",
      "capital_gain      32561 non-null int64\n",
      "capital_loss      32561 non-null int64\n",
      "hours_per_week    32561 non-null int64\n",
      "native_country    32561 non-null object\n",
      "wage_class        32561 non-null object\n",
      "dtypes: int64(6), object(9)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "train_set.info() # note that Pandas will not take '?' as nan automatically, use na_values to specify it\n",
    "# By default the following values are interpreted as NaN: ‘’, ‘#N/A’, ‘#N/A N/A’, ‘#NA’, ‘-1.#IND’, ‘-1.#QNAN’, ‘-NaN’, \n",
    "# ‘-nan’, ‘1.#IND’, ‘1.#QNAN’, ‘N/A’, ‘NA’, ‘NULL’, ‘NaN’, ‘n/a’, ‘nan’, ‘null’."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove rows containing unknown values (\" ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30162, 15) (15060, 15)\n"
     ]
    }
   ],
   "source": [
    "train_set = train_set.replace(\" ?\", np.nan).dropna()\n",
    "test_set = test_set.replace(\" ?\", np.nan).dropna()\n",
    "print(train_set.shape, test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education_num</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>native_country</th>\n",
       "      <th>wage_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>Private</td>\n",
       "      <td>226802</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>89814</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>336951</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>160323</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>7688</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>34</td>\n",
       "      <td>Private</td>\n",
       "      <td>198693</td>\n",
       "      <td>10th</td>\n",
       "      <td>6</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age   workclass  fnlwgt      education  education_num       marital_status  \\\n",
       "0   25     Private  226802           11th              7        Never-married   \n",
       "1   38     Private   89814        HS-grad              9   Married-civ-spouse   \n",
       "2   28   Local-gov  336951     Assoc-acdm             12   Married-civ-spouse   \n",
       "3   44     Private  160323   Some-college             10   Married-civ-spouse   \n",
       "5   34     Private  198693           10th              6        Never-married   \n",
       "\n",
       "           occupation    relationship    race    sex  capital_gain  \\\n",
       "0   Machine-op-inspct       Own-child   Black   Male             0   \n",
       "1     Farming-fishing         Husband   White   Male             0   \n",
       "2     Protective-serv         Husband   White   Male             0   \n",
       "3   Machine-op-inspct         Husband   Black   Male          7688   \n",
       "5       Other-service   Not-in-family   White   Male             0   \n",
       "\n",
       "   capital_loss  hours_per_week  native_country wage_class  \n",
       "0             0              40   United-States     <=50K.  \n",
       "1             0              50   United-States     <=50K.  \n",
       "2             0              40   United-States      >50K.  \n",
       "3             0              40   United-States      >50K.  \n",
       "5             0              30   United-States     <=50K.  "
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' <=50K', ' >50K'], dtype=object)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note that the wage_class in the test set has an additional dot after its value\n",
    "test_set['wage_class'] = test_set['wage_class'].replace({' <=50K.': ' <=50K', ' >50K.': ' >50K'})\n",
    "test_set['wage_class'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Ordinal Encoding to Categoricals\n",
    "All called numeric encoding. That is, assign a unique number to each category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 45222 entries, 0 to 16280\n",
      "Data columns (total 15 columns):\n",
      "age               45222 non-null int64\n",
      "workclass         45222 non-null object\n",
      "fnlwgt            45222 non-null int64\n",
      "education         45222 non-null object\n",
      "education_num     45222 non-null int64\n",
      "marital_status    45222 non-null object\n",
      "occupation        45222 non-null object\n",
      "relationship      45222 non-null object\n",
      "race              45222 non-null object\n",
      "sex               45222 non-null object\n",
      "capital_gain      45222 non-null int64\n",
      "capital_loss      45222 non-null int64\n",
      "hours_per_week    45222 non-null int64\n",
      "native_country    45222 non-null object\n",
      "wage_class        45222 non-null object\n",
      "dtypes: int64(6), object(9)\n",
      "memory usage: 5.5+ MB\n"
     ]
    }
   ],
   "source": [
    "combine_set = pd.concat([train_set, test_set])\n",
    "combine_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 0], dtype=int8)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how to use numerical encoding?\n",
    "pd.Categorical(['a', 'b', 'c', 'a']).codes # start from 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in combine_set.columns:\n",
    "    if combine_set[feature].dtype == 'object':\n",
    "        combine_set[feature] = pd.Categorical(combine_set[feature]).codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 45222 entries, 0 to 16280\n",
      "Data columns (total 15 columns):\n",
      "age               45222 non-null int64\n",
      "workclass         45222 non-null int8\n",
      "fnlwgt            45222 non-null int64\n",
      "education         45222 non-null int8\n",
      "education_num     45222 non-null int64\n",
      "marital_status    45222 non-null int8\n",
      "occupation        45222 non-null int8\n",
      "relationship      45222 non-null int8\n",
      "race              45222 non-null int8\n",
      "sex               45222 non-null int8\n",
      "capital_gain      45222 non-null int64\n",
      "capital_loss      45222 non-null int64\n",
      "hours_per_week    45222 non-null int64\n",
      "native_country    45222 non-null int8\n",
      "wage_class        45222 non-null int8\n",
      "dtypes: int64(6), int8(9)\n",
      "memory usage: 2.8 MB\n"
     ]
    }
   ],
   "source": [
    "combine_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_ne = combine_set[0:train_set.shape[0]]\n",
    "test_set_ne = combine_set[train_set.shape[0]:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply one-hot encoding to the categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_set = pd.concat([train_set, test_set])\n",
    "combine_set = pd.get_dummies(combine_set) # all columns of type object or catagory will be encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 45222 entries, 0 to 16280\n",
      "Columns: 106 entries, age to wage_class_ >50K\n",
      "dtypes: int64(6), uint8(100)\n",
      "memory usage: 6.7 MB\n"
     ]
    }
   ],
   "source": [
    "combine_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_oe = combine_set[0:train_set.shape[0]]\n",
    "test_set_oe = combine_set[train_set.shape[0]:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Model Setup and Grid Search Cross-validation for model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_set_ne.pop('wage_class')\n",
    "test_y = test_set_ne.pop('wage_class')\n",
    "train_x = train_set_ne\n",
    "test_x = test_set_ne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.unique() # codes has already turned the target variable into 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for xgboost, we tune the following two parameters, both controlling the individual tree complexity\n",
    "param_grid = {'max_depth': [3, 5, 7], 'min_child_weight': [1, 3, 5]}\n",
    "# these parameters are specified \n",
    "params = {'learning_rate': 0.1, 'n_estimators': 1000, 'seed': 0, 'subsample': 0.8, 'colsample_bytree': 0.8, 'objective': 'binary:logistic'}\n",
    "# the binary logistic objective is defined by the loss function\n",
    "# L(y, f(x)) = -log(p(y|x)) = - (ylog(p(x)) + (1-y)log(1-p(x))), where y is the binary target {0, 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=1000,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=0, silent=True,\n",
       "       subsample=0.8),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'max_depth': [3, 5, 7], 'min_child_weight': [1, 3, 5]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the grid search class\n",
    "# fivefold cross-validation and use parallel training\n",
    "gs = GridSearchCV(xgb.XGBClassifier(**params), param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "gs.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_min_child_weight</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.915700</td>\n",
       "      <td>0.239664</td>\n",
       "      <td>0.867118</td>\n",
       "      <td>0.895813</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>{'max_depth': 3, 'min_child_weight': 1}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.865904</td>\n",
       "      <td>0.895105</td>\n",
       "      <td>...</td>\n",
       "      <td>0.869717</td>\n",
       "      <td>0.895603</td>\n",
       "      <td>0.868866</td>\n",
       "      <td>0.895110</td>\n",
       "      <td>0.867684</td>\n",
       "      <td>0.896647</td>\n",
       "      <td>0.270182</td>\n",
       "      <td>0.004872</td>\n",
       "      <td>0.002249</td>\n",
       "      <td>0.000686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19.442540</td>\n",
       "      <td>0.244267</td>\n",
       "      <td>0.866587</td>\n",
       "      <td>0.893343</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>{'max_depth': 3, 'min_child_weight': 3}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.861429</td>\n",
       "      <td>0.893696</td>\n",
       "      <td>...</td>\n",
       "      <td>0.867396</td>\n",
       "      <td>0.893613</td>\n",
       "      <td>0.872016</td>\n",
       "      <td>0.892831</td>\n",
       "      <td>0.866357</td>\n",
       "      <td>0.892835</td>\n",
       "      <td>0.328272</td>\n",
       "      <td>0.012941</td>\n",
       "      <td>0.003391</td>\n",
       "      <td>0.000418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.387035</td>\n",
       "      <td>0.242265</td>\n",
       "      <td>0.866587</td>\n",
       "      <td>0.891627</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>{'max_depth': 3, 'min_child_weight': 5}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.863915</td>\n",
       "      <td>0.892080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.867230</td>\n",
       "      <td>0.890961</td>\n",
       "      <td>0.871850</td>\n",
       "      <td>0.891380</td>\n",
       "      <td>0.866191</td>\n",
       "      <td>0.891592</td>\n",
       "      <td>0.271457</td>\n",
       "      <td>0.007643</td>\n",
       "      <td>0.002948</td>\n",
       "      <td>0.000437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30.380039</td>\n",
       "      <td>0.394970</td>\n",
       "      <td>0.862144</td>\n",
       "      <td>0.941375</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>{'max_depth': 5, 'min_child_weight': 1}</td>\n",
       "      <td>4</td>\n",
       "      <td>0.861760</td>\n",
       "      <td>0.941440</td>\n",
       "      <td>...</td>\n",
       "      <td>0.863584</td>\n",
       "      <td>0.942434</td>\n",
       "      <td>0.864556</td>\n",
       "      <td>0.940323</td>\n",
       "      <td>0.862046</td>\n",
       "      <td>0.941237</td>\n",
       "      <td>0.293196</td>\n",
       "      <td>0.020070</td>\n",
       "      <td>0.001969</td>\n",
       "      <td>0.000672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30.787117</td>\n",
       "      <td>0.392168</td>\n",
       "      <td>0.861614</td>\n",
       "      <td>0.930277</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>{'max_depth': 5, 'min_child_weight': 3}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.859606</td>\n",
       "      <td>0.929007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.861760</td>\n",
       "      <td>0.930872</td>\n",
       "      <td>0.863395</td>\n",
       "      <td>0.929258</td>\n",
       "      <td>0.862875</td>\n",
       "      <td>0.930173</td>\n",
       "      <td>0.576656</td>\n",
       "      <td>0.014007</td>\n",
       "      <td>0.001429</td>\n",
       "      <td>0.001117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30.012888</td>\n",
       "      <td>0.391968</td>\n",
       "      <td>0.862078</td>\n",
       "      <td>0.922634</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>{'max_depth': 5, 'min_child_weight': 5}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.859606</td>\n",
       "      <td>0.920179</td>\n",
       "      <td>...</td>\n",
       "      <td>0.861760</td>\n",
       "      <td>0.924572</td>\n",
       "      <td>0.866545</td>\n",
       "      <td>0.921177</td>\n",
       "      <td>0.861549</td>\n",
       "      <td>0.922548</td>\n",
       "      <td>0.467101</td>\n",
       "      <td>0.010252</td>\n",
       "      <td>0.002356</td>\n",
       "      <td>0.001798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>41.941632</td>\n",
       "      <td>0.675761</td>\n",
       "      <td>0.856508</td>\n",
       "      <td>0.984998</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>{'max_depth': 7, 'min_child_weight': 1}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.853638</td>\n",
       "      <td>0.984832</td>\n",
       "      <td>...</td>\n",
       "      <td>0.857616</td>\n",
       "      <td>0.984707</td>\n",
       "      <td>0.858588</td>\n",
       "      <td>0.984501</td>\n",
       "      <td>0.857569</td>\n",
       "      <td>0.985040</td>\n",
       "      <td>0.867556</td>\n",
       "      <td>0.061276</td>\n",
       "      <td>0.001834</td>\n",
       "      <td>0.000488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>41.645029</td>\n",
       "      <td>0.600810</td>\n",
       "      <td>0.855746</td>\n",
       "      <td>0.969523</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>{'max_depth': 7, 'min_child_weight': 3}</td>\n",
       "      <td>9</td>\n",
       "      <td>0.856622</td>\n",
       "      <td>0.968171</td>\n",
       "      <td>...</td>\n",
       "      <td>0.856125</td>\n",
       "      <td>0.969083</td>\n",
       "      <td>0.859582</td>\n",
       "      <td>0.968753</td>\n",
       "      <td>0.854087</td>\n",
       "      <td>0.970702</td>\n",
       "      <td>1.134703</td>\n",
       "      <td>0.075386</td>\n",
       "      <td>0.002456</td>\n",
       "      <td>0.001088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>36.127976</td>\n",
       "      <td>0.514251</td>\n",
       "      <td>0.856939</td>\n",
       "      <td>0.956021</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>{'max_depth': 7, 'min_child_weight': 5}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.858114</td>\n",
       "      <td>0.956277</td>\n",
       "      <td>...</td>\n",
       "      <td>0.859937</td>\n",
       "      <td>0.956111</td>\n",
       "      <td>0.859748</td>\n",
       "      <td>0.955077</td>\n",
       "      <td>0.856409</td>\n",
       "      <td>0.955659</td>\n",
       "      <td>0.289997</td>\n",
       "      <td>0.060916</td>\n",
       "      <td>0.003468</td>\n",
       "      <td>0.000635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0      18.915700         0.239664         0.867118          0.895813   \n",
       "1      19.442540         0.244267         0.866587          0.893343   \n",
       "2      19.387035         0.242265         0.866587          0.891627   \n",
       "3      30.380039         0.394970         0.862144          0.941375   \n",
       "4      30.787117         0.392168         0.861614          0.930277   \n",
       "5      30.012888         0.391968         0.862078          0.922634   \n",
       "6      41.941632         0.675761         0.856508          0.984998   \n",
       "7      41.645029         0.600810         0.855746          0.969523   \n",
       "8      36.127976         0.514251         0.856939          0.956021   \n",
       "\n",
       "  param_max_depth param_min_child_weight  \\\n",
       "0               3                      1   \n",
       "1               3                      3   \n",
       "2               3                      5   \n",
       "3               5                      1   \n",
       "4               5                      3   \n",
       "5               5                      5   \n",
       "6               7                      1   \n",
       "7               7                      3   \n",
       "8               7                      5   \n",
       "\n",
       "                                    params  rank_test_score  \\\n",
       "0  {'max_depth': 3, 'min_child_weight': 1}                1   \n",
       "1  {'max_depth': 3, 'min_child_weight': 3}                2   \n",
       "2  {'max_depth': 3, 'min_child_weight': 5}                2   \n",
       "3  {'max_depth': 5, 'min_child_weight': 1}                4   \n",
       "4  {'max_depth': 5, 'min_child_weight': 3}                6   \n",
       "5  {'max_depth': 5, 'min_child_weight': 5}                5   \n",
       "6  {'max_depth': 7, 'min_child_weight': 1}                8   \n",
       "7  {'max_depth': 7, 'min_child_weight': 3}                9   \n",
       "8  {'max_depth': 7, 'min_child_weight': 5}                7   \n",
       "\n",
       "   split0_test_score  split0_train_score       ...         split2_test_score  \\\n",
       "0           0.865904            0.895105       ...                  0.869717   \n",
       "1           0.861429            0.893696       ...                  0.867396   \n",
       "2           0.863915            0.892080       ...                  0.867230   \n",
       "3           0.861760            0.941440       ...                  0.863584   \n",
       "4           0.859606            0.929007       ...                  0.861760   \n",
       "5           0.859606            0.920179       ...                  0.861760   \n",
       "6           0.853638            0.984832       ...                  0.857616   \n",
       "7           0.856622            0.968171       ...                  0.856125   \n",
       "8           0.858114            0.956277       ...                  0.859937   \n",
       "\n",
       "   split2_train_score  split3_test_score  split3_train_score  \\\n",
       "0            0.895603           0.868866            0.895110   \n",
       "1            0.893613           0.872016            0.892831   \n",
       "2            0.890961           0.871850            0.891380   \n",
       "3            0.942434           0.864556            0.940323   \n",
       "4            0.930872           0.863395            0.929258   \n",
       "5            0.924572           0.866545            0.921177   \n",
       "6            0.984707           0.858588            0.984501   \n",
       "7            0.969083           0.859582            0.968753   \n",
       "8            0.956111           0.859748            0.955077   \n",
       "\n",
       "   split4_test_score  split4_train_score  std_fit_time  std_score_time  \\\n",
       "0           0.867684            0.896647      0.270182        0.004872   \n",
       "1           0.866357            0.892835      0.328272        0.012941   \n",
       "2           0.866191            0.891592      0.271457        0.007643   \n",
       "3           0.862046            0.941237      0.293196        0.020070   \n",
       "4           0.862875            0.930173      0.576656        0.014007   \n",
       "5           0.861549            0.922548      0.467101        0.010252   \n",
       "6           0.857569            0.985040      0.867556        0.061276   \n",
       "7           0.854087            0.970702      1.134703        0.075386   \n",
       "8           0.856409            0.955659      0.289997        0.060916   \n",
       "\n",
       "   std_test_score  std_train_score  \n",
       "0        0.002249         0.000686  \n",
       "1        0.003391         0.000418  \n",
       "2        0.002948         0.000437  \n",
       "3        0.001969         0.000672  \n",
       "4        0.001429         0.001117  \n",
       "5        0.002356         0.001798  \n",
       "6        0.001834         0.000488  \n",
       "7        0.002456         0.001088  \n",
       "8        0.003468         0.000635  \n",
       "\n",
       "[9 rows x 22 columns]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(gs.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86711756514819971"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to tune other parameters by cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_subsample</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>19.513421</td>\n",
       "      <td>0.233760</td>\n",
       "      <td>0.860255</td>\n",
       "      <td>0.863106</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.9</td>\n",
       "      <td>{'learning_rate': 0.01, 'subsample': 0.9}</td>\n",
       "      <td>9</td>\n",
       "      <td>0.855793</td>\n",
       "      <td>0.864810</td>\n",
       "      <td>...</td>\n",
       "      <td>0.861760</td>\n",
       "      <td>0.861950</td>\n",
       "      <td>0.864721</td>\n",
       "      <td>0.861790</td>\n",
       "      <td>0.862212</td>\n",
       "      <td>0.863039</td>\n",
       "      <td>0.347558</td>\n",
       "      <td>0.005673</td>\n",
       "      <td>0.003405</td>\n",
       "      <td>0.001155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19.763291</td>\n",
       "      <td>0.231058</td>\n",
       "      <td>0.860288</td>\n",
       "      <td>0.863512</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'learning_rate': 0.01, 'subsample': 0.8}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.856290</td>\n",
       "      <td>0.865556</td>\n",
       "      <td>...</td>\n",
       "      <td>0.861926</td>\n",
       "      <td>0.862821</td>\n",
       "      <td>0.863395</td>\n",
       "      <td>0.861956</td>\n",
       "      <td>0.862709</td>\n",
       "      <td>0.863371</td>\n",
       "      <td>0.368974</td>\n",
       "      <td>0.006979</td>\n",
       "      <td>0.002974</td>\n",
       "      <td>0.001201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19.566757</td>\n",
       "      <td>0.235861</td>\n",
       "      <td>0.860520</td>\n",
       "      <td>0.863587</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.7</td>\n",
       "      <td>{'learning_rate': 0.01, 'subsample': 0.7}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.857285</td>\n",
       "      <td>0.865224</td>\n",
       "      <td>...</td>\n",
       "      <td>0.862092</td>\n",
       "      <td>0.862116</td>\n",
       "      <td>0.863727</td>\n",
       "      <td>0.862329</td>\n",
       "      <td>0.862709</td>\n",
       "      <td>0.864200</td>\n",
       "      <td>0.390065</td>\n",
       "      <td>0.008292</td>\n",
       "      <td>0.002896</td>\n",
       "      <td>0.001186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19.037435</td>\n",
       "      <td>0.247769</td>\n",
       "      <td>0.866222</td>\n",
       "      <td>0.895108</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>{'learning_rate': 0.1, 'subsample': 0.7}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.863418</td>\n",
       "      <td>0.895769</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866236</td>\n",
       "      <td>0.894152</td>\n",
       "      <td>0.869529</td>\n",
       "      <td>0.894903</td>\n",
       "      <td>0.865362</td>\n",
       "      <td>0.895363</td>\n",
       "      <td>0.479870</td>\n",
       "      <td>0.009373</td>\n",
       "      <td>0.001983</td>\n",
       "      <td>0.000551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19.543141</td>\n",
       "      <td>0.239364</td>\n",
       "      <td>0.867118</td>\n",
       "      <td>0.895813</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'learning_rate': 0.1, 'subsample': 0.8}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.865904</td>\n",
       "      <td>0.895105</td>\n",
       "      <td>...</td>\n",
       "      <td>0.869717</td>\n",
       "      <td>0.895603</td>\n",
       "      <td>0.868866</td>\n",
       "      <td>0.895110</td>\n",
       "      <td>0.867684</td>\n",
       "      <td>0.896647</td>\n",
       "      <td>0.308657</td>\n",
       "      <td>0.009407</td>\n",
       "      <td>0.002249</td>\n",
       "      <td>0.000686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.625697</td>\n",
       "      <td>0.235661</td>\n",
       "      <td>0.867582</td>\n",
       "      <td>0.895332</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>{'learning_rate': 0.1, 'subsample': 0.9}</td>\n",
       "      <td>4</td>\n",
       "      <td>0.863086</td>\n",
       "      <td>0.895603</td>\n",
       "      <td>...</td>\n",
       "      <td>0.869717</td>\n",
       "      <td>0.896017</td>\n",
       "      <td>0.871850</td>\n",
       "      <td>0.894820</td>\n",
       "      <td>0.866523</td>\n",
       "      <td>0.894451</td>\n",
       "      <td>0.352307</td>\n",
       "      <td>0.008480</td>\n",
       "      <td>0.002994</td>\n",
       "      <td>0.000595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>19.354813</td>\n",
       "      <td>0.240964</td>\n",
       "      <td>0.868875</td>\n",
       "      <td>0.885046</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.7</td>\n",
       "      <td>{'learning_rate': 0.05, 'subsample': 0.7}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.864744</td>\n",
       "      <td>0.886154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.871706</td>\n",
       "      <td>0.884662</td>\n",
       "      <td>0.874337</td>\n",
       "      <td>0.883962</td>\n",
       "      <td>0.870337</td>\n",
       "      <td>0.885210</td>\n",
       "      <td>0.329900</td>\n",
       "      <td>0.007787</td>\n",
       "      <td>0.004211</td>\n",
       "      <td>0.000723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>19.670128</td>\n",
       "      <td>0.255174</td>\n",
       "      <td>0.869538</td>\n",
       "      <td>0.884399</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'learning_rate': 0.05, 'subsample': 0.8}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.867230</td>\n",
       "      <td>0.884910</td>\n",
       "      <td>...</td>\n",
       "      <td>0.871540</td>\n",
       "      <td>0.884123</td>\n",
       "      <td>0.873508</td>\n",
       "      <td>0.883672</td>\n",
       "      <td>0.870005</td>\n",
       "      <td>0.884630</td>\n",
       "      <td>0.315329</td>\n",
       "      <td>0.020321</td>\n",
       "      <td>0.002911</td>\n",
       "      <td>0.000445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>16.018662</td>\n",
       "      <td>0.193932</td>\n",
       "      <td>0.869637</td>\n",
       "      <td>0.884963</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.9</td>\n",
       "      <td>{'learning_rate': 0.05, 'subsample': 0.9}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.866401</td>\n",
       "      <td>0.885739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.870877</td>\n",
       "      <td>0.884206</td>\n",
       "      <td>0.874668</td>\n",
       "      <td>0.884791</td>\n",
       "      <td>0.871829</td>\n",
       "      <td>0.884630</td>\n",
       "      <td>0.674756</td>\n",
       "      <td>0.011779</td>\n",
       "      <td>0.003726</td>\n",
       "      <td>0.000557</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "5      19.513421         0.233760         0.860255          0.863106   \n",
       "4      19.763291         0.231058         0.860288          0.863512   \n",
       "3      19.566757         0.235861         0.860520          0.863587   \n",
       "0      19.037435         0.247769         0.866222          0.895108   \n",
       "1      19.543141         0.239364         0.867118          0.895813   \n",
       "2      19.625697         0.235661         0.867582          0.895332   \n",
       "6      19.354813         0.240964         0.868875          0.885046   \n",
       "7      19.670128         0.255174         0.869538          0.884399   \n",
       "8      16.018662         0.193932         0.869637          0.884963   \n",
       "\n",
       "  param_learning_rate param_subsample  \\\n",
       "5                0.01             0.9   \n",
       "4                0.01             0.8   \n",
       "3                0.01             0.7   \n",
       "0                 0.1             0.7   \n",
       "1                 0.1             0.8   \n",
       "2                 0.1             0.9   \n",
       "6                0.05             0.7   \n",
       "7                0.05             0.8   \n",
       "8                0.05             0.9   \n",
       "\n",
       "                                      params  rank_test_score  \\\n",
       "5  {'learning_rate': 0.01, 'subsample': 0.9}                9   \n",
       "4  {'learning_rate': 0.01, 'subsample': 0.8}                8   \n",
       "3  {'learning_rate': 0.01, 'subsample': 0.7}                7   \n",
       "0   {'learning_rate': 0.1, 'subsample': 0.7}                6   \n",
       "1   {'learning_rate': 0.1, 'subsample': 0.8}                5   \n",
       "2   {'learning_rate': 0.1, 'subsample': 0.9}                4   \n",
       "6  {'learning_rate': 0.05, 'subsample': 0.7}                3   \n",
       "7  {'learning_rate': 0.05, 'subsample': 0.8}                2   \n",
       "8  {'learning_rate': 0.05, 'subsample': 0.9}                1   \n",
       "\n",
       "   split0_test_score  split0_train_score       ...         split2_test_score  \\\n",
       "5           0.855793            0.864810       ...                  0.861760   \n",
       "4           0.856290            0.865556       ...                  0.861926   \n",
       "3           0.857285            0.865224       ...                  0.862092   \n",
       "0           0.863418            0.895769       ...                  0.866236   \n",
       "1           0.865904            0.895105       ...                  0.869717   \n",
       "2           0.863086            0.895603       ...                  0.869717   \n",
       "6           0.864744            0.886154       ...                  0.871706   \n",
       "7           0.867230            0.884910       ...                  0.871540   \n",
       "8           0.866401            0.885739       ...                  0.870877   \n",
       "\n",
       "   split2_train_score  split3_test_score  split3_train_score  \\\n",
       "5            0.861950           0.864721            0.861790   \n",
       "4            0.862821           0.863395            0.861956   \n",
       "3            0.862116           0.863727            0.862329   \n",
       "0            0.894152           0.869529            0.894903   \n",
       "1            0.895603           0.868866            0.895110   \n",
       "2            0.896017           0.871850            0.894820   \n",
       "6            0.884662           0.874337            0.883962   \n",
       "7            0.884123           0.873508            0.883672   \n",
       "8            0.884206           0.874668            0.884791   \n",
       "\n",
       "   split4_test_score  split4_train_score  std_fit_time  std_score_time  \\\n",
       "5           0.862212            0.863039      0.347558        0.005673   \n",
       "4           0.862709            0.863371      0.368974        0.006979   \n",
       "3           0.862709            0.864200      0.390065        0.008292   \n",
       "0           0.865362            0.895363      0.479870        0.009373   \n",
       "1           0.867684            0.896647      0.308657        0.009407   \n",
       "2           0.866523            0.894451      0.352307        0.008480   \n",
       "6           0.870337            0.885210      0.329900        0.007787   \n",
       "7           0.870005            0.884630      0.315329        0.020321   \n",
       "8           0.871829            0.884630      0.674756        0.011779   \n",
       "\n",
       "   std_test_score  std_train_score  \n",
       "5        0.003405         0.001155  \n",
       "4        0.002974         0.001201  \n",
       "3        0.002896         0.001186  \n",
       "0        0.001983         0.000551  \n",
       "1        0.002249         0.000686  \n",
       "2        0.002994         0.000595  \n",
       "6        0.004211         0.000723  \n",
       "7        0.002911         0.000445  \n",
       "8        0.003726         0.000557  \n",
       "\n",
       "[9 rows x 22 columns]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_params = {'learning_rate': [0.1, 0.01, 0.05], 'subsample': [0.7,0.8,0.9]}\n",
    "gs2 = GridSearchCV(gs.best_estimator_, cv_params, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "gs2.fit(train_x, train_y)\n",
    "# report\n",
    "pd.DataFrame(gs2.cv_results_).sort_values('mean_test_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.05, 'subsample': 0.9}"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs2.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early stopping CV\n",
    "In GBM, how many iterations shall we take?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_score': 0.5,\n",
       " 'booster': 'gbtree',\n",
       " 'colsample_bylevel': 1,\n",
       " 'colsample_bytree': 0.8,\n",
       " 'gamma': 0,\n",
       " 'learning_rate': 0.05,\n",
       " 'max_delta_step': 0,\n",
       " 'max_depth': 3,\n",
       " 'min_child_weight': 1,\n",
       " 'missing': None,\n",
       " 'n_estimators': 1000,\n",
       " 'n_jobs': 1,\n",
       " 'nthread': None,\n",
       " 'objective': 'binary:logistic',\n",
       " 'random_state': 0,\n",
       " 'reg_alpha': 0,\n",
       " 'reg_lambda': 1,\n",
       " 'scale_pos_weight': 1,\n",
       " 'seed': 0,\n",
       " 'silent': True,\n",
       " 'subsample': 0.9}"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs2.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost supports built-in cross-validation to determine early stopping\n",
    "# http://xgboost.readthedocs.io/en/latest/python/python_api.html\n",
    "xgbmat = xgb.DMatrix(train_x, train_y)\n",
    "current_params = params=gs2.best_estimator_.get_params()\n",
    "current_params['n_jobs'] = -1\n",
    "current_params['nthread'] = -1\n",
    "current_params['learning_rate'] = 0.1\n",
    "current_params['subsample'] = 0.7\n",
    "early_stopping_gbt = xgb.cv(current_params, dtrain=xgbmat, num_boost_round=1000, \n",
    "                            nfold=5, metrics=['error'], # 'error' for binary classification errors\n",
    "                            early_stopping_rounds=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test-error-mean</th>\n",
       "      <th>test-error-std</th>\n",
       "      <th>train-error-mean</th>\n",
       "      <th>train-error-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.192295</td>\n",
       "      <td>0.009347</td>\n",
       "      <td>0.190985</td>\n",
       "      <td>0.003605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.174557</td>\n",
       "      <td>0.014765</td>\n",
       "      <td>0.172038</td>\n",
       "      <td>0.011635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.166334</td>\n",
       "      <td>0.014616</td>\n",
       "      <td>0.165100</td>\n",
       "      <td>0.011414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.164643</td>\n",
       "      <td>0.014460</td>\n",
       "      <td>0.164280</td>\n",
       "      <td>0.011500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.159704</td>\n",
       "      <td>0.005398</td>\n",
       "      <td>0.159447</td>\n",
       "      <td>0.002519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.159902</td>\n",
       "      <td>0.006612</td>\n",
       "      <td>0.159348</td>\n",
       "      <td>0.002081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.158676</td>\n",
       "      <td>0.004889</td>\n",
       "      <td>0.157558</td>\n",
       "      <td>0.001623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.157748</td>\n",
       "      <td>0.003106</td>\n",
       "      <td>0.157914</td>\n",
       "      <td>0.001202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.157350</td>\n",
       "      <td>0.002752</td>\n",
       "      <td>0.157168</td>\n",
       "      <td>0.001762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.157284</td>\n",
       "      <td>0.002940</td>\n",
       "      <td>0.156546</td>\n",
       "      <td>0.002501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.156521</td>\n",
       "      <td>0.002949</td>\n",
       "      <td>0.155908</td>\n",
       "      <td>0.002286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.155825</td>\n",
       "      <td>0.003274</td>\n",
       "      <td>0.154888</td>\n",
       "      <td>0.001975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.155560</td>\n",
       "      <td>0.004396</td>\n",
       "      <td>0.155005</td>\n",
       "      <td>0.001618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.155228</td>\n",
       "      <td>0.004005</td>\n",
       "      <td>0.154416</td>\n",
       "      <td>0.001355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.155427</td>\n",
       "      <td>0.003707</td>\n",
       "      <td>0.154905</td>\n",
       "      <td>0.001971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.154665</td>\n",
       "      <td>0.004003</td>\n",
       "      <td>0.154143</td>\n",
       "      <td>0.001019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.154267</td>\n",
       "      <td>0.003691</td>\n",
       "      <td>0.153778</td>\n",
       "      <td>0.002043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.153769</td>\n",
       "      <td>0.003034</td>\n",
       "      <td>0.153322</td>\n",
       "      <td>0.002178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.154134</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>0.153521</td>\n",
       "      <td>0.002023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.154002</td>\n",
       "      <td>0.003399</td>\n",
       "      <td>0.153198</td>\n",
       "      <td>0.002328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.153803</td>\n",
       "      <td>0.003601</td>\n",
       "      <td>0.153015</td>\n",
       "      <td>0.002089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.153637</td>\n",
       "      <td>0.003673</td>\n",
       "      <td>0.152858</td>\n",
       "      <td>0.002032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.153239</td>\n",
       "      <td>0.003996</td>\n",
       "      <td>0.152593</td>\n",
       "      <td>0.002139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.153239</td>\n",
       "      <td>0.003883</td>\n",
       "      <td>0.152493</td>\n",
       "      <td>0.001982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.153372</td>\n",
       "      <td>0.004061</td>\n",
       "      <td>0.152593</td>\n",
       "      <td>0.001804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.152476</td>\n",
       "      <td>0.003512</td>\n",
       "      <td>0.152054</td>\n",
       "      <td>0.001922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.152642</td>\n",
       "      <td>0.003626</td>\n",
       "      <td>0.152054</td>\n",
       "      <td>0.001822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.152576</td>\n",
       "      <td>0.003860</td>\n",
       "      <td>0.151913</td>\n",
       "      <td>0.001857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.151946</td>\n",
       "      <td>0.004285</td>\n",
       "      <td>0.151358</td>\n",
       "      <td>0.001479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.151846</td>\n",
       "      <td>0.004752</td>\n",
       "      <td>0.150860</td>\n",
       "      <td>0.001033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>0.130926</td>\n",
       "      <td>0.004499</td>\n",
       "      <td>0.117988</td>\n",
       "      <td>0.001190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>0.130926</td>\n",
       "      <td>0.004585</td>\n",
       "      <td>0.117996</td>\n",
       "      <td>0.001128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>0.130727</td>\n",
       "      <td>0.004473</td>\n",
       "      <td>0.117905</td>\n",
       "      <td>0.001119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>0.130694</td>\n",
       "      <td>0.004503</td>\n",
       "      <td>0.117806</td>\n",
       "      <td>0.001157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>0.130727</td>\n",
       "      <td>0.004756</td>\n",
       "      <td>0.117822</td>\n",
       "      <td>0.001172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>0.130760</td>\n",
       "      <td>0.004772</td>\n",
       "      <td>0.117855</td>\n",
       "      <td>0.001156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>0.130595</td>\n",
       "      <td>0.004695</td>\n",
       "      <td>0.117888</td>\n",
       "      <td>0.001229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>0.130462</td>\n",
       "      <td>0.004722</td>\n",
       "      <td>0.117839</td>\n",
       "      <td>0.001225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>0.130462</td>\n",
       "      <td>0.004804</td>\n",
       "      <td>0.117747</td>\n",
       "      <td>0.001269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>0.130661</td>\n",
       "      <td>0.004775</td>\n",
       "      <td>0.117698</td>\n",
       "      <td>0.001204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>0.130661</td>\n",
       "      <td>0.004784</td>\n",
       "      <td>0.117648</td>\n",
       "      <td>0.001149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>0.130595</td>\n",
       "      <td>0.004751</td>\n",
       "      <td>0.117598</td>\n",
       "      <td>0.001169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>0.130661</td>\n",
       "      <td>0.004653</td>\n",
       "      <td>0.117598</td>\n",
       "      <td>0.001071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>0.130594</td>\n",
       "      <td>0.004713</td>\n",
       "      <td>0.117482</td>\n",
       "      <td>0.001122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>0.130395</td>\n",
       "      <td>0.004732</td>\n",
       "      <td>0.117548</td>\n",
       "      <td>0.001137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>0.130595</td>\n",
       "      <td>0.004759</td>\n",
       "      <td>0.117499</td>\n",
       "      <td>0.001114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>0.130661</td>\n",
       "      <td>0.004816</td>\n",
       "      <td>0.117573</td>\n",
       "      <td>0.001226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>0.130860</td>\n",
       "      <td>0.004893</td>\n",
       "      <td>0.117532</td>\n",
       "      <td>0.001203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>0.130926</td>\n",
       "      <td>0.004849</td>\n",
       "      <td>0.117607</td>\n",
       "      <td>0.001226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>0.130760</td>\n",
       "      <td>0.004802</td>\n",
       "      <td>0.117565</td>\n",
       "      <td>0.001193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>0.130661</td>\n",
       "      <td>0.004870</td>\n",
       "      <td>0.117491</td>\n",
       "      <td>0.001144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>0.130561</td>\n",
       "      <td>0.004750</td>\n",
       "      <td>0.117466</td>\n",
       "      <td>0.001246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>0.130727</td>\n",
       "      <td>0.004811</td>\n",
       "      <td>0.117408</td>\n",
       "      <td>0.001243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>0.130628</td>\n",
       "      <td>0.004527</td>\n",
       "      <td>0.117474</td>\n",
       "      <td>0.001297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>0.130661</td>\n",
       "      <td>0.004660</td>\n",
       "      <td>0.117466</td>\n",
       "      <td>0.001311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>0.130727</td>\n",
       "      <td>0.004749</td>\n",
       "      <td>0.117350</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>0.130628</td>\n",
       "      <td>0.004892</td>\n",
       "      <td>0.117341</td>\n",
       "      <td>0.001261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>0.130329</td>\n",
       "      <td>0.004649</td>\n",
       "      <td>0.117258</td>\n",
       "      <td>0.001238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>0.130329</td>\n",
       "      <td>0.004696</td>\n",
       "      <td>0.117350</td>\n",
       "      <td>0.001361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>0.130230</td>\n",
       "      <td>0.004627</td>\n",
       "      <td>0.117391</td>\n",
       "      <td>0.001343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>438 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     test-error-mean  test-error-std  train-error-mean  train-error-std\n",
       "0           0.192295        0.009347          0.190985         0.003605\n",
       "1           0.174557        0.014765          0.172038         0.011635\n",
       "2           0.166334        0.014616          0.165100         0.011414\n",
       "3           0.164643        0.014460          0.164280         0.011500\n",
       "4           0.159704        0.005398          0.159447         0.002519\n",
       "5           0.159902        0.006612          0.159348         0.002081\n",
       "6           0.158676        0.004889          0.157558         0.001623\n",
       "7           0.157748        0.003106          0.157914         0.001202\n",
       "8           0.157350        0.002752          0.157168         0.001762\n",
       "9           0.157284        0.002940          0.156546         0.002501\n",
       "10          0.156521        0.002949          0.155908         0.002286\n",
       "11          0.155825        0.003274          0.154888         0.001975\n",
       "12          0.155560        0.004396          0.155005         0.001618\n",
       "13          0.155228        0.004005          0.154416         0.001355\n",
       "14          0.155427        0.003707          0.154905         0.001971\n",
       "15          0.154665        0.004003          0.154143         0.001019\n",
       "16          0.154267        0.003691          0.153778         0.002043\n",
       "17          0.153769        0.003034          0.153322         0.002178\n",
       "18          0.154134        0.003431          0.153521         0.002023\n",
       "19          0.154002        0.003399          0.153198         0.002328\n",
       "20          0.153803        0.003601          0.153015         0.002089\n",
       "21          0.153637        0.003673          0.152858         0.002032\n",
       "22          0.153239        0.003996          0.152593         0.002139\n",
       "23          0.153239        0.003883          0.152493         0.001982\n",
       "24          0.153372        0.004061          0.152593         0.001804\n",
       "25          0.152476        0.003512          0.152054         0.001922\n",
       "26          0.152642        0.003626          0.152054         0.001822\n",
       "27          0.152576        0.003860          0.151913         0.001857\n",
       "28          0.151946        0.004285          0.151358         0.001479\n",
       "29          0.151846        0.004752          0.150860         0.001033\n",
       "..               ...             ...               ...              ...\n",
       "408         0.130926        0.004499          0.117988         0.001190\n",
       "409         0.130926        0.004585          0.117996         0.001128\n",
       "410         0.130727        0.004473          0.117905         0.001119\n",
       "411         0.130694        0.004503          0.117806         0.001157\n",
       "412         0.130727        0.004756          0.117822         0.001172\n",
       "413         0.130760        0.004772          0.117855         0.001156\n",
       "414         0.130595        0.004695          0.117888         0.001229\n",
       "415         0.130462        0.004722          0.117839         0.001225\n",
       "416         0.130462        0.004804          0.117747         0.001269\n",
       "417         0.130661        0.004775          0.117698         0.001204\n",
       "418         0.130661        0.004784          0.117648         0.001149\n",
       "419         0.130595        0.004751          0.117598         0.001169\n",
       "420         0.130661        0.004653          0.117598         0.001071\n",
       "421         0.130594        0.004713          0.117482         0.001122\n",
       "422         0.130395        0.004732          0.117548         0.001137\n",
       "423         0.130595        0.004759          0.117499         0.001114\n",
       "424         0.130661        0.004816          0.117573         0.001226\n",
       "425         0.130860        0.004893          0.117532         0.001203\n",
       "426         0.130926        0.004849          0.117607         0.001226\n",
       "427         0.130760        0.004802          0.117565         0.001193\n",
       "428         0.130661        0.004870          0.117491         0.001144\n",
       "429         0.130561        0.004750          0.117466         0.001246\n",
       "430         0.130727        0.004811          0.117408         0.001243\n",
       "431         0.130628        0.004527          0.117474         0.001297\n",
       "432         0.130661        0.004660          0.117466         0.001311\n",
       "433         0.130727        0.004749          0.117350         0.001200\n",
       "434         0.130628        0.004892          0.117341         0.001261\n",
       "435         0.130329        0.004649          0.117258         0.001238\n",
       "436         0.130329        0.004696          0.117350         0.001361\n",
       "437         0.130230        0.004627          0.117391         0.001343\n",
       "\n",
       "[438 rows x 4 columns]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stopping_gbt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "437"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stopping_gbt['test-error-mean'].idxmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we get the best set of parameters\n",
    "current_params['n_estimators'] = 437"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_gbt = xgb.train(current_params, xgbmat, 437)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dmat = xgb.DMatrix(test_x)\n",
    "from sklearn.metrics import accuracy_score\n",
    "pred_y = final_gbt.predict(test_dmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00295815,  0.20412157,  0.27673399, ...,  0.81844318,\n",
       "        0.12009671,  0.7825765 ], dtype=float32)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y # this is p(y=1|x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86872509960159361"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y[pred_y >= 0.5] = 1\n",
    "pred_y[pred_y < 0.5] = 0\n",
    "accuracy_score(pred_y, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
