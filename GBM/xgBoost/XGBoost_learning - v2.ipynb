{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn XGBoost with [A Guide to Gradient Boosted Trees with XGBoost in Python](https://jessesw.com/XG-Boost/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data', header = None)\n",
    "test_set = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test',\n",
    "                      skiprows = 1, header = None) # Make sure to skip a row for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0                  1       2           3   4                    5   \\\n",
       "0  39          State-gov   77516   Bachelors  13        Never-married   \n",
       "1  50   Self-emp-not-inc   83311   Bachelors  13   Married-civ-spouse   \n",
       "2  38            Private  215646     HS-grad   9             Divorced   \n",
       "3  53            Private  234721        11th   7   Married-civ-spouse   \n",
       "4  28            Private  338409   Bachelors  13   Married-civ-spouse   \n",
       "\n",
       "                   6               7       8        9     10  11  12  \\\n",
       "0        Adm-clerical   Not-in-family   White     Male  2174   0  40   \n",
       "1     Exec-managerial         Husband   White     Male     0   0  13   \n",
       "2   Handlers-cleaners   Not-in-family   White     Male     0   0  40   \n",
       "3   Handlers-cleaners         Husband   Black     Male     0   0  40   \n",
       "4      Prof-specialty            Wife   Black   Female     0   0  40   \n",
       "\n",
       "               13      14  \n",
       "0   United-States   <=50K  \n",
       "1   United-States   <=50K  \n",
       "2   United-States   <=50K  \n",
       "3   United-States   <=50K  \n",
       "4            Cuba   <=50K  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>Private</td>\n",
       "      <td>226802</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>89814</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>336951</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>160323</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>7688</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>?</td>\n",
       "      <td>103497</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>?</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0           1       2              3   4                    5   \\\n",
       "0  25     Private  226802           11th   7        Never-married   \n",
       "1  38     Private   89814        HS-grad   9   Married-civ-spouse   \n",
       "2  28   Local-gov  336951     Assoc-acdm  12   Married-civ-spouse   \n",
       "3  44     Private  160323   Some-college  10   Married-civ-spouse   \n",
       "4  18           ?  103497   Some-college  10        Never-married   \n",
       "\n",
       "                   6           7       8        9     10  11  12  \\\n",
       "0   Machine-op-inspct   Own-child   Black     Male     0   0  40   \n",
       "1     Farming-fishing     Husband   White     Male     0   0  50   \n",
       "2     Protective-serv     Husband   White     Male     0   0  40   \n",
       "3   Machine-op-inspct     Husband   Black     Male  7688   0  40   \n",
       "4                   ?   Own-child   White   Female     0   0  30   \n",
       "\n",
       "               13       14  \n",
       "0   United-States   <=50K.  \n",
       "1   United-States   <=50K.  \n",
       "2   United-States    >50K.  \n",
       "3   United-States    >50K.  \n",
       "4   United-States   <=50K.  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_labels = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation', \n",
    "              'relationship', 'race', 'sex', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country',\n",
    "             'wage_class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.columns = col_labels\n",
    "train_set.columns = col_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32561 entries, 0 to 32560\n",
      "Data columns (total 15 columns):\n",
      "age               32561 non-null int64\n",
      "workclass         32561 non-null object\n",
      "fnlwgt            32561 non-null int64\n",
      "education         32561 non-null object\n",
      "education_num     32561 non-null int64\n",
      "marital_status    32561 non-null object\n",
      "occupation        32561 non-null object\n",
      "relationship      32561 non-null object\n",
      "race              32561 non-null object\n",
      "sex               32561 non-null object\n",
      "capital_gain      32561 non-null int64\n",
      "capital_loss      32561 non-null int64\n",
      "hours_per_week    32561 non-null int64\n",
      "native_country    32561 non-null object\n",
      "wage_class        32561 non-null object\n",
      "dtypes: int64(6), object(9)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "train_set.info() # note that Pandas will not take '?' as nan automatically, use na_values to specify it\n",
    "# By default the following values are interpreted as NaN: ‘’, ‘#N/A’, ‘#N/A N/A’, ‘#NA’, ‘-1.#IND’, ‘-1.#QNAN’, ‘-NaN’, \n",
    "# ‘-nan’, ‘1.#IND’, ‘1.#QNAN’, ‘N/A’, ‘NA’, ‘NULL’, ‘NaN’, ‘n/a’, ‘nan’, ‘null’."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove rows containing unknown values (\" ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30162, 15) (15060, 15)\n"
     ]
    }
   ],
   "source": [
    "train_set = train_set.replace(\" ?\", np.nan).dropna()\n",
    "test_set = test_set.replace(\" ?\", np.nan).dropna()\n",
    "print(train_set.shape, test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education_num</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>native_country</th>\n",
       "      <th>wage_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>Private</td>\n",
       "      <td>226802</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>89814</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>336951</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>160323</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>7688</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>34</td>\n",
       "      <td>Private</td>\n",
       "      <td>198693</td>\n",
       "      <td>10th</td>\n",
       "      <td>6</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age   workclass  fnlwgt      education  education_num       marital_status  \\\n",
       "0   25     Private  226802           11th              7        Never-married   \n",
       "1   38     Private   89814        HS-grad              9   Married-civ-spouse   \n",
       "2   28   Local-gov  336951     Assoc-acdm             12   Married-civ-spouse   \n",
       "3   44     Private  160323   Some-college             10   Married-civ-spouse   \n",
       "5   34     Private  198693           10th              6        Never-married   \n",
       "\n",
       "           occupation    relationship    race    sex  capital_gain  \\\n",
       "0   Machine-op-inspct       Own-child   Black   Male             0   \n",
       "1     Farming-fishing         Husband   White   Male             0   \n",
       "2     Protective-serv         Husband   White   Male             0   \n",
       "3   Machine-op-inspct         Husband   Black   Male          7688   \n",
       "5       Other-service   Not-in-family   White   Male             0   \n",
       "\n",
       "   capital_loss  hours_per_week  native_country wage_class  \n",
       "0             0              40   United-States     <=50K.  \n",
       "1             0              50   United-States     <=50K.  \n",
       "2             0              40   United-States      >50K.  \n",
       "3             0              40   United-States      >50K.  \n",
       "5             0              30   United-States     <=50K.  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' <=50K', ' >50K'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note that the wage_class in the test set has an additional dot after its value\n",
    "test_set['wage_class'] = test_set['wage_class'].replace({' <=50K.': ' <=50K', ' >50K.': ' >50K'})\n",
    "test_set['wage_class'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Ordinal Encoding to Categoricals\n",
    "All called numeric encoding. That is, assign a unique number to each category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_set = pd.concat([train_set, test_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in combine_set.columns:\n",
    "    if combine_set[feature].dtype == 'object': # 'category'\n",
    "        combine_set[feature] = pd.Categorical(combine_set[feature]).codes # code each category from 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = combine_set[0:train_set.shape[0]]\n",
    "test_set = combine_set[train_set.shape[0]:]\n",
    "train_y = train_set.pop('wage_class')\n",
    "test_y = test_set.pop('wage_class')\n",
    "train_X = train_set;\n",
    "test_X = test_set;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    22654\n",
       "1     7508\n",
       "Name: wage_class, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the class balance\n",
    "train_y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    11360\n",
       "1     3700\n",
       "Name: wage_class, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y.value_counts() # as we can see, the training and test set are imbalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost: parameter tuning\n",
    "Reference: https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. learning rate and number of estimators\n",
    "These two are usually coupled, i.e., a small learning rate (shrinkage) requires more trees.\n",
    "We do a grid search coarsely to identify a proper pair of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_score': 0.5,\n",
       " 'booster': 'gbtree',\n",
       " 'colsample_bylevel': 1,\n",
       " 'colsample_bytree': 1,\n",
       " 'gamma': 0,\n",
       " 'learning_rate': 0.1,\n",
       " 'max_delta_step': 0,\n",
       " 'max_depth': 3,\n",
       " 'min_child_weight': 1,\n",
       " 'missing': None,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': 1,\n",
       " 'nthread': None,\n",
       " 'objective': 'binary:logistic',\n",
       " 'random_state': 0,\n",
       " 'reg_alpha': 0,\n",
       " 'reg_lambda': 1,\n",
       " 'scale_pos_weight': 1,\n",
       " 'seed': None,\n",
       " 'silent': True,\n",
       " 'subsample': 1}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# defaut values for the parameters\n",
    "xgb.XGBClassifier().get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'learning_rate': [0.005, 0.01, 0.05, 0.1, 0.3], 'n_estimators': [100, 500, 1000, 2000]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'learning_rate': [0.005, 0.01, 0.05, 0.1, 0.3], 'n_estimators': [100, 500, 1000, 2000]}\n",
    "gs = GridSearchCV(xgb.XGBClassifier(), param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "gs.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>24.967986</td>\n",
       "      <td>0.283092</td>\n",
       "      <td>0.869140</td>\n",
       "      <td>0.882808</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'learning_rate': 0.05, 'n_estimators': 1000}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.863252</td>\n",
       "      <td>0.883294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.870545</td>\n",
       "      <td>0.883128</td>\n",
       "      <td>0.875332</td>\n",
       "      <td>0.881434</td>\n",
       "      <td>0.871663</td>\n",
       "      <td>0.881853</td>\n",
       "      <td>1.653300</td>\n",
       "      <td>0.018144</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>0.001045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>46.909326</td>\n",
       "      <td>0.476222</td>\n",
       "      <td>0.868842</td>\n",
       "      <td>0.892895</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2000</td>\n",
       "      <td>{'learning_rate': 0.05, 'n_estimators': 2000}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.864412</td>\n",
       "      <td>0.893448</td>\n",
       "      <td>...</td>\n",
       "      <td>0.870380</td>\n",
       "      <td>0.892785</td>\n",
       "      <td>0.871353</td>\n",
       "      <td>0.892996</td>\n",
       "      <td>0.869176</td>\n",
       "      <td>0.892213</td>\n",
       "      <td>1.063263</td>\n",
       "      <td>0.029266</td>\n",
       "      <td>0.002383</td>\n",
       "      <td>0.000403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>11.577430</td>\n",
       "      <td>0.112976</td>\n",
       "      <td>0.868808</td>\n",
       "      <td>0.883313</td>\n",
       "      <td>0.1</td>\n",
       "      <td>500</td>\n",
       "      <td>{'learning_rate': 0.1, 'n_estimators': 500}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.863086</td>\n",
       "      <td>0.883626</td>\n",
       "      <td>...</td>\n",
       "      <td>0.869882</td>\n",
       "      <td>0.883584</td>\n",
       "      <td>0.875332</td>\n",
       "      <td>0.882719</td>\n",
       "      <td>0.870171</td>\n",
       "      <td>0.883055</td>\n",
       "      <td>0.232400</td>\n",
       "      <td>0.006943</td>\n",
       "      <td>0.004214</td>\n",
       "      <td>0.000365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>22.808727</td>\n",
       "      <td>0.252971</td>\n",
       "      <td>0.867814</td>\n",
       "      <td>0.893550</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'learning_rate': 0.1, 'n_estimators': 1000}</td>\n",
       "      <td>4</td>\n",
       "      <td>0.865241</td>\n",
       "      <td>0.893945</td>\n",
       "      <td>...</td>\n",
       "      <td>0.869385</td>\n",
       "      <td>0.893945</td>\n",
       "      <td>0.870690</td>\n",
       "      <td>0.893618</td>\n",
       "      <td>0.868513</td>\n",
       "      <td>0.892959</td>\n",
       "      <td>0.847640</td>\n",
       "      <td>0.018229</td>\n",
       "      <td>0.002212</td>\n",
       "      <td>0.000384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.433247</td>\n",
       "      <td>0.023116</td>\n",
       "      <td>0.867714</td>\n",
       "      <td>0.878299</td>\n",
       "      <td>0.3</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.3, 'n_estimators': 100}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.861429</td>\n",
       "      <td>0.879315</td>\n",
       "      <td>...</td>\n",
       "      <td>0.869551</td>\n",
       "      <td>0.877492</td>\n",
       "      <td>0.874005</td>\n",
       "      <td>0.876212</td>\n",
       "      <td>0.869508</td>\n",
       "      <td>0.879077</td>\n",
       "      <td>0.037339</td>\n",
       "      <td>0.003218</td>\n",
       "      <td>0.004447</td>\n",
       "      <td>0.001253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13.003094</td>\n",
       "      <td>0.130388</td>\n",
       "      <td>0.867449</td>\n",
       "      <td>0.874909</td>\n",
       "      <td>0.05</td>\n",
       "      <td>500</td>\n",
       "      <td>{'learning_rate': 0.05, 'n_estimators': 500}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.861926</td>\n",
       "      <td>0.875254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.868722</td>\n",
       "      <td>0.874715</td>\n",
       "      <td>0.873011</td>\n",
       "      <td>0.873311</td>\n",
       "      <td>0.869673</td>\n",
       "      <td>0.875098</td>\n",
       "      <td>0.944458</td>\n",
       "      <td>0.013712</td>\n",
       "      <td>0.004012</td>\n",
       "      <td>0.000930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>50.139010</td>\n",
       "      <td>0.556176</td>\n",
       "      <td>0.865924</td>\n",
       "      <td>0.872629</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>{'learning_rate': 0.01, 'n_estimators': 2000}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.860434</td>\n",
       "      <td>0.873513</td>\n",
       "      <td>...</td>\n",
       "      <td>0.868391</td>\n",
       "      <td>0.872601</td>\n",
       "      <td>0.871850</td>\n",
       "      <td>0.870908</td>\n",
       "      <td>0.867186</td>\n",
       "      <td>0.872032</td>\n",
       "      <td>3.399280</td>\n",
       "      <td>0.048046</td>\n",
       "      <td>0.004249</td>\n",
       "      <td>0.001118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>11.278328</td>\n",
       "      <td>0.109574</td>\n",
       "      <td>0.865195</td>\n",
       "      <td>0.902369</td>\n",
       "      <td>0.3</td>\n",
       "      <td>500</td>\n",
       "      <td>{'learning_rate': 0.3, 'n_estimators': 500}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.861263</td>\n",
       "      <td>0.903021</td>\n",
       "      <td>...</td>\n",
       "      <td>0.868888</td>\n",
       "      <td>0.902482</td>\n",
       "      <td>0.868037</td>\n",
       "      <td>0.902487</td>\n",
       "      <td>0.864367</td>\n",
       "      <td>0.900543</td>\n",
       "      <td>0.374753</td>\n",
       "      <td>0.007104</td>\n",
       "      <td>0.002864</td>\n",
       "      <td>0.000967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>45.138633</td>\n",
       "      <td>0.465014</td>\n",
       "      <td>0.864764</td>\n",
       "      <td>0.907342</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2000</td>\n",
       "      <td>{'learning_rate': 0.1, 'n_estimators': 2000}</td>\n",
       "      <td>9</td>\n",
       "      <td>0.860766</td>\n",
       "      <td>0.907995</td>\n",
       "      <td>...</td>\n",
       "      <td>0.867064</td>\n",
       "      <td>0.907373</td>\n",
       "      <td>0.865550</td>\n",
       "      <td>0.906216</td>\n",
       "      <td>0.865528</td>\n",
       "      <td>0.907339</td>\n",
       "      <td>1.377181</td>\n",
       "      <td>0.025230</td>\n",
       "      <td>0.002121</td>\n",
       "      <td>0.000615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>22.784109</td>\n",
       "      <td>0.232957</td>\n",
       "      <td>0.860652</td>\n",
       "      <td>0.920761</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'learning_rate': 0.3, 'n_estimators': 1000}</td>\n",
       "      <td>10</td>\n",
       "      <td>0.858611</td>\n",
       "      <td>0.920842</td>\n",
       "      <td>...</td>\n",
       "      <td>0.864578</td>\n",
       "      <td>0.919889</td>\n",
       "      <td>0.859748</td>\n",
       "      <td>0.920431</td>\n",
       "      <td>0.860222</td>\n",
       "      <td>0.919605</td>\n",
       "      <td>0.509489</td>\n",
       "      <td>0.012887</td>\n",
       "      <td>0.002044</td>\n",
       "      <td>0.001216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.448956</td>\n",
       "      <td>0.024116</td>\n",
       "      <td>0.860619</td>\n",
       "      <td>0.863114</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.1, 'n_estimators': 100}</td>\n",
       "      <td>11</td>\n",
       "      <td>0.855959</td>\n",
       "      <td>0.864851</td>\n",
       "      <td>...</td>\n",
       "      <td>0.862258</td>\n",
       "      <td>0.862116</td>\n",
       "      <td>0.865385</td>\n",
       "      <td>0.862370</td>\n",
       "      <td>0.862709</td>\n",
       "      <td>0.863122</td>\n",
       "      <td>0.144689</td>\n",
       "      <td>0.001595</td>\n",
       "      <td>0.003638</td>\n",
       "      <td>0.000956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>26.297285</td>\n",
       "      <td>0.271484</td>\n",
       "      <td>0.860288</td>\n",
       "      <td>0.863056</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'learning_rate': 0.01, 'n_estimators': 1000}</td>\n",
       "      <td>12</td>\n",
       "      <td>0.855462</td>\n",
       "      <td>0.865100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.862423</td>\n",
       "      <td>0.862199</td>\n",
       "      <td>0.863395</td>\n",
       "      <td>0.861707</td>\n",
       "      <td>0.862046</td>\n",
       "      <td>0.862169</td>\n",
       "      <td>1.534673</td>\n",
       "      <td>0.022986</td>\n",
       "      <td>0.003011</td>\n",
       "      <td>0.001313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>51.941529</td>\n",
       "      <td>0.554675</td>\n",
       "      <td>0.860122</td>\n",
       "      <td>0.863164</td>\n",
       "      <td>0.005</td>\n",
       "      <td>2000</td>\n",
       "      <td>{'learning_rate': 0.005, 'n_estimators': 2000}</td>\n",
       "      <td>13</td>\n",
       "      <td>0.855296</td>\n",
       "      <td>0.865059</td>\n",
       "      <td>...</td>\n",
       "      <td>0.862423</td>\n",
       "      <td>0.862448</td>\n",
       "      <td>0.863064</td>\n",
       "      <td>0.861666</td>\n",
       "      <td>0.861880</td>\n",
       "      <td>0.862210</td>\n",
       "      <td>3.010313</td>\n",
       "      <td>0.041009</td>\n",
       "      <td>0.003002</td>\n",
       "      <td>0.001332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>37.723076</td>\n",
       "      <td>0.386261</td>\n",
       "      <td>0.855878</td>\n",
       "      <td>0.946108</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2000</td>\n",
       "      <td>{'learning_rate': 0.3, 'n_estimators': 2000}</td>\n",
       "      <td>14</td>\n",
       "      <td>0.851981</td>\n",
       "      <td>0.947200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.858445</td>\n",
       "      <td>0.945667</td>\n",
       "      <td>0.855272</td>\n",
       "      <td>0.944426</td>\n",
       "      <td>0.859227</td>\n",
       "      <td>0.945174</td>\n",
       "      <td>2.161666</td>\n",
       "      <td>0.022943</td>\n",
       "      <td>0.002659</td>\n",
       "      <td>0.001337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.485281</td>\n",
       "      <td>0.022415</td>\n",
       "      <td>0.850839</td>\n",
       "      <td>0.852571</td>\n",
       "      <td>0.05</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.05, 'n_estimators': 100}</td>\n",
       "      <td>15</td>\n",
       "      <td>0.846511</td>\n",
       "      <td>0.855485</td>\n",
       "      <td>...</td>\n",
       "      <td>0.854964</td>\n",
       "      <td>0.853164</td>\n",
       "      <td>0.853282</td>\n",
       "      <td>0.849896</td>\n",
       "      <td>0.853424</td>\n",
       "      <td>0.851767</td>\n",
       "      <td>0.162744</td>\n",
       "      <td>0.001961</td>\n",
       "      <td>0.003787</td>\n",
       "      <td>0.001824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13.508436</td>\n",
       "      <td>0.133791</td>\n",
       "      <td>0.850308</td>\n",
       "      <td>0.851809</td>\n",
       "      <td>0.01</td>\n",
       "      <td>500</td>\n",
       "      <td>{'learning_rate': 0.01, 'n_estimators': 500}</td>\n",
       "      <td>16</td>\n",
       "      <td>0.844356</td>\n",
       "      <td>0.853330</td>\n",
       "      <td>...</td>\n",
       "      <td>0.853804</td>\n",
       "      <td>0.850760</td>\n",
       "      <td>0.852785</td>\n",
       "      <td>0.850021</td>\n",
       "      <td>0.855082</td>\n",
       "      <td>0.852596</td>\n",
       "      <td>0.796570</td>\n",
       "      <td>0.015662</td>\n",
       "      <td>0.004462</td>\n",
       "      <td>0.001225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25.751916</td>\n",
       "      <td>0.247267</td>\n",
       "      <td>0.850209</td>\n",
       "      <td>0.851659</td>\n",
       "      <td>0.005</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'learning_rate': 0.005, 'n_estimators': 1000}</td>\n",
       "      <td>17</td>\n",
       "      <td>0.844025</td>\n",
       "      <td>0.852916</td>\n",
       "      <td>...</td>\n",
       "      <td>0.853804</td>\n",
       "      <td>0.850926</td>\n",
       "      <td>0.852785</td>\n",
       "      <td>0.850104</td>\n",
       "      <td>0.854253</td>\n",
       "      <td>0.851685</td>\n",
       "      <td>1.393011</td>\n",
       "      <td>0.012980</td>\n",
       "      <td>0.004252</td>\n",
       "      <td>0.001054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.157599</td>\n",
       "      <td>0.115378</td>\n",
       "      <td>0.843412</td>\n",
       "      <td>0.844291</td>\n",
       "      <td>0.005</td>\n",
       "      <td>500</td>\n",
       "      <td>{'learning_rate': 0.005, 'n_estimators': 500}</td>\n",
       "      <td>18</td>\n",
       "      <td>0.836897</td>\n",
       "      <td>0.846326</td>\n",
       "      <td>...</td>\n",
       "      <td>0.846511</td>\n",
       "      <td>0.843259</td>\n",
       "      <td>0.846817</td>\n",
       "      <td>0.843556</td>\n",
       "      <td>0.845963</td>\n",
       "      <td>0.843396</td>\n",
       "      <td>0.515741</td>\n",
       "      <td>0.012380</td>\n",
       "      <td>0.003914</td>\n",
       "      <td>0.001178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.733549</td>\n",
       "      <td>0.018913</td>\n",
       "      <td>0.837677</td>\n",
       "      <td>0.837718</td>\n",
       "      <td>0.01</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.01, 'n_estimators': 100}</td>\n",
       "      <td>19</td>\n",
       "      <td>0.829770</td>\n",
       "      <td>0.838825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.843527</td>\n",
       "      <td>0.839488</td>\n",
       "      <td>0.841844</td>\n",
       "      <td>0.834811</td>\n",
       "      <td>0.838004</td>\n",
       "      <td>0.836517</td>\n",
       "      <td>0.173384</td>\n",
       "      <td>0.003385</td>\n",
       "      <td>0.004902</td>\n",
       "      <td>0.001775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.855760</td>\n",
       "      <td>0.015811</td>\n",
       "      <td>0.835787</td>\n",
       "      <td>0.835886</td>\n",
       "      <td>0.005</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.005, 'n_estimators': 100}</td>\n",
       "      <td>20</td>\n",
       "      <td>0.828775</td>\n",
       "      <td>0.837664</td>\n",
       "      <td>...</td>\n",
       "      <td>0.839052</td>\n",
       "      <td>0.835095</td>\n",
       "      <td>0.841678</td>\n",
       "      <td>0.834438</td>\n",
       "      <td>0.836511</td>\n",
       "      <td>0.835606</td>\n",
       "      <td>0.151926</td>\n",
       "      <td>0.001861</td>\n",
       "      <td>0.004545</td>\n",
       "      <td>0.001141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "10      24.967986         0.283092         0.869140          0.882808   \n",
       "11      46.909326         0.476222         0.868842          0.892895   \n",
       "13      11.577430         0.112976         0.868808          0.883313   \n",
       "14      22.808727         0.252971         0.867814          0.893550   \n",
       "16       2.433247         0.023116         0.867714          0.878299   \n",
       "9       13.003094         0.130388         0.867449          0.874909   \n",
       "7       50.139010         0.556176         0.865924          0.872629   \n",
       "17      11.278328         0.109574         0.865195          0.902369   \n",
       "15      45.138633         0.465014         0.864764          0.907342   \n",
       "18      22.784109         0.232957         0.860652          0.920761   \n",
       "12       2.448956         0.024116         0.860619          0.863114   \n",
       "6       26.297285         0.271484         0.860288          0.863056   \n",
       "3       51.941529         0.554675         0.860122          0.863164   \n",
       "19      37.723076         0.386261         0.855878          0.946108   \n",
       "8        2.485281         0.022415         0.850839          0.852571   \n",
       "5       13.508436         0.133791         0.850308          0.851809   \n",
       "2       25.751916         0.247267         0.850209          0.851659   \n",
       "1       13.157599         0.115378         0.843412          0.844291   \n",
       "4        2.733549         0.018913         0.837677          0.837718   \n",
       "0        1.855760         0.015811         0.835787          0.835886   \n",
       "\n",
       "   param_learning_rate param_n_estimators  \\\n",
       "10                0.05               1000   \n",
       "11                0.05               2000   \n",
       "13                 0.1                500   \n",
       "14                 0.1               1000   \n",
       "16                 0.3                100   \n",
       "9                 0.05                500   \n",
       "7                 0.01               2000   \n",
       "17                 0.3                500   \n",
       "15                 0.1               2000   \n",
       "18                 0.3               1000   \n",
       "12                 0.1                100   \n",
       "6                 0.01               1000   \n",
       "3                0.005               2000   \n",
       "19                 0.3               2000   \n",
       "8                 0.05                100   \n",
       "5                 0.01                500   \n",
       "2                0.005               1000   \n",
       "1                0.005                500   \n",
       "4                 0.01                100   \n",
       "0                0.005                100   \n",
       "\n",
       "                                            params  rank_test_score  \\\n",
       "10   {'learning_rate': 0.05, 'n_estimators': 1000}                1   \n",
       "11   {'learning_rate': 0.05, 'n_estimators': 2000}                2   \n",
       "13     {'learning_rate': 0.1, 'n_estimators': 500}                3   \n",
       "14    {'learning_rate': 0.1, 'n_estimators': 1000}                4   \n",
       "16     {'learning_rate': 0.3, 'n_estimators': 100}                5   \n",
       "9     {'learning_rate': 0.05, 'n_estimators': 500}                6   \n",
       "7    {'learning_rate': 0.01, 'n_estimators': 2000}                7   \n",
       "17     {'learning_rate': 0.3, 'n_estimators': 500}                8   \n",
       "15    {'learning_rate': 0.1, 'n_estimators': 2000}                9   \n",
       "18    {'learning_rate': 0.3, 'n_estimators': 1000}               10   \n",
       "12     {'learning_rate': 0.1, 'n_estimators': 100}               11   \n",
       "6    {'learning_rate': 0.01, 'n_estimators': 1000}               12   \n",
       "3   {'learning_rate': 0.005, 'n_estimators': 2000}               13   \n",
       "19    {'learning_rate': 0.3, 'n_estimators': 2000}               14   \n",
       "8     {'learning_rate': 0.05, 'n_estimators': 100}               15   \n",
       "5     {'learning_rate': 0.01, 'n_estimators': 500}               16   \n",
       "2   {'learning_rate': 0.005, 'n_estimators': 1000}               17   \n",
       "1    {'learning_rate': 0.005, 'n_estimators': 500}               18   \n",
       "4     {'learning_rate': 0.01, 'n_estimators': 100}               19   \n",
       "0    {'learning_rate': 0.005, 'n_estimators': 100}               20   \n",
       "\n",
       "    split0_test_score  split0_train_score       ...         split2_test_score  \\\n",
       "10           0.863252            0.883294       ...                  0.870545   \n",
       "11           0.864412            0.893448       ...                  0.870380   \n",
       "13           0.863086            0.883626       ...                  0.869882   \n",
       "14           0.865241            0.893945       ...                  0.869385   \n",
       "16           0.861429            0.879315       ...                  0.869551   \n",
       "9            0.861926            0.875254       ...                  0.868722   \n",
       "7            0.860434            0.873513       ...                  0.868391   \n",
       "17           0.861263            0.903021       ...                  0.868888   \n",
       "15           0.860766            0.907995       ...                  0.867064   \n",
       "18           0.858611            0.920842       ...                  0.864578   \n",
       "12           0.855959            0.864851       ...                  0.862258   \n",
       "6            0.855462            0.865100       ...                  0.862423   \n",
       "3            0.855296            0.865059       ...                  0.862423   \n",
       "19           0.851981            0.947200       ...                  0.858445   \n",
       "8            0.846511            0.855485       ...                  0.854964   \n",
       "5            0.844356            0.853330       ...                  0.853804   \n",
       "2            0.844025            0.852916       ...                  0.853804   \n",
       "1            0.836897            0.846326       ...                  0.846511   \n",
       "4            0.829770            0.838825       ...                  0.843527   \n",
       "0            0.828775            0.837664       ...                  0.839052   \n",
       "\n",
       "    split2_train_score  split3_test_score  split3_train_score  \\\n",
       "10            0.883128           0.875332            0.881434   \n",
       "11            0.892785           0.871353            0.892996   \n",
       "13            0.883584           0.875332            0.882719   \n",
       "14            0.893945           0.870690            0.893618   \n",
       "16            0.877492           0.874005            0.876212   \n",
       "9             0.874715           0.873011            0.873311   \n",
       "7             0.872601           0.871850            0.870908   \n",
       "17            0.902482           0.868037            0.902487   \n",
       "15            0.907373           0.865550            0.906216   \n",
       "18            0.919889           0.859748            0.920431   \n",
       "12            0.862116           0.865385            0.862370   \n",
       "6             0.862199           0.863395            0.861707   \n",
       "3             0.862448           0.863064            0.861666   \n",
       "19            0.945667           0.855272            0.944426   \n",
       "8             0.853164           0.853282            0.849896   \n",
       "5             0.850760           0.852785            0.850021   \n",
       "2             0.850926           0.852785            0.850104   \n",
       "1             0.843259           0.846817            0.843556   \n",
       "4             0.839488           0.841844            0.834811   \n",
       "0             0.835095           0.841678            0.834438   \n",
       "\n",
       "    split4_test_score  split4_train_score  std_fit_time  std_score_time  \\\n",
       "10           0.871663            0.881853      1.653300        0.018144   \n",
       "11           0.869176            0.892213      1.063263        0.029266   \n",
       "13           0.870171            0.883055      0.232400        0.006943   \n",
       "14           0.868513            0.892959      0.847640        0.018229   \n",
       "16           0.869508            0.879077      0.037339        0.003218   \n",
       "9            0.869673            0.875098      0.944458        0.013712   \n",
       "7            0.867186            0.872032      3.399280        0.048046   \n",
       "17           0.864367            0.900543      0.374753        0.007104   \n",
       "15           0.865528            0.907339      1.377181        0.025230   \n",
       "18           0.860222            0.919605      0.509489        0.012887   \n",
       "12           0.862709            0.863122      0.144689        0.001595   \n",
       "6            0.862046            0.862169      1.534673        0.022986   \n",
       "3            0.861880            0.862210      3.010313        0.041009   \n",
       "19           0.859227            0.945174      2.161666        0.022943   \n",
       "8            0.853424            0.851767      0.162744        0.001961   \n",
       "5            0.855082            0.852596      0.796570        0.015662   \n",
       "2            0.854253            0.851685      1.393011        0.012980   \n",
       "1            0.845963            0.843396      0.515741        0.012380   \n",
       "4            0.838004            0.836517      0.173384        0.003385   \n",
       "0            0.836511            0.835606      0.151926        0.001861   \n",
       "\n",
       "    std_test_score  std_train_score  \n",
       "10        0.004455         0.001045  \n",
       "11        0.002383         0.000403  \n",
       "13        0.004214         0.000365  \n",
       "14        0.002212         0.000384  \n",
       "16        0.004447         0.001253  \n",
       "9         0.004012         0.000930  \n",
       "7         0.004249         0.001118  \n",
       "17        0.002864         0.000967  \n",
       "15        0.002121         0.000615  \n",
       "18        0.002044         0.001216  \n",
       "12        0.003638         0.000956  \n",
       "6         0.003011         0.001313  \n",
       "3         0.003002         0.001332  \n",
       "19        0.002659         0.001337  \n",
       "8         0.003787         0.001824  \n",
       "5         0.004462         0.001225  \n",
       "2         0.004252         0.001054  \n",
       "1         0.003914         0.001178  \n",
       "4         0.004902         0.001775  \n",
       "0         0.004545         0.001141  \n",
       "\n",
       "[20 rows x 22 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(gs.cv_results_).sort_values('mean_test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best mean_test_score picks learning_rate=0.05 and n_estimators=1000. Since neither lies on the boundary of our grid search, they are chosen as the best parameters\n",
    "## 2. Control individual booster (decision tree) complexity\n",
    "max_depth, min_child_weight and gamma\n",
    "Because generally boosting decreases bias, we should choose a *weak* base learner, for instance, a shallow decision tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.05, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=1000,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'max_depth': [1, 2, 3, 4, 5], 'min_child_weight': [0.1, 1, 1.5, 5]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'max_depth': [1, 2, 3, 4, 5], 'min_child_weight': [0.1, 1, 1.5, 5]}\n",
    "gs2 = GridSearchCV(gs.best_estimator_, param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "gs2.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_min_child_weight</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>22.298040</td>\n",
       "      <td>0.240561</td>\n",
       "      <td>0.869903</td>\n",
       "      <td>0.883173</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'max_depth': 3, 'min_child_weight': 0.1}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.864910</td>\n",
       "      <td>0.884537</td>\n",
       "      <td>...</td>\n",
       "      <td>0.871706</td>\n",
       "      <td>0.882631</td>\n",
       "      <td>0.876492</td>\n",
       "      <td>0.882180</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>0.882102</td>\n",
       "      <td>0.558747</td>\n",
       "      <td>0.003429</td>\n",
       "      <td>0.004313</td>\n",
       "      <td>0.001079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>29.884425</td>\n",
       "      <td>0.316512</td>\n",
       "      <td>0.869836</td>\n",
       "      <td>0.893997</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>{'max_depth': 4, 'min_child_weight': 1}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.864910</td>\n",
       "      <td>0.894857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.871043</td>\n",
       "      <td>0.893986</td>\n",
       "      <td>0.873840</td>\n",
       "      <td>0.893162</td>\n",
       "      <td>0.870337</td>\n",
       "      <td>0.893954</td>\n",
       "      <td>0.564917</td>\n",
       "      <td>0.013850</td>\n",
       "      <td>0.002919</td>\n",
       "      <td>0.000537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>29.581726</td>\n",
       "      <td>0.297499</td>\n",
       "      <td>0.869505</td>\n",
       "      <td>0.893674</td>\n",
       "      <td>4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>{'max_depth': 4, 'min_child_weight': 1.5}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.863584</td>\n",
       "      <td>0.894898</td>\n",
       "      <td>...</td>\n",
       "      <td>0.871706</td>\n",
       "      <td>0.892702</td>\n",
       "      <td>0.873674</td>\n",
       "      <td>0.893121</td>\n",
       "      <td>0.870337</td>\n",
       "      <td>0.893581</td>\n",
       "      <td>0.850701</td>\n",
       "      <td>0.013022</td>\n",
       "      <td>0.003453</td>\n",
       "      <td>0.000764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>30.923218</td>\n",
       "      <td>0.318514</td>\n",
       "      <td>0.869405</td>\n",
       "      <td>0.894337</td>\n",
       "      <td>4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'max_depth': 4, 'min_child_weight': 0.1}</td>\n",
       "      <td>4</td>\n",
       "      <td>0.865407</td>\n",
       "      <td>0.894525</td>\n",
       "      <td>...</td>\n",
       "      <td>0.871208</td>\n",
       "      <td>0.893448</td>\n",
       "      <td>0.871684</td>\n",
       "      <td>0.894861</td>\n",
       "      <td>0.870337</td>\n",
       "      <td>0.893995</td>\n",
       "      <td>0.593056</td>\n",
       "      <td>0.017686</td>\n",
       "      <td>0.002295</td>\n",
       "      <td>0.000546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>23.050348</td>\n",
       "      <td>0.247366</td>\n",
       "      <td>0.869140</td>\n",
       "      <td>0.882808</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>{'max_depth': 3, 'min_child_weight': 1}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.863252</td>\n",
       "      <td>0.883294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.870545</td>\n",
       "      <td>0.883128</td>\n",
       "      <td>0.875332</td>\n",
       "      <td>0.881434</td>\n",
       "      <td>0.871663</td>\n",
       "      <td>0.881853</td>\n",
       "      <td>0.658974</td>\n",
       "      <td>0.021482</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>0.001045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>31.165689</td>\n",
       "      <td>0.325518</td>\n",
       "      <td>0.869074</td>\n",
       "      <td>0.889165</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>{'max_depth': 4, 'min_child_weight': 5}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.862921</td>\n",
       "      <td>0.890091</td>\n",
       "      <td>...</td>\n",
       "      <td>0.871374</td>\n",
       "      <td>0.888599</td>\n",
       "      <td>0.874337</td>\n",
       "      <td>0.888976</td>\n",
       "      <td>0.870005</td>\n",
       "      <td>0.888401</td>\n",
       "      <td>1.168402</td>\n",
       "      <td>0.017469</td>\n",
       "      <td>0.003929</td>\n",
       "      <td>0.000656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>36.316131</td>\n",
       "      <td>0.388260</td>\n",
       "      <td>0.868941</td>\n",
       "      <td>0.896683</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>{'max_depth': 5, 'min_child_weight': 5}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.864578</td>\n",
       "      <td>0.897012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.869054</td>\n",
       "      <td>0.896349</td>\n",
       "      <td>0.873840</td>\n",
       "      <td>0.895814</td>\n",
       "      <td>0.870005</td>\n",
       "      <td>0.896440</td>\n",
       "      <td>1.814589</td>\n",
       "      <td>0.029364</td>\n",
       "      <td>0.003069</td>\n",
       "      <td>0.000675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>23.154615</td>\n",
       "      <td>0.244064</td>\n",
       "      <td>0.868908</td>\n",
       "      <td>0.882609</td>\n",
       "      <td>3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>{'max_depth': 3, 'min_child_weight': 1.5}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.863086</td>\n",
       "      <td>0.883170</td>\n",
       "      <td>...</td>\n",
       "      <td>0.871374</td>\n",
       "      <td>0.882672</td>\n",
       "      <td>0.874337</td>\n",
       "      <td>0.881475</td>\n",
       "      <td>0.870337</td>\n",
       "      <td>0.880942</td>\n",
       "      <td>0.907110</td>\n",
       "      <td>0.008874</td>\n",
       "      <td>0.004093</td>\n",
       "      <td>0.001351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>37.305093</td>\n",
       "      <td>0.412677</td>\n",
       "      <td>0.868808</td>\n",
       "      <td>0.907599</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>{'max_depth': 5, 'min_child_weight': 1}</td>\n",
       "      <td>9</td>\n",
       "      <td>0.864247</td>\n",
       "      <td>0.907497</td>\n",
       "      <td>...</td>\n",
       "      <td>0.869551</td>\n",
       "      <td>0.907497</td>\n",
       "      <td>0.873011</td>\n",
       "      <td>0.907004</td>\n",
       "      <td>0.871663</td>\n",
       "      <td>0.907339</td>\n",
       "      <td>1.412363</td>\n",
       "      <td>0.044477</td>\n",
       "      <td>0.003395</td>\n",
       "      <td>0.000559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>36.803865</td>\n",
       "      <td>0.377653</td>\n",
       "      <td>0.868411</td>\n",
       "      <td>0.909074</td>\n",
       "      <td>5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'max_depth': 5, 'min_child_weight': 0.1}</td>\n",
       "      <td>10</td>\n",
       "      <td>0.865241</td>\n",
       "      <td>0.909279</td>\n",
       "      <td>...</td>\n",
       "      <td>0.869385</td>\n",
       "      <td>0.909196</td>\n",
       "      <td>0.871353</td>\n",
       "      <td>0.910153</td>\n",
       "      <td>0.870005</td>\n",
       "      <td>0.908209</td>\n",
       "      <td>1.066291</td>\n",
       "      <td>0.023622</td>\n",
       "      <td>0.002353</td>\n",
       "      <td>0.000673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>22.477659</td>\n",
       "      <td>0.237559</td>\n",
       "      <td>0.868179</td>\n",
       "      <td>0.880935</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>{'max_depth': 3, 'min_child_weight': 5}</td>\n",
       "      <td>11</td>\n",
       "      <td>0.864744</td>\n",
       "      <td>0.882631</td>\n",
       "      <td>...</td>\n",
       "      <td>0.868888</td>\n",
       "      <td>0.879937</td>\n",
       "      <td>0.874337</td>\n",
       "      <td>0.880066</td>\n",
       "      <td>0.868015</td>\n",
       "      <td>0.880113</td>\n",
       "      <td>0.589290</td>\n",
       "      <td>0.010044</td>\n",
       "      <td>0.003491</td>\n",
       "      <td>0.001121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>38.330280</td>\n",
       "      <td>0.444798</td>\n",
       "      <td>0.867913</td>\n",
       "      <td>0.906563</td>\n",
       "      <td>5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>{'max_depth': 5, 'min_child_weight': 1.5}</td>\n",
       "      <td>12</td>\n",
       "      <td>0.864910</td>\n",
       "      <td>0.907166</td>\n",
       "      <td>...</td>\n",
       "      <td>0.869385</td>\n",
       "      <td>0.906295</td>\n",
       "      <td>0.871519</td>\n",
       "      <td>0.906589</td>\n",
       "      <td>0.868678</td>\n",
       "      <td>0.905640</td>\n",
       "      <td>2.363528</td>\n",
       "      <td>0.050586</td>\n",
       "      <td>0.002562</td>\n",
       "      <td>0.000566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16.030240</td>\n",
       "      <td>0.192429</td>\n",
       "      <td>0.867847</td>\n",
       "      <td>0.873168</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>{'max_depth': 2, 'min_child_weight': 1}</td>\n",
       "      <td>13</td>\n",
       "      <td>0.862755</td>\n",
       "      <td>0.873430</td>\n",
       "      <td>...</td>\n",
       "      <td>0.869219</td>\n",
       "      <td>0.872892</td>\n",
       "      <td>0.873176</td>\n",
       "      <td>0.872275</td>\n",
       "      <td>0.870005</td>\n",
       "      <td>0.873026</td>\n",
       "      <td>0.530780</td>\n",
       "      <td>0.012229</td>\n",
       "      <td>0.003874</td>\n",
       "      <td>0.000643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.368867</td>\n",
       "      <td>0.192629</td>\n",
       "      <td>0.867847</td>\n",
       "      <td>0.873417</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'max_depth': 2, 'min_child_weight': 0.1}</td>\n",
       "      <td>13</td>\n",
       "      <td>0.862258</td>\n",
       "      <td>0.873886</td>\n",
       "      <td>...</td>\n",
       "      <td>0.870877</td>\n",
       "      <td>0.873016</td>\n",
       "      <td>0.873508</td>\n",
       "      <td>0.872814</td>\n",
       "      <td>0.869342</td>\n",
       "      <td>0.873109</td>\n",
       "      <td>0.385066</td>\n",
       "      <td>0.009660</td>\n",
       "      <td>0.004378</td>\n",
       "      <td>0.000557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16.126705</td>\n",
       "      <td>0.186625</td>\n",
       "      <td>0.867416</td>\n",
       "      <td>0.872820</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>{'max_depth': 2, 'min_child_weight': 5}</td>\n",
       "      <td>15</td>\n",
       "      <td>0.861760</td>\n",
       "      <td>0.873472</td>\n",
       "      <td>...</td>\n",
       "      <td>0.869219</td>\n",
       "      <td>0.871690</td>\n",
       "      <td>0.873176</td>\n",
       "      <td>0.872192</td>\n",
       "      <td>0.869673</td>\n",
       "      <td>0.872405</td>\n",
       "      <td>0.252073</td>\n",
       "      <td>0.009546</td>\n",
       "      <td>0.004263</td>\n",
       "      <td>0.000958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15.987511</td>\n",
       "      <td>0.186625</td>\n",
       "      <td>0.867283</td>\n",
       "      <td>0.873235</td>\n",
       "      <td>2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>{'max_depth': 2, 'min_child_weight': 1.5}</td>\n",
       "      <td>16</td>\n",
       "      <td>0.861429</td>\n",
       "      <td>0.873596</td>\n",
       "      <td>...</td>\n",
       "      <td>0.868888</td>\n",
       "      <td>0.872560</td>\n",
       "      <td>0.873176</td>\n",
       "      <td>0.872648</td>\n",
       "      <td>0.870337</td>\n",
       "      <td>0.873316</td>\n",
       "      <td>0.382320</td>\n",
       "      <td>0.006249</td>\n",
       "      <td>0.004537</td>\n",
       "      <td>0.000566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.359590</td>\n",
       "      <td>0.094663</td>\n",
       "      <td>0.855712</td>\n",
       "      <td>0.856807</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>{'max_depth': 1, 'min_child_weight': 1}</td>\n",
       "      <td>17</td>\n",
       "      <td>0.848500</td>\n",
       "      <td>0.857806</td>\n",
       "      <td>...</td>\n",
       "      <td>0.858611</td>\n",
       "      <td>0.856563</td>\n",
       "      <td>0.860411</td>\n",
       "      <td>0.856196</td>\n",
       "      <td>0.859062</td>\n",
       "      <td>0.856077</td>\n",
       "      <td>0.334044</td>\n",
       "      <td>0.005949</td>\n",
       "      <td>0.004640</td>\n",
       "      <td>0.000679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.333453</td>\n",
       "      <td>0.094463</td>\n",
       "      <td>0.855712</td>\n",
       "      <td>0.856798</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>{'max_depth': 1, 'min_child_weight': 5}</td>\n",
       "      <td>17</td>\n",
       "      <td>0.848500</td>\n",
       "      <td>0.857806</td>\n",
       "      <td>...</td>\n",
       "      <td>0.858611</td>\n",
       "      <td>0.856563</td>\n",
       "      <td>0.860411</td>\n",
       "      <td>0.856196</td>\n",
       "      <td>0.859062</td>\n",
       "      <td>0.856077</td>\n",
       "      <td>0.178026</td>\n",
       "      <td>0.006834</td>\n",
       "      <td>0.004640</td>\n",
       "      <td>0.000672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.342559</td>\n",
       "      <td>0.098066</td>\n",
       "      <td>0.855712</td>\n",
       "      <td>0.856807</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>{'max_depth': 1, 'min_child_weight': 1.5}</td>\n",
       "      <td>17</td>\n",
       "      <td>0.848500</td>\n",
       "      <td>0.857806</td>\n",
       "      <td>...</td>\n",
       "      <td>0.858611</td>\n",
       "      <td>0.856563</td>\n",
       "      <td>0.860411</td>\n",
       "      <td>0.856196</td>\n",
       "      <td>0.859062</td>\n",
       "      <td>0.856077</td>\n",
       "      <td>0.331052</td>\n",
       "      <td>0.004778</td>\n",
       "      <td>0.004640</td>\n",
       "      <td>0.000679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.901615</td>\n",
       "      <td>0.094563</td>\n",
       "      <td>0.855712</td>\n",
       "      <td>0.856807</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'max_depth': 1, 'min_child_weight': 0.1}</td>\n",
       "      <td>17</td>\n",
       "      <td>0.848500</td>\n",
       "      <td>0.857806</td>\n",
       "      <td>...</td>\n",
       "      <td>0.858611</td>\n",
       "      <td>0.856563</td>\n",
       "      <td>0.860411</td>\n",
       "      <td>0.856196</td>\n",
       "      <td>0.859062</td>\n",
       "      <td>0.856077</td>\n",
       "      <td>0.397222</td>\n",
       "      <td>0.004851</td>\n",
       "      <td>0.004640</td>\n",
       "      <td>0.000679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "8       22.298040         0.240561         0.869903          0.883173   \n",
       "13      29.884425         0.316512         0.869836          0.893997   \n",
       "14      29.581726         0.297499         0.869505          0.893674   \n",
       "12      30.923218         0.318514         0.869405          0.894337   \n",
       "9       23.050348         0.247366         0.869140          0.882808   \n",
       "15      31.165689         0.325518         0.869074          0.889165   \n",
       "19      36.316131         0.388260         0.868941          0.896683   \n",
       "10      23.154615         0.244064         0.868908          0.882609   \n",
       "17      37.305093         0.412677         0.868808          0.907599   \n",
       "16      36.803865         0.377653         0.868411          0.909074   \n",
       "11      22.477659         0.237559         0.868179          0.880935   \n",
       "18      38.330280         0.444798         0.867913          0.906563   \n",
       "5       16.030240         0.192429         0.867847          0.873168   \n",
       "4       16.368867         0.192629         0.867847          0.873417   \n",
       "7       16.126705         0.186625         0.867416          0.872820   \n",
       "6       15.987511         0.186625         0.867283          0.873235   \n",
       "1        9.359590         0.094663         0.855712          0.856807   \n",
       "3        9.333453         0.094463         0.855712          0.856798   \n",
       "2        9.342559         0.098066         0.855712          0.856807   \n",
       "0        8.901615         0.094563         0.855712          0.856807   \n",
       "\n",
       "   param_max_depth param_min_child_weight  \\\n",
       "8                3                    0.1   \n",
       "13               4                      1   \n",
       "14               4                    1.5   \n",
       "12               4                    0.1   \n",
       "9                3                      1   \n",
       "15               4                      5   \n",
       "19               5                      5   \n",
       "10               3                    1.5   \n",
       "17               5                      1   \n",
       "16               5                    0.1   \n",
       "11               3                      5   \n",
       "18               5                    1.5   \n",
       "5                2                      1   \n",
       "4                2                    0.1   \n",
       "7                2                      5   \n",
       "6                2                    1.5   \n",
       "1                1                      1   \n",
       "3                1                      5   \n",
       "2                1                    1.5   \n",
       "0                1                    0.1   \n",
       "\n",
       "                                       params  rank_test_score  \\\n",
       "8   {'max_depth': 3, 'min_child_weight': 0.1}                1   \n",
       "13    {'max_depth': 4, 'min_child_weight': 1}                2   \n",
       "14  {'max_depth': 4, 'min_child_weight': 1.5}                3   \n",
       "12  {'max_depth': 4, 'min_child_weight': 0.1}                4   \n",
       "9     {'max_depth': 3, 'min_child_weight': 1}                5   \n",
       "15    {'max_depth': 4, 'min_child_weight': 5}                6   \n",
       "19    {'max_depth': 5, 'min_child_weight': 5}                7   \n",
       "10  {'max_depth': 3, 'min_child_weight': 1.5}                8   \n",
       "17    {'max_depth': 5, 'min_child_weight': 1}                9   \n",
       "16  {'max_depth': 5, 'min_child_weight': 0.1}               10   \n",
       "11    {'max_depth': 3, 'min_child_weight': 5}               11   \n",
       "18  {'max_depth': 5, 'min_child_weight': 1.5}               12   \n",
       "5     {'max_depth': 2, 'min_child_weight': 1}               13   \n",
       "4   {'max_depth': 2, 'min_child_weight': 0.1}               13   \n",
       "7     {'max_depth': 2, 'min_child_weight': 5}               15   \n",
       "6   {'max_depth': 2, 'min_child_weight': 1.5}               16   \n",
       "1     {'max_depth': 1, 'min_child_weight': 1}               17   \n",
       "3     {'max_depth': 1, 'min_child_weight': 5}               17   \n",
       "2   {'max_depth': 1, 'min_child_weight': 1.5}               17   \n",
       "0   {'max_depth': 1, 'min_child_weight': 0.1}               17   \n",
       "\n",
       "    split0_test_score  split0_train_score       ...         split2_test_score  \\\n",
       "8            0.864910            0.884537       ...                  0.871706   \n",
       "13           0.864910            0.894857       ...                  0.871043   \n",
       "14           0.863584            0.894898       ...                  0.871706   \n",
       "12           0.865407            0.894525       ...                  0.871208   \n",
       "9            0.863252            0.883294       ...                  0.870545   \n",
       "15           0.862921            0.890091       ...                  0.871374   \n",
       "19           0.864578            0.897012       ...                  0.869054   \n",
       "10           0.863086            0.883170       ...                  0.871374   \n",
       "17           0.864247            0.907497       ...                  0.869551   \n",
       "16           0.865241            0.909279       ...                  0.869385   \n",
       "11           0.864744            0.882631       ...                  0.868888   \n",
       "18           0.864910            0.907166       ...                  0.869385   \n",
       "5            0.862755            0.873430       ...                  0.869219   \n",
       "4            0.862258            0.873886       ...                  0.870877   \n",
       "7            0.861760            0.873472       ...                  0.869219   \n",
       "6            0.861429            0.873596       ...                  0.868888   \n",
       "1            0.848500            0.857806       ...                  0.858611   \n",
       "3            0.848500            0.857806       ...                  0.858611   \n",
       "2            0.848500            0.857806       ...                  0.858611   \n",
       "0            0.848500            0.857806       ...                  0.858611   \n",
       "\n",
       "    split2_train_score  split3_test_score  split3_train_score  \\\n",
       "8             0.882631           0.876492            0.882180   \n",
       "13            0.893986           0.873840            0.893162   \n",
       "14            0.892702           0.873674            0.893121   \n",
       "12            0.893448           0.871684            0.894861   \n",
       "9             0.883128           0.875332            0.881434   \n",
       "15            0.888599           0.874337            0.888976   \n",
       "19            0.896349           0.873840            0.895814   \n",
       "10            0.882672           0.874337            0.881475   \n",
       "17            0.907497           0.873011            0.907004   \n",
       "16            0.909196           0.871353            0.910153   \n",
       "11            0.879937           0.874337            0.880066   \n",
       "18            0.906295           0.871519            0.906589   \n",
       "5             0.872892           0.873176            0.872275   \n",
       "4             0.873016           0.873508            0.872814   \n",
       "7             0.871690           0.873176            0.872192   \n",
       "6             0.872560           0.873176            0.872648   \n",
       "1             0.856563           0.860411            0.856196   \n",
       "3             0.856563           0.860411            0.856196   \n",
       "2             0.856563           0.860411            0.856196   \n",
       "0             0.856563           0.860411            0.856196   \n",
       "\n",
       "    split4_test_score  split4_train_score  std_fit_time  std_score_time  \\\n",
       "8            0.871000            0.882102      0.558747        0.003429   \n",
       "13           0.870337            0.893954      0.564917        0.013850   \n",
       "14           0.870337            0.893581      0.850701        0.013022   \n",
       "12           0.870337            0.893995      0.593056        0.017686   \n",
       "9            0.871663            0.881853      0.658974        0.021482   \n",
       "15           0.870005            0.888401      1.168402        0.017469   \n",
       "19           0.870005            0.896440      1.814589        0.029364   \n",
       "10           0.870337            0.880942      0.907110        0.008874   \n",
       "17           0.871663            0.907339      1.412363        0.044477   \n",
       "16           0.870005            0.908209      1.066291        0.023622   \n",
       "11           0.868015            0.880113      0.589290        0.010044   \n",
       "18           0.868678            0.905640      2.363528        0.050586   \n",
       "5            0.870005            0.873026      0.530780        0.012229   \n",
       "4            0.869342            0.873109      0.385066        0.009660   \n",
       "7            0.869673            0.872405      0.252073        0.009546   \n",
       "6            0.870337            0.873316      0.382320        0.006249   \n",
       "1            0.859062            0.856077      0.334044        0.005949   \n",
       "3            0.859062            0.856077      0.178026        0.006834   \n",
       "2            0.859062            0.856077      0.331052        0.004778   \n",
       "0            0.859062            0.856077      0.397222        0.004851   \n",
       "\n",
       "    std_test_score  std_train_score  \n",
       "8         0.004313         0.001079  \n",
       "13        0.002919         0.000537  \n",
       "14        0.003453         0.000764  \n",
       "12        0.002295         0.000546  \n",
       "9         0.004455         0.001045  \n",
       "15        0.003929         0.000656  \n",
       "19        0.003069         0.000675  \n",
       "10        0.004093         0.001351  \n",
       "17        0.003395         0.000559  \n",
       "16        0.002353         0.000673  \n",
       "11        0.003491         0.001121  \n",
       "18        0.002562         0.000566  \n",
       "5         0.003874         0.000643  \n",
       "4         0.004378         0.000557  \n",
       "7         0.004263         0.000958  \n",
       "6         0.004537         0.000566  \n",
       "1         0.004640         0.000679  \n",
       "3         0.004640         0.000672  \n",
       "2         0.004640         0.000679  \n",
       "0         0.004640         0.000679  \n",
       "\n",
       "[20 rows x 22 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(gs2.cv_results_).sort_values('mean_test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 3, 'min_child_weight': 0.1}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs2.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune gamma, penalty of the leaf number\n",
    "By default, gamma = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_gamma</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22.688645</td>\n",
       "      <td>0.241464</td>\n",
       "      <td>0.869903</td>\n",
       "      <td>0.883173</td>\n",
       "      <td>0</td>\n",
       "      <td>{'gamma': 0}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.864910</td>\n",
       "      <td>0.884537</td>\n",
       "      <td>0.865407</td>\n",
       "      <td>...</td>\n",
       "      <td>0.871706</td>\n",
       "      <td>0.882631</td>\n",
       "      <td>0.876492</td>\n",
       "      <td>0.882180</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>0.882102</td>\n",
       "      <td>0.450435</td>\n",
       "      <td>0.011977</td>\n",
       "      <td>0.004313</td>\n",
       "      <td>0.001079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23.113732</td>\n",
       "      <td>0.237661</td>\n",
       "      <td>0.869637</td>\n",
       "      <td>0.883222</td>\n",
       "      <td>0.3</td>\n",
       "      <td>{'gamma': 0.3}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.865241</td>\n",
       "      <td>0.884496</td>\n",
       "      <td>0.866236</td>\n",
       "      <td>...</td>\n",
       "      <td>0.870711</td>\n",
       "      <td>0.882589</td>\n",
       "      <td>0.875497</td>\n",
       "      <td>0.882511</td>\n",
       "      <td>0.870502</td>\n",
       "      <td>0.882019</td>\n",
       "      <td>0.446195</td>\n",
       "      <td>0.008258</td>\n",
       "      <td>0.003664</td>\n",
       "      <td>0.001058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23.232112</td>\n",
       "      <td>0.246067</td>\n",
       "      <td>0.869505</td>\n",
       "      <td>0.883247</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'gamma': 0.1}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.864744</td>\n",
       "      <td>0.884413</td>\n",
       "      <td>0.865407</td>\n",
       "      <td>...</td>\n",
       "      <td>0.871540</td>\n",
       "      <td>0.883004</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.882801</td>\n",
       "      <td>0.870834</td>\n",
       "      <td>0.881936</td>\n",
       "      <td>0.511652</td>\n",
       "      <td>0.016109</td>\n",
       "      <td>0.003888</td>\n",
       "      <td>0.000898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22.749086</td>\n",
       "      <td>0.212744</td>\n",
       "      <td>0.869306</td>\n",
       "      <td>0.881531</td>\n",
       "      <td>0.7</td>\n",
       "      <td>{'gamma': 0.7}</td>\n",
       "      <td>4</td>\n",
       "      <td>0.864578</td>\n",
       "      <td>0.882341</td>\n",
       "      <td>0.865573</td>\n",
       "      <td>...</td>\n",
       "      <td>0.871208</td>\n",
       "      <td>0.882258</td>\n",
       "      <td>0.875995</td>\n",
       "      <td>0.880729</td>\n",
       "      <td>0.869176</td>\n",
       "      <td>0.878331</td>\n",
       "      <td>0.468620</td>\n",
       "      <td>0.020472</td>\n",
       "      <td>0.004115</td>\n",
       "      <td>0.001906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22.467395</td>\n",
       "      <td>0.188027</td>\n",
       "      <td>0.869007</td>\n",
       "      <td>0.879542</td>\n",
       "      <td>1</td>\n",
       "      <td>{'gamma': 1}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.864081</td>\n",
       "      <td>0.878818</td>\n",
       "      <td>0.864910</td>\n",
       "      <td>...</td>\n",
       "      <td>0.871374</td>\n",
       "      <td>0.879191</td>\n",
       "      <td>0.874668</td>\n",
       "      <td>0.878284</td>\n",
       "      <td>0.870005</td>\n",
       "      <td>0.878745</td>\n",
       "      <td>0.396564</td>\n",
       "      <td>0.010959</td>\n",
       "      <td>0.003993</td>\n",
       "      <td>0.001591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22.763095</td>\n",
       "      <td>0.236360</td>\n",
       "      <td>0.868742</td>\n",
       "      <td>0.883048</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'gamma': 0.5}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.864081</td>\n",
       "      <td>0.883916</td>\n",
       "      <td>0.864910</td>\n",
       "      <td>...</td>\n",
       "      <td>0.869717</td>\n",
       "      <td>0.882548</td>\n",
       "      <td>0.874337</td>\n",
       "      <td>0.882304</td>\n",
       "      <td>0.870668</td>\n",
       "      <td>0.882226</td>\n",
       "      <td>0.536714</td>\n",
       "      <td>0.008435</td>\n",
       "      <td>0.003805</td>\n",
       "      <td>0.000857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>18.975042</td>\n",
       "      <td>0.116880</td>\n",
       "      <td>0.867184</td>\n",
       "      <td>0.875124</td>\n",
       "      <td>2</td>\n",
       "      <td>{'gamma': 2}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.861760</td>\n",
       "      <td>0.874839</td>\n",
       "      <td>0.862589</td>\n",
       "      <td>...</td>\n",
       "      <td>0.869219</td>\n",
       "      <td>0.875212</td>\n",
       "      <td>0.872679</td>\n",
       "      <td>0.874099</td>\n",
       "      <td>0.869673</td>\n",
       "      <td>0.874435</td>\n",
       "      <td>2.944512</td>\n",
       "      <td>0.011456</td>\n",
       "      <td>0.004268</td>\n",
       "      <td>0.001027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0      22.688645         0.241464         0.869903          0.883173   \n",
       "2      23.113732         0.237661         0.869637          0.883222   \n",
       "1      23.232112         0.246067         0.869505          0.883247   \n",
       "4      22.749086         0.212744         0.869306          0.881531   \n",
       "5      22.467395         0.188027         0.869007          0.879542   \n",
       "3      22.763095         0.236360         0.868742          0.883048   \n",
       "6      18.975042         0.116880         0.867184          0.875124   \n",
       "\n",
       "  param_gamma          params  rank_test_score  split0_test_score  \\\n",
       "0           0    {'gamma': 0}                1           0.864910   \n",
       "2         0.3  {'gamma': 0.3}                2           0.865241   \n",
       "1         0.1  {'gamma': 0.1}                3           0.864744   \n",
       "4         0.7  {'gamma': 0.7}                4           0.864578   \n",
       "5           1    {'gamma': 1}                5           0.864081   \n",
       "3         0.5  {'gamma': 0.5}                6           0.864081   \n",
       "6           2    {'gamma': 2}                7           0.861760   \n",
       "\n",
       "   split0_train_score  split1_test_score       ...         split2_test_score  \\\n",
       "0            0.884537           0.865407       ...                  0.871706   \n",
       "2            0.884496           0.866236       ...                  0.870711   \n",
       "1            0.884413           0.865407       ...                  0.871540   \n",
       "4            0.882341           0.865573       ...                  0.871208   \n",
       "5            0.878818           0.864910       ...                  0.871374   \n",
       "3            0.883916           0.864910       ...                  0.869717   \n",
       "6            0.874839           0.862589       ...                  0.869219   \n",
       "\n",
       "   split2_train_score  split3_test_score  split3_train_score  \\\n",
       "0            0.882631           0.876492            0.882180   \n",
       "2            0.882589           0.875497            0.882511   \n",
       "1            0.883004           0.875000            0.882801   \n",
       "4            0.882258           0.875995            0.880729   \n",
       "5            0.879191           0.874668            0.878284   \n",
       "3            0.882548           0.874337            0.882304   \n",
       "6            0.875212           0.872679            0.874099   \n",
       "\n",
       "   split4_test_score  split4_train_score  std_fit_time  std_score_time  \\\n",
       "0           0.871000            0.882102      0.450435        0.011977   \n",
       "2           0.870502            0.882019      0.446195        0.008258   \n",
       "1           0.870834            0.881936      0.511652        0.016109   \n",
       "4           0.869176            0.878331      0.468620        0.020472   \n",
       "5           0.870005            0.878745      0.396564        0.010959   \n",
       "3           0.870668            0.882226      0.536714        0.008435   \n",
       "6           0.869673            0.874435      2.944512        0.011456   \n",
       "\n",
       "   std_test_score  std_train_score  \n",
       "0        0.004313         0.001079  \n",
       "2        0.003664         0.001058  \n",
       "1        0.003888         0.000898  \n",
       "4        0.004115         0.001906  \n",
       "5        0.003993         0.001591  \n",
       "3        0.003805         0.000857  \n",
       "6        0.004268         0.001027  \n",
       "\n",
       "[7 rows x 21 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid3 = {'gamma': [0, 0.1, 0.3, 0.5, 0.7, 1, 2]}\n",
    "gs3 = GridSearchCV(gs2.best_estimator_, param_grid3, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "gs3.fit(train_X, train_y)\n",
    "pd.DataFrame(gs3.cv_results_).sort_values('mean_test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Regulate the data source for each tree (robustness to noise, like bagging)\n",
    "subsample, colsample_bytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_colsample_bytree</th>\n",
       "      <th>param_subsample</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16.799864</td>\n",
       "      <td>0.262578</td>\n",
       "      <td>0.870035</td>\n",
       "      <td>0.884093</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'colsample_bytree': 0.6, 'subsample': 0.8}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.866899</td>\n",
       "      <td>0.884952</td>\n",
       "      <td>...</td>\n",
       "      <td>0.871043</td>\n",
       "      <td>0.883543</td>\n",
       "      <td>0.876160</td>\n",
       "      <td>0.883589</td>\n",
       "      <td>0.870834</td>\n",
       "      <td>0.883884</td>\n",
       "      <td>0.701843</td>\n",
       "      <td>0.022780</td>\n",
       "      <td>0.003795</td>\n",
       "      <td>0.000548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>22.066933</td>\n",
       "      <td>0.208942</td>\n",
       "      <td>0.869903</td>\n",
       "      <td>0.883173</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>{'colsample_bytree': 1.0, 'subsample': 1.0}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.864910</td>\n",
       "      <td>0.884537</td>\n",
       "      <td>...</td>\n",
       "      <td>0.871706</td>\n",
       "      <td>0.882631</td>\n",
       "      <td>0.876492</td>\n",
       "      <td>0.882180</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>0.882102</td>\n",
       "      <td>2.676483</td>\n",
       "      <td>0.015309</td>\n",
       "      <td>0.004313</td>\n",
       "      <td>0.001079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17.648136</td>\n",
       "      <td>0.248268</td>\n",
       "      <td>0.869770</td>\n",
       "      <td>0.884847</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.9</td>\n",
       "      <td>{'colsample_bytree': 0.7, 'subsample': 0.9}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.866899</td>\n",
       "      <td>0.885283</td>\n",
       "      <td>...</td>\n",
       "      <td>0.872203</td>\n",
       "      <td>0.884454</td>\n",
       "      <td>0.874171</td>\n",
       "      <td>0.884542</td>\n",
       "      <td>0.869010</td>\n",
       "      <td>0.884630</td>\n",
       "      <td>0.439406</td>\n",
       "      <td>0.019093</td>\n",
       "      <td>0.002979</td>\n",
       "      <td>0.000378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>21.328025</td>\n",
       "      <td>0.266780</td>\n",
       "      <td>0.869737</td>\n",
       "      <td>0.885104</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'colsample_bytree': 0.8, 'subsample': 0.8}</td>\n",
       "      <td>4</td>\n",
       "      <td>0.867230</td>\n",
       "      <td>0.886112</td>\n",
       "      <td>...</td>\n",
       "      <td>0.872203</td>\n",
       "      <td>0.884952</td>\n",
       "      <td>0.873508</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.871166</td>\n",
       "      <td>0.885251</td>\n",
       "      <td>0.812157</td>\n",
       "      <td>0.006340</td>\n",
       "      <td>0.003324</td>\n",
       "      <td>0.000612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20.547597</td>\n",
       "      <td>0.251470</td>\n",
       "      <td>0.869604</td>\n",
       "      <td>0.885270</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>{'colsample_bytree': 0.8, 'subsample': 0.9}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.866070</td>\n",
       "      <td>0.885532</td>\n",
       "      <td>...</td>\n",
       "      <td>0.870380</td>\n",
       "      <td>0.884869</td>\n",
       "      <td>0.875166</td>\n",
       "      <td>0.885081</td>\n",
       "      <td>0.870171</td>\n",
       "      <td>0.884878</td>\n",
       "      <td>0.932614</td>\n",
       "      <td>0.014756</td>\n",
       "      <td>0.003337</td>\n",
       "      <td>0.000432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>25.049642</td>\n",
       "      <td>0.264079</td>\n",
       "      <td>0.869604</td>\n",
       "      <td>0.885899</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'colsample_bytree': 1.0, 'subsample': 0.8}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.866899</td>\n",
       "      <td>0.886236</td>\n",
       "      <td>...</td>\n",
       "      <td>0.870877</td>\n",
       "      <td>0.885159</td>\n",
       "      <td>0.873176</td>\n",
       "      <td>0.885495</td>\n",
       "      <td>0.869342</td>\n",
       "      <td>0.886287</td>\n",
       "      <td>0.575087</td>\n",
       "      <td>0.012379</td>\n",
       "      <td>0.002249</td>\n",
       "      <td>0.000480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>22.467595</td>\n",
       "      <td>0.264779</td>\n",
       "      <td>0.869505</td>\n",
       "      <td>0.884938</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'colsample_bytree': 0.9, 'subsample': 0.8}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.867230</td>\n",
       "      <td>0.886319</td>\n",
       "      <td>...</td>\n",
       "      <td>0.870711</td>\n",
       "      <td>0.884827</td>\n",
       "      <td>0.873674</td>\n",
       "      <td>0.883796</td>\n",
       "      <td>0.869342</td>\n",
       "      <td>0.884837</td>\n",
       "      <td>0.647065</td>\n",
       "      <td>0.011185</td>\n",
       "      <td>0.002556</td>\n",
       "      <td>0.000804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24.109306</td>\n",
       "      <td>0.233458</td>\n",
       "      <td>0.869472</td>\n",
       "      <td>0.885419</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>{'colsample_bytree': 1.0, 'subsample': 0.9}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.867064</td>\n",
       "      <td>0.885781</td>\n",
       "      <td>...</td>\n",
       "      <td>0.870048</td>\n",
       "      <td>0.885449</td>\n",
       "      <td>0.873342</td>\n",
       "      <td>0.884915</td>\n",
       "      <td>0.872326</td>\n",
       "      <td>0.885251</td>\n",
       "      <td>0.614378</td>\n",
       "      <td>0.012246</td>\n",
       "      <td>0.003262</td>\n",
       "      <td>0.000314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>21.404776</td>\n",
       "      <td>0.247267</td>\n",
       "      <td>0.869472</td>\n",
       "      <td>0.885079</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>{'colsample_bytree': 0.9, 'subsample': 0.9}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.866733</td>\n",
       "      <td>0.885159</td>\n",
       "      <td>...</td>\n",
       "      <td>0.870545</td>\n",
       "      <td>0.884786</td>\n",
       "      <td>0.875497</td>\n",
       "      <td>0.884418</td>\n",
       "      <td>0.869673</td>\n",
       "      <td>0.884795</td>\n",
       "      <td>0.444734</td>\n",
       "      <td>0.015990</td>\n",
       "      <td>0.003628</td>\n",
       "      <td>0.000624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20.061768</td>\n",
       "      <td>0.250269</td>\n",
       "      <td>0.869405</td>\n",
       "      <td>0.883123</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>{'colsample_bytree': 0.9, 'subsample': 1.0}</td>\n",
       "      <td>10</td>\n",
       "      <td>0.865075</td>\n",
       "      <td>0.884662</td>\n",
       "      <td>...</td>\n",
       "      <td>0.870214</td>\n",
       "      <td>0.882962</td>\n",
       "      <td>0.875497</td>\n",
       "      <td>0.882138</td>\n",
       "      <td>0.869342</td>\n",
       "      <td>0.882268</td>\n",
       "      <td>0.446161</td>\n",
       "      <td>0.014698</td>\n",
       "      <td>0.003544</td>\n",
       "      <td>0.000928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>18.855752</td>\n",
       "      <td>0.233258</td>\n",
       "      <td>0.869339</td>\n",
       "      <td>0.883065</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>{'colsample_bytree': 0.8, 'subsample': 1.0}</td>\n",
       "      <td>11</td>\n",
       "      <td>0.864247</td>\n",
       "      <td>0.883253</td>\n",
       "      <td>...</td>\n",
       "      <td>0.872203</td>\n",
       "      <td>0.882838</td>\n",
       "      <td>0.873674</td>\n",
       "      <td>0.882511</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>0.882641</td>\n",
       "      <td>0.928799</td>\n",
       "      <td>0.013874</td>\n",
       "      <td>0.003738</td>\n",
       "      <td>0.000567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16.694059</td>\n",
       "      <td>0.252070</td>\n",
       "      <td>0.869339</td>\n",
       "      <td>0.883952</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>{'colsample_bytree': 0.6, 'subsample': 0.6}</td>\n",
       "      <td>11</td>\n",
       "      <td>0.867230</td>\n",
       "      <td>0.884330</td>\n",
       "      <td>...</td>\n",
       "      <td>0.871043</td>\n",
       "      <td>0.883957</td>\n",
       "      <td>0.873342</td>\n",
       "      <td>0.883962</td>\n",
       "      <td>0.869508</td>\n",
       "      <td>0.883096</td>\n",
       "      <td>0.349497</td>\n",
       "      <td>0.006276</td>\n",
       "      <td>0.002742</td>\n",
       "      <td>0.000466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>16.046352</td>\n",
       "      <td>0.244766</td>\n",
       "      <td>0.869339</td>\n",
       "      <td>0.882866</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>{'colsample_bytree': 0.7, 'subsample': 1.0}</td>\n",
       "      <td>11</td>\n",
       "      <td>0.863915</td>\n",
       "      <td>0.883543</td>\n",
       "      <td>...</td>\n",
       "      <td>0.872203</td>\n",
       "      <td>0.882216</td>\n",
       "      <td>0.874668</td>\n",
       "      <td>0.882801</td>\n",
       "      <td>0.870834</td>\n",
       "      <td>0.882060</td>\n",
       "      <td>0.585800</td>\n",
       "      <td>0.014742</td>\n",
       "      <td>0.004158</td>\n",
       "      <td>0.000670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17.234756</td>\n",
       "      <td>0.254072</td>\n",
       "      <td>0.869041</td>\n",
       "      <td>0.884101</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>{'colsample_bytree': 0.6, 'subsample': 0.7}</td>\n",
       "      <td>14</td>\n",
       "      <td>0.865075</td>\n",
       "      <td>0.884496</td>\n",
       "      <td>...</td>\n",
       "      <td>0.871374</td>\n",
       "      <td>0.884081</td>\n",
       "      <td>0.873342</td>\n",
       "      <td>0.883092</td>\n",
       "      <td>0.870668</td>\n",
       "      <td>0.884132</td>\n",
       "      <td>0.667812</td>\n",
       "      <td>0.011650</td>\n",
       "      <td>0.003487</td>\n",
       "      <td>0.000555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.056459</td>\n",
       "      <td>0.246267</td>\n",
       "      <td>0.869007</td>\n",
       "      <td>0.882733</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>{'colsample_bytree': 0.6, 'subsample': 1.0}</td>\n",
       "      <td>15</td>\n",
       "      <td>0.863915</td>\n",
       "      <td>0.883418</td>\n",
       "      <td>...</td>\n",
       "      <td>0.872866</td>\n",
       "      <td>0.882548</td>\n",
       "      <td>0.872347</td>\n",
       "      <td>0.882180</td>\n",
       "      <td>0.870668</td>\n",
       "      <td>0.881978</td>\n",
       "      <td>1.027833</td>\n",
       "      <td>0.014068</td>\n",
       "      <td>0.003713</td>\n",
       "      <td>0.000638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20.318842</td>\n",
       "      <td>0.248968</td>\n",
       "      <td>0.868941</td>\n",
       "      <td>0.885551</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>{'colsample_bytree': 0.8, 'subsample': 0.7}</td>\n",
       "      <td>16</td>\n",
       "      <td>0.865241</td>\n",
       "      <td>0.886900</td>\n",
       "      <td>...</td>\n",
       "      <td>0.870048</td>\n",
       "      <td>0.885200</td>\n",
       "      <td>0.874005</td>\n",
       "      <td>0.884542</td>\n",
       "      <td>0.870005</td>\n",
       "      <td>0.885376</td>\n",
       "      <td>0.903521</td>\n",
       "      <td>0.009175</td>\n",
       "      <td>0.003292</td>\n",
       "      <td>0.000778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18.888174</td>\n",
       "      <td>0.256273</td>\n",
       "      <td>0.868908</td>\n",
       "      <td>0.884540</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>{'colsample_bytree': 0.7, 'subsample': 0.6}</td>\n",
       "      <td>17</td>\n",
       "      <td>0.865904</td>\n",
       "      <td>0.885035</td>\n",
       "      <td>...</td>\n",
       "      <td>0.870877</td>\n",
       "      <td>0.884164</td>\n",
       "      <td>0.874005</td>\n",
       "      <td>0.884459</td>\n",
       "      <td>0.870668</td>\n",
       "      <td>0.883801</td>\n",
       "      <td>0.772280</td>\n",
       "      <td>0.010392</td>\n",
       "      <td>0.003895</td>\n",
       "      <td>0.000535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>19.081105</td>\n",
       "      <td>0.262577</td>\n",
       "      <td>0.868908</td>\n",
       "      <td>0.884830</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'colsample_bytree': 0.7, 'subsample': 0.8}</td>\n",
       "      <td>17</td>\n",
       "      <td>0.865573</td>\n",
       "      <td>0.885822</td>\n",
       "      <td>...</td>\n",
       "      <td>0.871208</td>\n",
       "      <td>0.884454</td>\n",
       "      <td>0.873508</td>\n",
       "      <td>0.884666</td>\n",
       "      <td>0.869508</td>\n",
       "      <td>0.884464</td>\n",
       "      <td>0.711088</td>\n",
       "      <td>0.009025</td>\n",
       "      <td>0.003325</td>\n",
       "      <td>0.000509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>22.322899</td>\n",
       "      <td>0.269983</td>\n",
       "      <td>0.868742</td>\n",
       "      <td>0.885303</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>{'colsample_bytree': 0.9, 'subsample': 0.6}</td>\n",
       "      <td>19</td>\n",
       "      <td>0.863749</td>\n",
       "      <td>0.886195</td>\n",
       "      <td>...</td>\n",
       "      <td>0.870545</td>\n",
       "      <td>0.884579</td>\n",
       "      <td>0.874171</td>\n",
       "      <td>0.885827</td>\n",
       "      <td>0.870171</td>\n",
       "      <td>0.885044</td>\n",
       "      <td>1.021869</td>\n",
       "      <td>0.014794</td>\n",
       "      <td>0.003825</td>\n",
       "      <td>0.000608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.119382</td>\n",
       "      <td>0.252371</td>\n",
       "      <td>0.868610</td>\n",
       "      <td>0.884325</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.9</td>\n",
       "      <td>{'colsample_bytree': 0.6, 'subsample': 0.9}</td>\n",
       "      <td>20</td>\n",
       "      <td>0.865241</td>\n",
       "      <td>0.884827</td>\n",
       "      <td>...</td>\n",
       "      <td>0.870545</td>\n",
       "      <td>0.883791</td>\n",
       "      <td>0.873176</td>\n",
       "      <td>0.884335</td>\n",
       "      <td>0.868844</td>\n",
       "      <td>0.883014</td>\n",
       "      <td>1.036673</td>\n",
       "      <td>0.020416</td>\n",
       "      <td>0.003077</td>\n",
       "      <td>0.000898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>21.091364</td>\n",
       "      <td>0.275887</td>\n",
       "      <td>0.868543</td>\n",
       "      <td>0.885120</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>{'colsample_bytree': 0.8, 'subsample': 0.6}</td>\n",
       "      <td>21</td>\n",
       "      <td>0.865241</td>\n",
       "      <td>0.885739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.870877</td>\n",
       "      <td>0.884537</td>\n",
       "      <td>0.872513</td>\n",
       "      <td>0.885288</td>\n",
       "      <td>0.870005</td>\n",
       "      <td>0.884464</td>\n",
       "      <td>1.113099</td>\n",
       "      <td>0.022975</td>\n",
       "      <td>0.003291</td>\n",
       "      <td>0.000527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>21.529065</td>\n",
       "      <td>0.252071</td>\n",
       "      <td>0.868377</td>\n",
       "      <td>0.885386</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.7</td>\n",
       "      <td>{'colsample_bytree': 0.9, 'subsample': 0.7}</td>\n",
       "      <td>22</td>\n",
       "      <td>0.864744</td>\n",
       "      <td>0.886029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.869717</td>\n",
       "      <td>0.884910</td>\n",
       "      <td>0.873176</td>\n",
       "      <td>0.884459</td>\n",
       "      <td>0.869010</td>\n",
       "      <td>0.885417</td>\n",
       "      <td>0.741774</td>\n",
       "      <td>0.014454</td>\n",
       "      <td>0.003107</td>\n",
       "      <td>0.000637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>24.789766</td>\n",
       "      <td>0.271684</td>\n",
       "      <td>0.868377</td>\n",
       "      <td>0.885618</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>{'colsample_bytree': 1.0, 'subsample': 0.6}</td>\n",
       "      <td>22</td>\n",
       "      <td>0.864412</td>\n",
       "      <td>0.885946</td>\n",
       "      <td>...</td>\n",
       "      <td>0.871374</td>\n",
       "      <td>0.885283</td>\n",
       "      <td>0.874337</td>\n",
       "      <td>0.885247</td>\n",
       "      <td>0.868347</td>\n",
       "      <td>0.886204</td>\n",
       "      <td>0.855589</td>\n",
       "      <td>0.018598</td>\n",
       "      <td>0.004119</td>\n",
       "      <td>0.000386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>18.572361</td>\n",
       "      <td>0.256574</td>\n",
       "      <td>0.868377</td>\n",
       "      <td>0.884524</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>{'colsample_bytree': 0.7, 'subsample': 0.7}</td>\n",
       "      <td>22</td>\n",
       "      <td>0.864910</td>\n",
       "      <td>0.885573</td>\n",
       "      <td>...</td>\n",
       "      <td>0.870048</td>\n",
       "      <td>0.884164</td>\n",
       "      <td>0.873508</td>\n",
       "      <td>0.883962</td>\n",
       "      <td>0.869673</td>\n",
       "      <td>0.884837</td>\n",
       "      <td>1.023529</td>\n",
       "      <td>0.014402</td>\n",
       "      <td>0.003584</td>\n",
       "      <td>0.000607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>24.075783</td>\n",
       "      <td>0.251270</td>\n",
       "      <td>0.868344</td>\n",
       "      <td>0.885667</td>\n",
       "      <td>1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>{'colsample_bytree': 1.0, 'subsample': 0.7}</td>\n",
       "      <td>25</td>\n",
       "      <td>0.866401</td>\n",
       "      <td>0.886195</td>\n",
       "      <td>...</td>\n",
       "      <td>0.868888</td>\n",
       "      <td>0.885739</td>\n",
       "      <td>0.872182</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.869010</td>\n",
       "      <td>0.886287</td>\n",
       "      <td>0.778983</td>\n",
       "      <td>0.014773</td>\n",
       "      <td>0.002403</td>\n",
       "      <td>0.000755</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "2       16.799864         0.262578         0.870035          0.884093   \n",
       "24      22.066933         0.208942         0.869903          0.883173   \n",
       "8       17.648136         0.248268         0.869770          0.884847   \n",
       "12      21.328025         0.266780         0.869737          0.885104   \n",
       "13      20.547597         0.251470         0.869604          0.885270   \n",
       "22      25.049642         0.264079         0.869604          0.885899   \n",
       "17      22.467595         0.264779         0.869505          0.884938   \n",
       "23      24.109306         0.233458         0.869472          0.885419   \n",
       "18      21.404776         0.247267         0.869472          0.885079   \n",
       "19      20.061768         0.250269         0.869405          0.883123   \n",
       "14      18.855752         0.233258         0.869339          0.883065   \n",
       "0       16.694059         0.252070         0.869339          0.883952   \n",
       "9       16.046352         0.244766         0.869339          0.882866   \n",
       "1       17.234756         0.254072         0.869041          0.884101   \n",
       "4       16.056459         0.246267         0.869007          0.882733   \n",
       "11      20.318842         0.248968         0.868941          0.885551   \n",
       "5       18.888174         0.256273         0.868908          0.884540   \n",
       "7       19.081105         0.262577         0.868908          0.884830   \n",
       "15      22.322899         0.269983         0.868742          0.885303   \n",
       "3       17.119382         0.252371         0.868610          0.884325   \n",
       "10      21.091364         0.275887         0.868543          0.885120   \n",
       "16      21.529065         0.252071         0.868377          0.885386   \n",
       "20      24.789766         0.271684         0.868377          0.885618   \n",
       "6       18.572361         0.256574         0.868377          0.884524   \n",
       "21      24.075783         0.251270         0.868344          0.885667   \n",
       "\n",
       "   param_colsample_bytree param_subsample  \\\n",
       "2                     0.6             0.8   \n",
       "24                      1               1   \n",
       "8                     0.7             0.9   \n",
       "12                    0.8             0.8   \n",
       "13                    0.8             0.9   \n",
       "22                      1             0.8   \n",
       "17                    0.9             0.8   \n",
       "23                      1             0.9   \n",
       "18                    0.9             0.9   \n",
       "19                    0.9               1   \n",
       "14                    0.8               1   \n",
       "0                     0.6             0.6   \n",
       "9                     0.7               1   \n",
       "1                     0.6             0.7   \n",
       "4                     0.6               1   \n",
       "11                    0.8             0.7   \n",
       "5                     0.7             0.6   \n",
       "7                     0.7             0.8   \n",
       "15                    0.9             0.6   \n",
       "3                     0.6             0.9   \n",
       "10                    0.8             0.6   \n",
       "16                    0.9             0.7   \n",
       "20                      1             0.6   \n",
       "6                     0.7             0.7   \n",
       "21                      1             0.7   \n",
       "\n",
       "                                         params  rank_test_score  \\\n",
       "2   {'colsample_bytree': 0.6, 'subsample': 0.8}                1   \n",
       "24  {'colsample_bytree': 1.0, 'subsample': 1.0}                2   \n",
       "8   {'colsample_bytree': 0.7, 'subsample': 0.9}                3   \n",
       "12  {'colsample_bytree': 0.8, 'subsample': 0.8}                4   \n",
       "13  {'colsample_bytree': 0.8, 'subsample': 0.9}                5   \n",
       "22  {'colsample_bytree': 1.0, 'subsample': 0.8}                5   \n",
       "17  {'colsample_bytree': 0.9, 'subsample': 0.8}                7   \n",
       "23  {'colsample_bytree': 1.0, 'subsample': 0.9}                8   \n",
       "18  {'colsample_bytree': 0.9, 'subsample': 0.9}                8   \n",
       "19  {'colsample_bytree': 0.9, 'subsample': 1.0}               10   \n",
       "14  {'colsample_bytree': 0.8, 'subsample': 1.0}               11   \n",
       "0   {'colsample_bytree': 0.6, 'subsample': 0.6}               11   \n",
       "9   {'colsample_bytree': 0.7, 'subsample': 1.0}               11   \n",
       "1   {'colsample_bytree': 0.6, 'subsample': 0.7}               14   \n",
       "4   {'colsample_bytree': 0.6, 'subsample': 1.0}               15   \n",
       "11  {'colsample_bytree': 0.8, 'subsample': 0.7}               16   \n",
       "5   {'colsample_bytree': 0.7, 'subsample': 0.6}               17   \n",
       "7   {'colsample_bytree': 0.7, 'subsample': 0.8}               17   \n",
       "15  {'colsample_bytree': 0.9, 'subsample': 0.6}               19   \n",
       "3   {'colsample_bytree': 0.6, 'subsample': 0.9}               20   \n",
       "10  {'colsample_bytree': 0.8, 'subsample': 0.6}               21   \n",
       "16  {'colsample_bytree': 0.9, 'subsample': 0.7}               22   \n",
       "20  {'colsample_bytree': 1.0, 'subsample': 0.6}               22   \n",
       "6   {'colsample_bytree': 0.7, 'subsample': 0.7}               22   \n",
       "21  {'colsample_bytree': 1.0, 'subsample': 0.7}               25   \n",
       "\n",
       "    split0_test_score  split0_train_score       ...         split2_test_score  \\\n",
       "2            0.866899            0.884952       ...                  0.871043   \n",
       "24           0.864910            0.884537       ...                  0.871706   \n",
       "8            0.866899            0.885283       ...                  0.872203   \n",
       "12           0.867230            0.886112       ...                  0.872203   \n",
       "13           0.866070            0.885532       ...                  0.870380   \n",
       "22           0.866899            0.886236       ...                  0.870877   \n",
       "17           0.867230            0.886319       ...                  0.870711   \n",
       "23           0.867064            0.885781       ...                  0.870048   \n",
       "18           0.866733            0.885159       ...                  0.870545   \n",
       "19           0.865075            0.884662       ...                  0.870214   \n",
       "14           0.864247            0.883253       ...                  0.872203   \n",
       "0            0.867230            0.884330       ...                  0.871043   \n",
       "9            0.863915            0.883543       ...                  0.872203   \n",
       "1            0.865075            0.884496       ...                  0.871374   \n",
       "4            0.863915            0.883418       ...                  0.872866   \n",
       "11           0.865241            0.886900       ...                  0.870048   \n",
       "5            0.865904            0.885035       ...                  0.870877   \n",
       "7            0.865573            0.885822       ...                  0.871208   \n",
       "15           0.863749            0.886195       ...                  0.870545   \n",
       "3            0.865241            0.884827       ...                  0.870545   \n",
       "10           0.865241            0.885739       ...                  0.870877   \n",
       "16           0.864744            0.886029       ...                  0.869717   \n",
       "20           0.864412            0.885946       ...                  0.871374   \n",
       "6            0.864910            0.885573       ...                  0.870048   \n",
       "21           0.866401            0.886195       ...                  0.868888   \n",
       "\n",
       "    split2_train_score  split3_test_score  split3_train_score  \\\n",
       "2             0.883543           0.876160            0.883589   \n",
       "24            0.882631           0.876492            0.882180   \n",
       "8             0.884454           0.874171            0.884542   \n",
       "12            0.884952           0.873508            0.884211   \n",
       "13            0.884869           0.875166            0.885081   \n",
       "22            0.885159           0.873176            0.885495   \n",
       "17            0.884827           0.873674            0.883796   \n",
       "23            0.885449           0.873342            0.884915   \n",
       "18            0.884786           0.875497            0.884418   \n",
       "19            0.882962           0.875497            0.882138   \n",
       "14            0.882838           0.873674            0.882511   \n",
       "0             0.883957           0.873342            0.883962   \n",
       "9             0.882216           0.874668            0.882801   \n",
       "1             0.884081           0.873342            0.883092   \n",
       "4             0.882548           0.872347            0.882180   \n",
       "11            0.885200           0.874005            0.884542   \n",
       "5             0.884164           0.874005            0.884459   \n",
       "7             0.884454           0.873508            0.884666   \n",
       "15            0.884579           0.874171            0.885827   \n",
       "3             0.883791           0.873176            0.884335   \n",
       "10            0.884537           0.872513            0.885288   \n",
       "16            0.884910           0.873176            0.884459   \n",
       "20            0.885283           0.874337            0.885247   \n",
       "6             0.884164           0.873508            0.883962   \n",
       "21            0.885739           0.872182            0.884211   \n",
       "\n",
       "    split4_test_score  split4_train_score  std_fit_time  std_score_time  \\\n",
       "2            0.870834            0.883884      0.701843        0.022780   \n",
       "24           0.871000            0.882102      2.676483        0.015309   \n",
       "8            0.869010            0.884630      0.439406        0.019093   \n",
       "12           0.871166            0.885251      0.812157        0.006340   \n",
       "13           0.870171            0.884878      0.932614        0.014756   \n",
       "22           0.869342            0.886287      0.575087        0.012379   \n",
       "17           0.869342            0.884837      0.647065        0.011185   \n",
       "23           0.872326            0.885251      0.614378        0.012246   \n",
       "18           0.869673            0.884795      0.444734        0.015990   \n",
       "19           0.869342            0.882268      0.446161        0.014698   \n",
       "14           0.871000            0.882641      0.928799        0.013874   \n",
       "0            0.869508            0.883096      0.349497        0.006276   \n",
       "9            0.870834            0.882060      0.585800        0.014742   \n",
       "1            0.870668            0.884132      0.667812        0.011650   \n",
       "4            0.870668            0.881978      1.027833        0.014068   \n",
       "11           0.870005            0.885376      0.903521        0.009175   \n",
       "5            0.870668            0.883801      0.772280        0.010392   \n",
       "7            0.869508            0.884464      0.711088        0.009025   \n",
       "15           0.870171            0.885044      1.021869        0.014794   \n",
       "3            0.868844            0.883014      1.036673        0.020416   \n",
       "10           0.870005            0.884464      1.113099        0.022975   \n",
       "16           0.869010            0.885417      0.741774        0.014454   \n",
       "20           0.868347            0.886204      0.855589        0.018598   \n",
       "6            0.869673            0.884837      1.023529        0.014402   \n",
       "21           0.869010            0.886287      0.778983        0.014773   \n",
       "\n",
       "    std_test_score  std_train_score  \n",
       "2         0.003795         0.000548  \n",
       "24        0.004313         0.001079  \n",
       "8         0.002979         0.000378  \n",
       "12        0.003324         0.000612  \n",
       "13        0.003337         0.000432  \n",
       "22        0.002249         0.000480  \n",
       "17        0.002556         0.000804  \n",
       "23        0.003262         0.000314  \n",
       "18        0.003628         0.000624  \n",
       "19        0.003544         0.000928  \n",
       "14        0.003738         0.000567  \n",
       "0         0.002742         0.000466  \n",
       "9         0.004158         0.000670  \n",
       "1         0.003487         0.000555  \n",
       "4         0.003713         0.000638  \n",
       "11        0.003292         0.000778  \n",
       "5         0.003895         0.000535  \n",
       "7         0.003325         0.000509  \n",
       "15        0.003825         0.000608  \n",
       "3         0.003077         0.000898  \n",
       "10        0.003291         0.000527  \n",
       "16        0.003107         0.000637  \n",
       "20        0.004119         0.000386  \n",
       "6         0.003584         0.000607  \n",
       "21        0.002403         0.000755  \n",
       "\n",
       "[25 rows x 22 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid4 = {'subsample': np.linspace(0.6, 1, 5), 'colsample_bytree': np.linspace(0.6, 1, 5)}\n",
    "gs4 = GridSearchCV(gs3.best_estimator_, param_grid4, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "gs4.fit(train_X, train_y)\n",
    "pd.DataFrame(gs4.cv_results_).sort_values('mean_test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we may notice, compared with the original defautl 1, the two parameters [0.6, 0.8] improve the accuracy by 0.1%. Besidew, if we pay attention to std_test_score, it is also reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. [Optional] Regularize the leaf weights using *reg_lambda* for L2 and *reg_alpha* for L1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fine tune the learning rate \n",
    "We might choose a large learning rate like 0.1 initially to accelerate the above tuning process, since a high learning rate usually requires a small number of trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Fine tune the nubmer of boosting iterations (trees): early stopping\n",
    "Generally, we can fix the learning rate and then tune the boosting rounds with XGBoost's built-in cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_score': 0.5,\n",
       " 'booster': 'gbtree',\n",
       " 'colsample_bylevel': 1,\n",
       " 'colsample_bytree': 0.59999999999999998,\n",
       " 'gamma': 0,\n",
       " 'learning_rate': 0.05,\n",
       " 'max_delta_step': 0,\n",
       " 'max_depth': 3,\n",
       " 'min_child_weight': 0.1,\n",
       " 'missing': None,\n",
       " 'n_estimators': 1000,\n",
       " 'n_jobs': 1,\n",
       " 'nthread': None,\n",
       " 'objective': 'binary:logistic',\n",
       " 'random_state': 0,\n",
       " 'reg_alpha': 0,\n",
       " 'reg_lambda': 1,\n",
       " 'scale_pos_weight': 1,\n",
       " 'seed': None,\n",
       " 'silent': True,\n",
       " 'subsample': 0.80000000000000004}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbmat = xgb.DMatrix(train_X, train_y)\n",
    "xgb_params =gs4.best_estimator_.get_params()\n",
    "xgb_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params['nthread'] = -1 # avoid error\n",
    "xgb_params['seed'] = 0\n",
    "early_stopping_gbt = xgb.cv(xgb_params, dtrain=xgbmat, num_boost_round=2000, \n",
    "                            nfold=5, metrics=['error'], # 'error' for binary classification errors\n",
    "                            early_stopping_rounds=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test-error-mean</th>\n",
       "      <th>test-error-std</th>\n",
       "      <th>train-error-mean</th>\n",
       "      <th>train-error-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.195578</td>\n",
       "      <td>0.005825</td>\n",
       "      <td>0.196704</td>\n",
       "      <td>0.006953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.171638</td>\n",
       "      <td>0.016166</td>\n",
       "      <td>0.171474</td>\n",
       "      <td>0.012778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.173197</td>\n",
       "      <td>0.014537</td>\n",
       "      <td>0.172444</td>\n",
       "      <td>0.011176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.170809</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.170049</td>\n",
       "      <td>0.014146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.163185</td>\n",
       "      <td>0.006067</td>\n",
       "      <td>0.162265</td>\n",
       "      <td>0.006668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.159240</td>\n",
       "      <td>0.003312</td>\n",
       "      <td>0.157980</td>\n",
       "      <td>0.003241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.159837</td>\n",
       "      <td>0.003604</td>\n",
       "      <td>0.158370</td>\n",
       "      <td>0.003681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.159837</td>\n",
       "      <td>0.002249</td>\n",
       "      <td>0.158304</td>\n",
       "      <td>0.005109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.160168</td>\n",
       "      <td>0.003221</td>\n",
       "      <td>0.158801</td>\n",
       "      <td>0.002364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.160765</td>\n",
       "      <td>0.004690</td>\n",
       "      <td>0.159903</td>\n",
       "      <td>0.001628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.159737</td>\n",
       "      <td>0.003630</td>\n",
       "      <td>0.158171</td>\n",
       "      <td>0.002164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.158378</td>\n",
       "      <td>0.003227</td>\n",
       "      <td>0.157384</td>\n",
       "      <td>0.002425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.158809</td>\n",
       "      <td>0.003347</td>\n",
       "      <td>0.158279</td>\n",
       "      <td>0.000654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.158411</td>\n",
       "      <td>0.004225</td>\n",
       "      <td>0.158146</td>\n",
       "      <td>0.001572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.157682</td>\n",
       "      <td>0.002727</td>\n",
       "      <td>0.156886</td>\n",
       "      <td>0.002132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.156985</td>\n",
       "      <td>0.003785</td>\n",
       "      <td>0.156397</td>\n",
       "      <td>0.001912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.156389</td>\n",
       "      <td>0.003957</td>\n",
       "      <td>0.155527</td>\n",
       "      <td>0.003349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.155925</td>\n",
       "      <td>0.004164</td>\n",
       "      <td>0.155610</td>\n",
       "      <td>0.002986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.155891</td>\n",
       "      <td>0.004523</td>\n",
       "      <td>0.155560</td>\n",
       "      <td>0.003445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.156156</td>\n",
       "      <td>0.005151</td>\n",
       "      <td>0.155602</td>\n",
       "      <td>0.003279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.155626</td>\n",
       "      <td>0.005406</td>\n",
       "      <td>0.155328</td>\n",
       "      <td>0.003186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.155129</td>\n",
       "      <td>0.004320</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>0.001609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.154532</td>\n",
       "      <td>0.004117</td>\n",
       "      <td>0.153670</td>\n",
       "      <td>0.002115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.154731</td>\n",
       "      <td>0.004354</td>\n",
       "      <td>0.153770</td>\n",
       "      <td>0.002479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.153305</td>\n",
       "      <td>0.003701</td>\n",
       "      <td>0.153015</td>\n",
       "      <td>0.002315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.153637</td>\n",
       "      <td>0.003869</td>\n",
       "      <td>0.153073</td>\n",
       "      <td>0.002386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.153803</td>\n",
       "      <td>0.003859</td>\n",
       "      <td>0.153181</td>\n",
       "      <td>0.002844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.153471</td>\n",
       "      <td>0.003590</td>\n",
       "      <td>0.152510</td>\n",
       "      <td>0.002107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.153737</td>\n",
       "      <td>0.003912</td>\n",
       "      <td>0.152808</td>\n",
       "      <td>0.002098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.154002</td>\n",
       "      <td>0.003786</td>\n",
       "      <td>0.152949</td>\n",
       "      <td>0.002104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>0.131589</td>\n",
       "      <td>0.005139</td>\n",
       "      <td>0.124685</td>\n",
       "      <td>0.001640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>0.131523</td>\n",
       "      <td>0.005107</td>\n",
       "      <td>0.124677</td>\n",
       "      <td>0.001581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>0.131556</td>\n",
       "      <td>0.005128</td>\n",
       "      <td>0.124743</td>\n",
       "      <td>0.001575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>0.131589</td>\n",
       "      <td>0.005122</td>\n",
       "      <td>0.124735</td>\n",
       "      <td>0.001565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>0.131655</td>\n",
       "      <td>0.005055</td>\n",
       "      <td>0.124660</td>\n",
       "      <td>0.001606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>0.131622</td>\n",
       "      <td>0.005053</td>\n",
       "      <td>0.124635</td>\n",
       "      <td>0.001589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>0.131523</td>\n",
       "      <td>0.005009</td>\n",
       "      <td>0.124561</td>\n",
       "      <td>0.001612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0.131490</td>\n",
       "      <td>0.005048</td>\n",
       "      <td>0.124569</td>\n",
       "      <td>0.001565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0.131490</td>\n",
       "      <td>0.005025</td>\n",
       "      <td>0.124577</td>\n",
       "      <td>0.001583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0.131556</td>\n",
       "      <td>0.005103</td>\n",
       "      <td>0.124527</td>\n",
       "      <td>0.001569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0.131556</td>\n",
       "      <td>0.005017</td>\n",
       "      <td>0.124544</td>\n",
       "      <td>0.001581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.131457</td>\n",
       "      <td>0.004949</td>\n",
       "      <td>0.124544</td>\n",
       "      <td>0.001479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.131258</td>\n",
       "      <td>0.004892</td>\n",
       "      <td>0.124528</td>\n",
       "      <td>0.001495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.131324</td>\n",
       "      <td>0.004782</td>\n",
       "      <td>0.124503</td>\n",
       "      <td>0.001570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.131324</td>\n",
       "      <td>0.004778</td>\n",
       "      <td>0.124478</td>\n",
       "      <td>0.001538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.131291</td>\n",
       "      <td>0.004776</td>\n",
       "      <td>0.124395</td>\n",
       "      <td>0.001569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.131456</td>\n",
       "      <td>0.004841</td>\n",
       "      <td>0.124379</td>\n",
       "      <td>0.001573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>0.131390</td>\n",
       "      <td>0.004969</td>\n",
       "      <td>0.124262</td>\n",
       "      <td>0.001551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>0.131324</td>\n",
       "      <td>0.005011</td>\n",
       "      <td>0.124262</td>\n",
       "      <td>0.001573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>0.131191</td>\n",
       "      <td>0.004895</td>\n",
       "      <td>0.124221</td>\n",
       "      <td>0.001535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>0.131225</td>\n",
       "      <td>0.004895</td>\n",
       "      <td>0.124213</td>\n",
       "      <td>0.001532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>0.131258</td>\n",
       "      <td>0.004908</td>\n",
       "      <td>0.124196</td>\n",
       "      <td>0.001584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>0.131258</td>\n",
       "      <td>0.004986</td>\n",
       "      <td>0.124237</td>\n",
       "      <td>0.001576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>0.131258</td>\n",
       "      <td>0.004993</td>\n",
       "      <td>0.124237</td>\n",
       "      <td>0.001548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>0.131258</td>\n",
       "      <td>0.004994</td>\n",
       "      <td>0.124204</td>\n",
       "      <td>0.001601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>0.131059</td>\n",
       "      <td>0.004957</td>\n",
       "      <td>0.124229</td>\n",
       "      <td>0.001634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>0.131026</td>\n",
       "      <td>0.004980</td>\n",
       "      <td>0.124213</td>\n",
       "      <td>0.001640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>0.131059</td>\n",
       "      <td>0.005072</td>\n",
       "      <td>0.124163</td>\n",
       "      <td>0.001684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>0.131059</td>\n",
       "      <td>0.005054</td>\n",
       "      <td>0.124113</td>\n",
       "      <td>0.001673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>0.130992</td>\n",
       "      <td>0.005189</td>\n",
       "      <td>0.124088</td>\n",
       "      <td>0.001672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>519 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     test-error-mean  test-error-std  train-error-mean  train-error-std\n",
       "0           0.195578        0.005825          0.196704         0.006953\n",
       "1           0.171638        0.016166          0.171474         0.012778\n",
       "2           0.173197        0.014537          0.172444         0.011176\n",
       "3           0.170809        0.017857          0.170049         0.014146\n",
       "4           0.163185        0.006067          0.162265         0.006668\n",
       "5           0.159240        0.003312          0.157980         0.003241\n",
       "6           0.159837        0.003604          0.158370         0.003681\n",
       "7           0.159837        0.002249          0.158304         0.005109\n",
       "8           0.160168        0.003221          0.158801         0.002364\n",
       "9           0.160765        0.004690          0.159903         0.001628\n",
       "10          0.159737        0.003630          0.158171         0.002164\n",
       "11          0.158378        0.003227          0.157384         0.002425\n",
       "12          0.158809        0.003347          0.158279         0.000654\n",
       "13          0.158411        0.004225          0.158146         0.001572\n",
       "14          0.157682        0.002727          0.156886         0.002132\n",
       "15          0.156985        0.003785          0.156397         0.001912\n",
       "16          0.156389        0.003957          0.155527         0.003349\n",
       "17          0.155925        0.004164          0.155610         0.002986\n",
       "18          0.155891        0.004523          0.155560         0.003445\n",
       "19          0.156156        0.005151          0.155602         0.003279\n",
       "20          0.155626        0.005406          0.155328         0.003186\n",
       "21          0.155129        0.004320          0.154930         0.001609\n",
       "22          0.154532        0.004117          0.153670         0.002115\n",
       "23          0.154731        0.004354          0.153770         0.002479\n",
       "24          0.153305        0.003701          0.153015         0.002315\n",
       "25          0.153637        0.003869          0.153073         0.002386\n",
       "26          0.153803        0.003859          0.153181         0.002844\n",
       "27          0.153471        0.003590          0.152510         0.002107\n",
       "28          0.153737        0.003912          0.152808         0.002098\n",
       "29          0.154002        0.003786          0.152949         0.002104\n",
       "..               ...             ...               ...              ...\n",
       "489         0.131589        0.005139          0.124685         0.001640\n",
       "490         0.131523        0.005107          0.124677         0.001581\n",
       "491         0.131556        0.005128          0.124743         0.001575\n",
       "492         0.131589        0.005122          0.124735         0.001565\n",
       "493         0.131655        0.005055          0.124660         0.001606\n",
       "494         0.131622        0.005053          0.124635         0.001589\n",
       "495         0.131523        0.005009          0.124561         0.001612\n",
       "496         0.131490        0.005048          0.124569         0.001565\n",
       "497         0.131490        0.005025          0.124577         0.001583\n",
       "498         0.131556        0.005103          0.124527         0.001569\n",
       "499         0.131556        0.005017          0.124544         0.001581\n",
       "500         0.131457        0.004949          0.124544         0.001479\n",
       "501         0.131258        0.004892          0.124528         0.001495\n",
       "502         0.131324        0.004782          0.124503         0.001570\n",
       "503         0.131324        0.004778          0.124478         0.001538\n",
       "504         0.131291        0.004776          0.124395         0.001569\n",
       "505         0.131456        0.004841          0.124379         0.001573\n",
       "506         0.131390        0.004969          0.124262         0.001551\n",
       "507         0.131324        0.005011          0.124262         0.001573\n",
       "508         0.131191        0.004895          0.124221         0.001535\n",
       "509         0.131225        0.004895          0.124213         0.001532\n",
       "510         0.131258        0.004908          0.124196         0.001584\n",
       "511         0.131258        0.004986          0.124237         0.001576\n",
       "512         0.131258        0.004993          0.124237         0.001548\n",
       "513         0.131258        0.004994          0.124204         0.001601\n",
       "514         0.131059        0.004957          0.124229         0.001634\n",
       "515         0.131026        0.004980          0.124213         0.001640\n",
       "516         0.131059        0.005072          0.124163         0.001684\n",
       "517         0.131059        0.005054          0.124113         0.001673\n",
       "518         0.130992        0.005189          0.124088         0.001672\n",
       "\n",
       "[519 rows x 4 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stopping_gbt # a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From 518 - 50 to 518, there are almost no changes in test_error_mean\n",
    "num_boost_round = early_stopping_gbt.shape[0] - 50 // 2\n",
    "# train the final model\n",
    "# For xgboost.train, the parameter 'n_estimators' is of no use. We should use the 3rd parameter num_boost_round to control boosting.\n",
    "final_gbt = xgb.train(xgb_params, xgbmat, num_boost_round)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the final model's performance on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86872509960159361"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dmat = xgb.DMatrix(test_X)\n",
    "from sklearn.metrics import accuracy_score\n",
    "pred_y = final_gbt.predict(test_dmat)\n",
    "pred_y[pred_y > 0.5] = 1\n",
    "pred_y[pred_y <= 0.5] = 0\n",
    "accuracy_score(pred_y, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_gbt.save_model('final_gbt.model')\n",
    "final_gbt.dump_model('final_gbt.txt')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
